{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5003f90d",
   "metadata": {
    "papermill": {
     "duration": 0.008477,
     "end_time": "2025-11-10T16:27:43.121824",
     "exception": false,
     "start_time": "2025-11-10T16:27:43.113347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Solution\n",
    "\n",
    "This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).\n",
    "Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. \n",
    "\n",
    "The submission takes around 4 hours with `max_length=2048` without TTA. With TTA, it may take around 5-6 hours or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360cf453",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-11-10T16:27:43.138239Z",
     "iopub.status.busy": "2025-11-10T16:27:43.138004Z",
     "iopub.status.idle": "2025-11-10T16:28:08.359059Z",
     "shell.execute_reply": "2025-11-10T16:28:08.357929Z"
    },
    "papermill": {
     "duration": 25.232168,
     "end_time": "2025-11-10T16:28:08.361695",
     "exception": false,
     "start_time": "2025-11-10T16:27:43.129527",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lmsys-wheel-files\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Installing collected packages: bitsandbytes, accelerate, transformers, peft\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.30.1\r\n",
      "    Uninstalling accelerate-0.30.1:\r\n",
      "      Successfully uninstalled accelerate-0.30.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.41.2\r\n",
      "    Uninstalling transformers-4.41.2:\r\n",
      "      Successfully uninstalled transformers-4.41.2\r\n",
      "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 transformers-4.42.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21c2e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:08.380539Z",
     "iopub.status.busy": "2025-11-10T16:28:08.380255Z",
     "iopub.status.idle": "2025-11-10T16:28:28.096552Z",
     "shell.execute_reply": "2025-11-10T16:28:28.095791Z"
    },
    "papermill": {
     "duration": 19.728051,
     "end_time": "2025-11-10T16:28:28.098562",
     "exception": false,
     "start_time": "2025-11-10T16:28:08.370511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 16:28:17.867367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-10 16:28:17.867497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-10 16:28:18.012287: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1f51f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.119525Z",
     "iopub.status.busy": "2025-11-10T16:28:28.119052Z",
     "iopub.status.idle": "2025-11-10T16:28:28.151230Z",
     "shell.execute_reply": "2025-11-10T16:28:28.150580Z"
    },
    "papermill": {
     "duration": 0.044114,
     "end_time": "2025-11-10T16:28:28.152895",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.108781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0011f7",
   "metadata": {
    "papermill": {
     "duration": 0.008664,
     "end_time": "2025-11-10T16:28:28.170304",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.161640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac32f781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.188964Z",
     "iopub.status.busy": "2025-11-10T16:28:28.188721Z",
     "iopub.status.idle": "2025-11-10T16:28:28.195381Z",
     "shell.execute_reply": "2025-11-10T16:28:28.194686Z"
    },
    "papermill": {
     "duration": 0.01797,
     "end_time": "2025-11-10T16:28:28.197145",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.179175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    #gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    \n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v6/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v6-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v7-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v7-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v8-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v8-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v9-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v9-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v10-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v10-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v11-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v11-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v12-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v12-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v13-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v13-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v14-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v14-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v15-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v15-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v16-8bit/transformers/default/1'\n",
    "    #gemma_dir = '/kaggle/input/gemma-2-v16-4bit/transformers/default/1'\n",
    "\n",
    "    #gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-8bit/1'\n",
    "\n",
    "    gemma_dir = '/kaggle/input/gemma-2-v21/transformers/default/1'\n",
    "    gemma2_dir = '/kaggle/input/gemma-2-v6/transformers/default/1'\n",
    "    \n",
    "    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    #lora_dir = '/kaggle/input/lora-fold0-2000'\n",
    "    lora2_dir = '/kaggle/input/lora-fold1-4000'\n",
    "    \n",
    "    #lora_dir = '/kaggle/input/lora-fold4-v2'\n",
    "    #lora_dir = '/kaggle/input/lora-fold0-2000'\n",
    "    #lora_dir0 = '/kaggle/input/lora-fold0'\n",
    "    #lora_dir1 = '/kaggle/input/lora-fold1'\n",
    "    #lora_dir2 = '/kaggle/input/lora-fold2'\n",
    "    \n",
    "    #lora_dir0 = '/kaggle/input/lora-fold0-5000'\n",
    "    #lora_dir1 = '/kaggle/input/lora-fold1-5000'\n",
    "    #lora_dir2 = '/kaggle/input/lora-fold2-5000'\n",
    "    \n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = True  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d92d9",
   "metadata": {
    "papermill": {
     "duration": 0.008268,
     "end_time": "2025-11-10T16:28:28.213812",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.205544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load & pre-process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2db866c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.232324Z",
     "iopub.status.busy": "2025-11-10T16:28:28.232071Z",
     "iopub.status.idle": "2025-11-10T16:28:28.244505Z",
     "shell.execute_reply": "2025-11-10T16:28:28.243685Z"
    },
    "papermill": {
     "duration": 0.023783,
     "end_time": "2025-11-10T16:28:28.246238",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.222455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3277685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.265767Z",
     "iopub.status.busy": "2025-11-10T16:28:28.265060Z",
     "iopub.status.idle": "2025-11-10T16:28:28.286367Z",
     "shell.execute_reply": "2025-11-10T16:28:28.285403Z"
    },
    "papermill": {
     "duration": 0.032842,
     "end_time": "2025-11-10T16:28:28.288034",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.255192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868ca38",
   "metadata": {
    "papermill": {
     "duration": 0.008627,
     "end_time": "2025-11-10T16:28:28.305707",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.297080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5837b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.324280Z",
     "iopub.status.busy": "2025-11-10T16:28:28.324047Z",
     "iopub.status.idle": "2025-11-10T16:28:28.330364Z",
     "shell.execute_reply": "2025-11-10T16:28:28.329660Z"
    },
    "papermill": {
     "duration": 0.017532,
     "end_time": "2025-11-10T16:28:28.332141",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.314609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a024ba8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:28.351127Z",
     "iopub.status.busy": "2025-11-10T16:28:28.350697Z",
     "iopub.status.idle": "2025-11-10T16:28:29.407046Z",
     "shell.execute_reply": "2025-11-10T16:28:29.406076Z"
    },
    "papermill": {
     "duration": 1.067796,
     "end_time": "2025-11-10T16:28:29.409001",
     "exception": false,
     "start_time": "2025-11-10T16:28:28.341205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 683 ms, sys: 158 ms, total: 841 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f5655a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:29.428040Z",
     "iopub.status.busy": "2025-11-10T16:28:29.427771Z",
     "iopub.status.idle": "2025-11-10T16:28:29.432905Z",
     "shell.execute_reply": "2025-11-10T16:28:29.431995Z"
    },
    "papermill": {
     "duration": 0.016399,
     "end_time": "2025-11-10T16:28:29.434485",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.418086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "<response_a>: You have two oranges today.\n",
      "\n",
      "<response_b>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdc48b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:29.452966Z",
     "iopub.status.busy": "2025-11-10T16:28:29.452726Z",
     "iopub.status.idle": "2025-11-10T16:28:29.457555Z",
     "shell.execute_reply": "2025-11-10T16:28:29.456757Z"
    },
    "papermill": {
     "duration": 0.016034,
     "end_time": "2025-11-10T16:28:29.459378",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.443344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><prompt>: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n",
      "\n",
      "<response_a>: You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n",
      "\n",
      "<response_b>: You have two oranges today.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60459349",
   "metadata": {
    "papermill": {
     "duration": 0.008978,
     "end_time": "2025-11-10T16:28:29.477706",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.468728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load model + Inferencing (Multi-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f19ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:29.497770Z",
     "iopub.status.busy": "2025-11-10T16:28:29.497444Z",
     "iopub.status.idle": "2025-11-10T16:28:29.508056Z",
     "shell.execute_reply": "2025-11-10T16:28:29.507111Z"
    },
    "papermill": {
     "duration": 0.022812,
     "end_time": "2025-11-10T16:28:29.509763",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.486951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresult_df.loc[:, \"winner_model_a\"] = final_proba[:, 0]\\nresult_df.loc[:, \"winner_model_b\"] = final_proba[:, 1]\\nresult_df.loc[:, \"winner_tie\"] = final_proba[:, 2]\\nsubmission_df = result_df[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\\nsubmission_df.to_csv(\"submission.csv\", index=False)\\ndisplay(submission_df)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NUM_FOLDS = 3\n",
    "fold_preds = []\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "\n",
    "    lora_path = \"\"\n",
    "    if fold == 0:\n",
    "        lora_path = cfg.lora_dir0\n",
    "    elif fold == 1:\n",
    "        lora_path = cfg.lora_dir1\n",
    "    elif fold == 2:\n",
    "        lora_path = cfg.lora_dir2\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected fold number: {fold}\")\n",
    "    \n",
    "    # Load base model on GPU 0\n",
    "    device_0 = torch.device('cuda:0')\n",
    "    model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma_dir,\n",
    "        device_map=device_0,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "    # Load base model on GPU 1\n",
    "    device_1 = torch.device('cuda:1')\n",
    "    model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "        cfg.gemma_dir,\n",
    "        device_map=device_1,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model_0 = PeftModel.from_pretrained(model_0, lora_path)\n",
    "    model_1 = PeftModel.from_pretrained(model_1, lora_path)\n",
    "    \n",
    "    # Inference\n",
    "    @torch.no_grad()\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "        a_win, b_win, tie = [], [], []\n",
    "        \n",
    "        for start_idx in range(0, len(df), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(df))\n",
    "            tmp = df.iloc[start_idx:end_idx]\n",
    "            input_ids = tmp[\"input_ids\"].to_list()\n",
    "            attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "            inputs = pad_without_fast_tokenizer_warning(\n",
    "                tokenizer,\n",
    "                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "                padding=\"longest\",\n",
    "                pad_to_multiple_of=None,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            outputs = model(**inputs.to(device))\n",
    "            proba = outputs.logits.softmax(-1).cpu()\n",
    "            \n",
    "            a_win.extend(proba[:, 0].tolist())\n",
    "            b_win.extend(proba[:, 1].tolist())\n",
    "            tie.extend(proba[:, 2].tolist())\n",
    "        \n",
    "        df[\"winner_model_a\"] = a_win\n",
    "        df[\"winner_model_b\"] = b_win\n",
    "        df[\"winner_tie\"] = tie\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    # sort by input length to fully leverage dynaminc padding\n",
    "    data = data.sort_values(\"length\", ascending=False)\n",
    "    # the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "    \n",
    "    result_df = pd.concat(list(results), axis=0)\n",
    "    proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "    \n",
    "    print(f\"elapsed time: {time.time() - st}\")\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    if cfg.tta:\n",
    "        data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "        sub_1 = data.iloc[0::2].copy()\n",
    "        sub_2 = data.iloc[1::2].copy()\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "    \n",
    "        tta_result_df = pd.concat(list(results), axis=0)\n",
    "        # recall TTA's order is flipped\n",
    "        tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "        # average original result and TTA result.\n",
    "        proba = (proba + tta_proba) / 2\n",
    "    \n",
    "    print(f\"elapsed time: {time.time() - st}\")\n",
    "    fold_preds.append(proba)\n",
    "'''\n",
    "# === Original\n",
    "\n",
    "\"\"\"\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n",
    "\"\"\"\n",
    "\n",
    "# === Average Probabilities of all Folds =====\n",
    "'''\n",
    "final_proba = np.mean(fold_preds, axis=0)\n",
    "'''\n",
    "# === Weighted Averaging (Validation-Weighted) ===\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# fold_preds = [fold0_pred, fold1_pred, fold2_pred]  # numpy arrays of shape (n_samples, 3)\n",
    "fold_logloss = np.array([1.0615, 1.0743, 1.3646])\n",
    "\n",
    "# Weight folds by inverse log loss\n",
    "weights = 1 / fold_logloss\n",
    "weights /= weights.sum()  # normalize to sum=1\n",
    "print(\"Weights:\", weights)\n",
    "\n",
    "# Weighted blend\n",
    "final_proba = np.average(fold_preds, axis=0, weights=weights)\n",
    "'''\n",
    "# === Stacking (Meta-Ensemble) ===\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# suppose each fold_pred is shape (N, num_classes)\n",
    "X_stack = np.concatenate(fold_preds, axis=1)\n",
    "# true labels (from validation set)\n",
    "y_stack = np.array(y_true)\n",
    "\n",
    "# train a meta model\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "meta_model.fit(X_stack, y_stack)\n",
    "\n",
    "# during inference\n",
    "X_test_stack = np.concatenate([fold0_test, fold1_test, fold2_test], axis=1)\n",
    "final_pred = meta_model.predict_proba(X_test_stack)\n",
    "final_proba = final_pred\n",
    "'''\n",
    "\n",
    "# To try:\n",
    "# === Geometric Mean (Multiplicative Ensemble) ===\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# avoid zeros\n",
    "eps = 1e-9\n",
    "final_pred = (fold_preds[0] + eps) * (fold_preds[1] + eps) * (fold_preds[2] + eps)\n",
    "final_pred = final_pred ** (1/3)  # geometric mean\n",
    "final_pred /= final_pred.sum(axis=1, keepdims=True)  # normalize\n",
    "'''\n",
    "\n",
    "# === Majority Voting / Argmax Voting ===\n",
    "'''\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "fold0_label = np.argmax(fold_preds[0], axis=1)\n",
    "fold1_label = np.argmax(fold_preds[1], axis=1)\n",
    "fold2_label = np.argmax(fold_preds[2], axis=1)\n",
    "\n",
    "votes = np.vstack([fold0_label, fold1_label, fold2_label]).T\n",
    "final_label, _ = mode(votes, axis=1)\n",
    "final_label = final_label.flatten()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert labels → one-hot probabilities\n",
    "num_classes = 3\n",
    "final_proba = np.eye(num_classes)[final_label]\n",
    "'''\n",
    "\n",
    "# === Bayesian Model Averaging\n",
    "'''\n",
    "# Example validation accuracies (you can replace with your own)\n",
    "val_acc = np.array([0.86, 0.88, 0.84])\n",
    "\n",
    "# Convert to softmax weights so they sum to 1\n",
    "weights = np.exp(val_acc) / np.exp(val_acc).sum()\n",
    "print(\"Weights:\", weights)\n",
    "\n",
    "# Weighted average of fold prediction probabilities\n",
    "final_proba = (\n",
    "    weights[0] * fold_preds[0] +\n",
    "    weights[1] * fold_preds[1] +\n",
    "    weights[2] * fold_preds[2]\n",
    ")\n",
    "\n",
    "# Normalize just in case (should already be close to valid probabilities)\n",
    "final_proba /= final_proba.sum(axis=1, keepdims=True)\n",
    "'''\n",
    "\n",
    "\n",
    "# === Hybrid – Weighted + Geometric Mean ===\n",
    "'''\n",
    "fold_preds = np.clip(fold_preds, 1e-9, 1.0)\n",
    "log_preds = [np.log(p) * w for p, w in zip(fold_preds, weights)]\n",
    "final_proba = np.exp(np.sum(log_preds, axis=0) / np.sum(weights))\n",
    "final_proba /= final_proba.sum(axis=1, keepdims=True)\n",
    "'''\n",
    "# ===\n",
    "'''\n",
    "result_df.loc[:, \"winner_model_a\"] = final_proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = final_proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = final_proba[:, 2]\n",
    "submission_df = result_df[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "display(submission_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e47a8",
   "metadata": {
    "papermill": {
     "duration": 0.008828,
     "end_time": "2025-11-10T16:28:29.527361",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.518533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load model + Inferencing (Single Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3299ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:28:29.546190Z",
     "iopub.status.busy": "2025-11-10T16:28:29.545951Z",
     "iopub.status.idle": "2025-11-10T16:30:13.239165Z",
     "shell.execute_reply": "2025-11-10T16:30:13.238437Z"
    },
    "papermill": {
     "duration": 103.705024,
     "end_time": "2025-11-10T16:30:13.241260",
     "exception": false,
     "start_time": "2025-11-10T16:28:29.536236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb9a5842ca44b90830c9417b9be6d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-v21/transformers/default/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44358617558f47ada4552ceb939b35b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-v21/transformers/default/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model on GPU 0\n",
    "\n",
    "\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68fcd18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:13.261568Z",
     "iopub.status.busy": "2025-11-10T16:30:13.261305Z",
     "iopub.status.idle": "2025-11-10T16:30:13.268091Z",
     "shell.execute_reply": "2025-11-10T16:30:13.267251Z"
    },
    "papermill": {
     "duration": 0.018654,
     "end_time": "2025-11-10T16:30:13.269678",
     "exception": false,
     "start_time": "2025-11-10T16:30:13.251024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7078a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:13.289189Z",
     "iopub.status.busy": "2025-11-10T16:30:13.288735Z",
     "iopub.status.idle": "2025-11-10T16:30:18.009513Z",
     "shell.execute_reply": "2025-11-10T16:30:18.008612Z"
    },
    "papermill": {
     "duration": 4.732461,
     "end_time": "2025-11-10T16:30:18.011360",
     "exception": false,
     "start_time": "2025-11-10T16:30:13.278899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.7152769565582275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynamic padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f676e0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:18.031967Z",
     "iopub.status.busy": "2025-11-10T16:30:18.031470Z",
     "iopub.status.idle": "2025-11-10T16:30:21.728022Z",
     "shell.execute_reply": "2025-11-10T16:30:21.727097Z"
    },
    "papermill": {
     "duration": 3.708671,
     "end_time": "2025-11-10T16:30:21.729865",
     "exception": false,
     "start_time": "2025-11-10T16:30:18.021194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.6905341148376465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b63641",
   "metadata": {
    "papermill": {
     "duration": 0.009214,
     "end_time": "2025-11-10T16:30:21.749189",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.739975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load model + Inferencing (Ensembled Folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81bee507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:21.769235Z",
     "iopub.status.busy": "2025-11-10T16:30:21.768989Z",
     "iopub.status.idle": "2025-11-10T16:30:21.776059Z",
     "shell.execute_reply": "2025-11-10T16:30:21.775218Z"
    },
    "papermill": {
     "duration": 0.019319,
     "end_time": "2025-11-10T16:30:21.777690",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.758371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer\\nfrom peft import PeftModel\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport torch\\nimport pandas as pd\\nimport numpy as np\\nimport time\\n\\n# ---------------- CONFIG ----------------\\nFOLD_FOLDERS = [\\n    \"/kaggle/input/lora-fold0-4000\",\\n    \"/kaggle/input/lora-fold1-4000\",\\n    \"/kaggle/input/lora-fold2-4000\"\\n]\\n\\nBASE_MODEL = \"/kaggle/input/gemma-2-v21/transformers/default/1\"\\nTTA_ENABLED = True\\nBATCH_SIZE = 4\\n\\nDEVICE_0 = torch.device(\"cuda:0\")\\nDEVICE_1 = torch.device(\"cuda:1\")\\n\\n# ---------------- HELPER FUNCTIONS ----------------\\ndef inference(df, model, device, tokenizer, batch_size=BATCH_SIZE):\\n    model.eval()\\n    a_win, b_win, tie = [], [], []\\n\\n    for start_idx in range(0, len(df), batch_size):\\n        end_idx = min(start_idx + batch_size, len(df))\\n        tmp = df.iloc[start_idx:end_idx]\\n        input_ids = tmp[\"input_ids\"].to_list()\\n        attention_mask = tmp[\"attention_mask\"].to_list()\\n        inputs = tokenizer.pad(\\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\\n            padding=\"longest\",\\n            return_tensors=\"pt\",\\n        ).to(device)\\n\\n        with torch.no_grad(), torch.cuda.amp.autocast():\\n            outputs = model(**inputs)\\n            proba = outputs.logits.softmax(-1).cpu()\\n\\n        a_win.extend(proba[:,0].tolist())\\n        b_win.extend(proba[:,1].tolist())\\n        tie.extend(proba[:,2].tolist())\\n\\n    df_out = df.copy()\\n    df_out[\"winner_model_a\"] = a_win\\n    df_out[\"winner_model_b\"] = b_win\\n    df_out[\"winner_tie\"] = tie\\n    return df_out\\n\\ndef symmetric_average(df):\\n    df_copy = df.copy()\\n    df_flipped = df_copy[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].copy()\\n    df_avg = (df_copy[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] + df_flipped) / 2\\n    df_copy[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = df_avg\\n    return df_copy\\n\\ndef generate_tta_variants(df):\\n    # Placeholder: just returns original for now\\n    return [df.copy()]  \\n\\n# ---------------- MAIN LOOP ----------------\\nfold_preds = []\\n\\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\\n\\nfor fold_idx, fold_folder in enumerate(FOLD_FOLDERS):\\n    print(f\"\\n=== FOLD {fold_idx} ===\")\\n    \\n    # load full base + LoRA per fold\\n    model_0 = Gemma2ForSequenceClassification.from_pretrained(BASE_MODEL)\\n    model_1 = Gemma2ForSequenceClassification.from_pretrained(BASE_MODEL)\\n    model_0 = PeftModel.from_pretrained(model_0, fold_folder)\\n    model_1 = PeftModel.from_pretrained(model_1, fold_folder)\\n\\n    # ---- NORMAL INFERENCE ----\\n    test[\"length\"] = test[\"input_ids\"].apply(len)\\n    data = test.sort_values(\"length\", ascending=False)\\n    sub_1 = data.iloc[0::2].copy()\\n    sub_2 = data.iloc[1::2].copy()\\n\\n    st = time.time()\\n    with ThreadPoolExecutor(max_workers=2) as executor:\\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (tokenizer, tokenizer))\\n    result_df = pd.concat(list(results), axis=0)\\n    proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\\n    print(f\"Normal inference done in {time.time() - st:.1f}s\")\\n\\n    # ---- TTA INFERENCE ----\\n    if TTA_ENABLED:\\n        tta_variants = generate_tta_variants(test)\\n        tta_probs_list = []\\n        for df_aug in tta_variants:\\n            df_out = inference(df_aug, model_0, DEVICE_0, tokenizer)  # single model for simplicity\\n            # Symmetrize\\n            df_out = symmetric_average(df_out)\\n            tta_probs_list.append(df_out[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values)\\n        # average across TTA variants\\n        proba = np.mean(tta_probs_list, axis=0)\\n\\n    fold_df = pd.DataFrame({\\n        \"id\": np.arange(proba.shape[0]),\\n        \"winner_model_a\": proba[:,0],\\n        \"winner_model_b\": proba[:,1],\\n        \"winner_tie\": proba[:,2]\\n    })\\n    \\n    fold_preds.append(fold_df)\\n\\n# ---- ENSEMBLE ACROSS FOLDS ----\\nensemble_df = pd.concat(fold_preds).groupby(\"id\").mean().reset_index()\\nensemble_df.to_csv(\"submission.csv\", index=False)\\ndisplay(ensemble_df)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "FOLD_FOLDERS = [\n",
    "    \"/kaggle/input/lora-fold0-4000\",\n",
    "    \"/kaggle/input/lora-fold1-4000\",\n",
    "    \"/kaggle/input/lora-fold2-4000\"\n",
    "]\n",
    "\n",
    "BASE_MODEL = \"/kaggle/input/gemma-2-v21/transformers/default/1\"\n",
    "TTA_ENABLED = True\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DEVICE_0 = torch.device(\"cuda:0\")\n",
    "DEVICE_1 = torch.device(\"cuda:1\")\n",
    "\n",
    "# ---------------- HELPER FUNCTIONS ----------------\n",
    "def inference(df, model, device, tokenizer, batch_size=BATCH_SIZE):\n",
    "    model.eval()\n",
    "    a_win, b_win, tie = [], [], []\n",
    "\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = tokenizer.pad(\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            outputs = model(**inputs)\n",
    "            proba = outputs.logits.softmax(-1).cpu()\n",
    "\n",
    "        a_win.extend(proba[:,0].tolist())\n",
    "        b_win.extend(proba[:,1].tolist())\n",
    "        tie.extend(proba[:,2].tolist())\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out[\"winner_model_a\"] = a_win\n",
    "    df_out[\"winner_model_b\"] = b_win\n",
    "    df_out[\"winner_tie\"] = tie\n",
    "    return df_out\n",
    "\n",
    "def symmetric_average(df):\n",
    "    df_copy = df.copy()\n",
    "    df_flipped = df_copy[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].copy()\n",
    "    df_avg = (df_copy[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] + df_flipped) / 2\n",
    "    df_copy[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = df_avg\n",
    "    return df_copy\n",
    "\n",
    "def generate_tta_variants(df):\n",
    "    # Placeholder: just returns original for now\n",
    "    return [df.copy()]  \n",
    "\n",
    "# ---------------- MAIN LOOP ----------------\n",
    "fold_preds = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "for fold_idx, fold_folder in enumerate(FOLD_FOLDERS):\n",
    "    print(f\"\\n=== FOLD {fold_idx} ===\")\n",
    "    \n",
    "    # load full base + LoRA per fold\n",
    "    model_0 = Gemma2ForSequenceClassification.from_pretrained(BASE_MODEL)\n",
    "    model_1 = Gemma2ForSequenceClassification.from_pretrained(BASE_MODEL)\n",
    "    model_0 = PeftModel.from_pretrained(model_0, fold_folder)\n",
    "    model_1 = PeftModel.from_pretrained(model_1, fold_folder)\n",
    "\n",
    "    # ---- NORMAL INFERENCE ----\n",
    "    test[\"length\"] = test[\"input_ids\"].apply(len)\n",
    "    data = test.sort_values(\"length\", ascending=False)\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    st = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (tokenizer, tokenizer))\n",
    "    result_df = pd.concat(list(results), axis=0)\n",
    "    proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "    print(f\"Normal inference done in {time.time() - st:.1f}s\")\n",
    "\n",
    "    # ---- TTA INFERENCE ----\n",
    "    if TTA_ENABLED:\n",
    "        tta_variants = generate_tta_variants(test)\n",
    "        tta_probs_list = []\n",
    "        for df_aug in tta_variants:\n",
    "            df_out = inference(df_aug, model_0, DEVICE_0, tokenizer)  # single model for simplicity\n",
    "            # Symmetrize\n",
    "            df_out = symmetric_average(df_out)\n",
    "            tta_probs_list.append(df_out[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values)\n",
    "        # average across TTA variants\n",
    "        proba = np.mean(tta_probs_list, axis=0)\n",
    "\n",
    "    fold_df = pd.DataFrame({\n",
    "        \"id\": np.arange(proba.shape[0]),\n",
    "        \"winner_model_a\": proba[:,0],\n",
    "        \"winner_model_b\": proba[:,1],\n",
    "        \"winner_tie\": proba[:,2]\n",
    "    })\n",
    "    \n",
    "    fold_preds.append(fold_df)\n",
    "\n",
    "# ---- ENSEMBLE ACROSS FOLDS ----\n",
    "ensemble_df = pd.concat(fold_preds).groupby(\"id\").mean().reset_index()\n",
    "ensemble_df.to_csv(\"submission.csv\", index=False)\n",
    "display(ensemble_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2e19d",
   "metadata": {
    "papermill": {
     "duration": 0.00928,
     "end_time": "2025-11-10T16:30:21.796268",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.786988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Self-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7ffcd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:21.816071Z",
     "iopub.status.busy": "2025-11-10T16:30:21.815830Z",
     "iopub.status.idle": "2025-11-10T16:30:21.821384Z",
     "shell.execute_reply": "2025-11-10T16:30:21.820585Z"
    },
    "papermill": {
     "duration": 0.017154,
     "end_time": "2025-11-10T16:30:21.822870",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.805716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\n\\ndef self_ensemble_per_sample(probas: np.ndarray, group_size: int = 8) -> np.ndarray:\\n    \"\"\"\\n    Smooths logits/probabilities by blending each sample with a small local neighborhood average.\\n    Inspired by \\'Self-Ensemble Calibration\\' (arXiv:2506.01951).\\n    \"\"\"\\n    n = len(probas)\\n    smoothed = probas.copy().astype(np.float32)\\n    for i in range(n):\\n        start = max(0, i - group_size // 2)\\n        end = min(n, i + group_size // 2)\\n        local_mean = probas[start:end].mean(axis=0)\\n        smoothed[i] = 0.7 * probas[i] + 0.3 * local_mean\\n    return smoothed / smoothed.sum(axis=1, keepdims=True)\\n\\n# Apply smoothing\\n# proba = self_ensemble_per_sample(proba)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def self_ensemble(probas, group_size=2, n_rounds=3):\n",
    "    n = len(probas)\n",
    "    all_outputs = np.zeros_like(probas)\n",
    "    for _ in range(n_rounds):\n",
    "        indices = np.random.permutation(n)\n",
    "        groups = [indices[i:i+group_size] for i in range(0, n, group_size)]\n",
    "        smoothed = np.zeros_like(probas)\n",
    "        for g in groups:\n",
    "            group_mean = probas[g].mean(axis=0)\n",
    "            smoothed[g] = 0.5 * probas[g] + 0.5 * group_mean  # blend with group mean\n",
    "        all_outputs += smoothed\n",
    "    return all_outputs / n_rounds\n",
    "'''\n",
    "# proba = self_ensemble(proba, group_size=4, n_rounds=5)\n",
    "\n",
    "# --- Self-Ensemble Smoothing (per-sample calibration) ---\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "def self_ensemble_per_sample(probas: np.ndarray, group_size: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Smooths logits/probabilities by blending each sample with a small local neighborhood average.\n",
    "    Inspired by 'Self-Ensemble Calibration' (arXiv:2506.01951).\n",
    "    \"\"\"\n",
    "    n = len(probas)\n",
    "    smoothed = probas.copy().astype(np.float32)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - group_size // 2)\n",
    "        end = min(n, i + group_size // 2)\n",
    "        local_mean = probas[start:end].mean(axis=0)\n",
    "        smoothed[i] = 0.7 * probas[i] + 0.3 * local_mean\n",
    "    return smoothed / smoothed.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Apply smoothing\n",
    "# proba = self_ensemble_per_sample(proba)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ed3b3",
   "metadata": {
    "papermill": {
     "duration": 0.009259,
     "end_time": "2025-11-10T16:30:21.841723",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.832464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Math Processing (NLP Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56add336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:21.899884Z",
     "iopub.status.busy": "2025-11-10T16:30:21.899542Z",
     "iopub.status.idle": "2025-11-10T16:30:21.906780Z",
     "shell.execute_reply": "2025-11-10T16:30:21.906036Z"
    },
    "papermill": {
     "duration": 0.057418,
     "end_time": "2025-11-10T16:30:21.908490",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.851072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# ---- parameters (the tuned values found above) ----\n",
    "alpha = 19.831675854828404\n",
    "gamma = 49.50181595059537\n",
    "eta   = 13.438806629499513\n",
    "eps0  = 0.2512371308916018\n",
    "beta  = 0.0   # optional suppression of A when B leads\n",
    "\n",
    "def probs_from_scores(sA, sB, alpha, gamma, eta, eps0, beta=0.0):\n",
    "    d = sB - sA\n",
    "    RA = (sA ** alpha) * (1 - beta * max(0.0, d))\n",
    "    RB = (sB ** alpha) * (1 + gamma * max(0.0, d))\n",
    "    eps = eps0 * math.exp(-eta * max(0.0, d))\n",
    "    RT = eps * (RA + RB)\n",
    "    S = RA + RB + RT\n",
    "    return (RA/S, RB/S, RT/S)\n",
    "\n",
    "# your rows: scores from NLP research task (model_a, model_b) - case 1 ignored\n",
    "\n",
    "# your rows:\n",
    "rows = {\n",
    "    #0: (0.640567, 0.860696),\n",
    "    # 1: (0.7023, 0.6961),\n",
    "    # 1: (0.6026, 0.5994),\n",
    "    # 1: (0.5911, 0.5819),\n",
    "    #1: (0.5979, 0.6212),\n",
    "    #2: (0.748224, 0.760298),\n",
    "}\n",
    "\n",
    "for cid, (sA, sB) in rows.items():\n",
    "    pA,pB,pT = probs_from_scores(sA, sB, alpha, gamma, eta, eps0, beta)\n",
    "\n",
    "    r = 0\n",
    "    if cid == 0:\n",
    "        r = 1\n",
    "    elif cid == 2:\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 2\n",
    "\n",
    "    proba[r,0]= pA\n",
    "    proba[r,1]= pB\n",
    "    proba[r,2]= pT\n",
    "    \n",
    "    print(f\"case {cid}: pA={pA:.12f}, pB={pB:.12f}, pT={pT:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa05b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T16:30:21.929459Z",
     "iopub.status.busy": "2025-11-10T16:30:21.928718Z",
     "iopub.status.idle": "2025-11-10T16:30:21.943389Z",
     "shell.execute_reply": "2025-11-10T16:30:21.942541Z"
    },
    "papermill": {
     "duration": 0.026832,
     "end_time": "2025-11-10T16:30:21.945122",
     "exception": false,
     "start_time": "2025-11-10T16:30:21.918290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.213238</td>\n",
       "      <td>0.474788</td>\n",
       "      <td>0.311974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>0.903275</td>\n",
       "      <td>0.073860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.208357</td>\n",
       "      <td>0.393043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "2  1233961        0.213238        0.474788    0.311974\n",
       "0   136060        0.022865        0.903275    0.073860\n",
       "1   211333        0.398600        0.208357    0.393043"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "isSourceIdPinned": false,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8631759,
     "sourceId": 13586185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8631796,
     "sourceId": 13586234,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8632347,
     "sourceId": 13587004,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8645096,
     "sourceId": 13604650,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8645117,
     "sourceId": 13604680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8645135,
     "sourceId": 13604705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8660245,
     "sourceId": 13626003,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8660252,
     "sourceId": 13626016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8660260,
     "sourceId": 13626026,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8662234,
     "sourceId": 13628851,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663409,
     "sourceId": 13630509,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663414,
     "sourceId": 13630515,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663420,
     "sourceId": 13630522,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663425,
     "sourceId": 13630530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663435,
     "sourceId": 13630550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8663437,
     "sourceId": 13630561,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8664982,
     "sourceId": 13632696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8664991,
     "sourceId": 13632710,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8670966,
     "sourceId": 13641076,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8670969,
     "sourceId": 13641081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8670973,
     "sourceId": 13641086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8670975,
     "sourceId": 13641090,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8670978,
     "sourceId": 13641093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8697054,
     "sourceId": 13677056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8697064,
     "sourceId": 13677068,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8697083,
     "sourceId": 13677090,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8700431,
     "sourceId": 13681559,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 491174,
     "modelInstanceId": 475279,
     "sourceId": 630716,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 495514,
     "modelInstanceId": 479793,
     "sourceId": 636321,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 86587,
     "modelInstanceId": 63761,
     "sourceId": 75886,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 165.291587,
   "end_time": "2025-11-10T16:30:25.708437",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-10T16:27:40.416850",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "129cfc3a829744b1b3275442dec4e8b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "29969795ed6c44229d5eb206e448cf6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2c53dad38eed42949d7c6650647c0879": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "44358617558f47ada4552ceb939b35b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7766145161fa461baf2897b7edefdf03",
        "IPY_MODEL_47529b329c99465089e0cb5405e299cf",
        "IPY_MODEL_5bd64ad6bc0b48b299a47db333260331"
       ],
       "layout": "IPY_MODEL_a4b5272842c541e4bf3dde5928db3339"
      }
     },
     "47529b329c99465089e0cb5405e299cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ea3f19b9228a4947bfee8da98b4adfa3",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_670d57c8c1db4ae1a9c35a25a53e9c20",
       "value": 3.0
      }
     },
     "5bd64ad6bc0b48b299a47db333260331": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_686ae0b82cc04b69989274158853a434",
       "placeholder": "​",
       "style": "IPY_MODEL_6f3681a0a3474a84a96a52b3c3b23c0c",
       "value": " 3/3 [00:15&lt;00:00,  4.55s/it]"
      }
     },
     "5cb7f70a75a2459cb62956a4dae16bf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f54bd88872948e1b5d9c96b9bef3b39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "603e49ed29e54735b61b756576bb4d93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "670d57c8c1db4ae1a9c35a25a53e9c20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "686ae0b82cc04b69989274158853a434": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f3681a0a3474a84a96a52b3c3b23c0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7766145161fa461baf2897b7edefdf03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cf4451d086414ebdae5d81245383cfcd",
       "placeholder": "​",
       "style": "IPY_MODEL_29969795ed6c44229d5eb206e448cf6b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "8c5b766f893849b89a41c272642f7ab5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f54bd88872948e1b5d9c96b9bef3b39",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2c53dad38eed42949d7c6650647c0879",
       "value": 3.0
      }
     },
     "989084bfce094662a49d5df7b22d2639": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5cb7f70a75a2459cb62956a4dae16bf5",
       "placeholder": "​",
       "style": "IPY_MODEL_eceb4f750a444342b68eeed7d22c141e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "a4b5272842c541e4bf3dde5928db3339": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3524c9a7a5e46888dfd217b76e21470": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfb9a5842ca44b90830c9417b9be6d71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_989084bfce094662a49d5df7b22d2639",
        "IPY_MODEL_8c5b766f893849b89a41c272642f7ab5",
        "IPY_MODEL_e7a2a9c01dcf4276a89e81ed2c8df61e"
       ],
       "layout": "IPY_MODEL_603e49ed29e54735b61b756576bb4d93"
      }
     },
     "cf4451d086414ebdae5d81245383cfcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7a2a9c01dcf4276a89e81ed2c8df61e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b3524c9a7a5e46888dfd217b76e21470",
       "placeholder": "​",
       "style": "IPY_MODEL_129cfc3a829744b1b3275442dec4e8b2",
       "value": " 3/3 [01:25&lt;00:00, 23.23s/it]"
      }
     },
     "ea3f19b9228a4947bfee8da98b4adfa3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eceb4f750a444342b68eeed7d22c141e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
