[Step1] Resolving SCRATCH_BASE...
[Debug] SCRATCH_BASE resolved to: /scratch-shared/tc1proj005
[Step1] Working dir: /home/UG/ivansoji001/exported-assets_sc4000
[Step1] Home dir: /home/UG/ivansoji001
[Step1] SCRATCH_BASE: /scratch-shared/tc1proj005
[Step1] HF_HOME: /scratch-shared/tc1proj005
[Step1] HUGGINGFACE_HUB_CACHE: /scratch-shared/tc1proj005
[Step1] HF_DATASETS_CACHE: /scratch-shared/tc1proj005
[Step1] TRANSFORMERS_CACHE: /scratch-shared/tc1proj005
[Step1] UT_DATA: data/ultrafeedback.csv | EPOCHS: 1 | LR: 1e-5 | MAXLEN: 512 | PER_DEVICE_BS: 1 | MAX_STEPS: -1 | LORA_R: 16 | LORA_ALPHA: 32
[Step1][AutoCap] Projected 500 steps (~6h) exceeds budget 6h; capping to 450.
[Step1][Force] Overriding max steps to 240 (ignoring POSTPRE_MAX_STEPS=450)
[Step1] Deps OK
[Step1] Online mode enabled (TRANSFORMERS_OFFLINE=0)
[Step1] Downloading tokenizer files for Qwen/Qwen2.5-14B...
[Step1] Tokenizer snapshot at: /scratch-shared/tc1proj005/models--Qwen--Qwen2.5-14B/snapshots/97e1e76335b7017d8f67c08a19d103c0504298c9
[Step1] Prefetched tokenizer: vocab size 151665
[Step1] Starting post-pretraining (LoRA/QLoRA)
[Step1] Selected stage: qwen
[Step1] Disk check for /scratch-shared/tc1proj005: need 180G, avail 85415G
[Step1] Cleaning HF cache at /scratch-shared/tc1proj005
[Step1] Skipping cache clean to avoid wiping SCRATCH_BASE root
[Step1] Qwen/Qwen2.5-14B QLoRA: starting
[Step1] Qwen args: --base-model Qwen/Qwen2.5-14B --output-dir /scratch-shared/tc1proj005/post_pretrain_qwen2-72b_lora --data-path data/ultrafeedback.csv --tokenizer-path Qwen/Qwen2.5-14B --bf16 --qlora --epochs 1 --lr 1e-5 --max-length 512 --r 16 --lora-alpha 32 --per-device-batch 1 --grad-accum 12 --subset-size 6000 --max-steps 240
[DEBUG] Trainable params: 68,812,800 / 8,232,817,664 (0.8358%)
[INFO] Subsetting training data: 6000 / 157675 examples (requested 6000)
{'loss': 0.8145, 'grad_norm': 0.18419574201107025, 'learning_rate': 9.208333333333333e-06, 'epoch': 0.04}
{'loss': 0.7724, 'grad_norm': 0.1909014731645584, 'learning_rate': 8.375e-06, 'epoch': 0.08}
{'loss': 0.7329, 'grad_norm': 0.1539488285779953, 'learning_rate': 7.541666666666667e-06, 'epoch': 0.12}
{'loss': 0.7261, 'grad_norm': 0.1754492223262787, 'learning_rate': 6.708333333333333e-06, 'epoch': 0.16}
{'loss': 0.7246, 'grad_norm': 0.17446140944957733, 'learning_rate': 5.8750000000000005e-06, 'epoch': 0.2}
{'loss': 0.7397, 'grad_norm': 0.17957690358161926, 'learning_rate': 5.041666666666667e-06, 'epoch': 0.24}
{'loss': 0.715, 'grad_norm': 0.1499522477388382, 'learning_rate': 4.208333333333333e-06, 'epoch': 0.28}
{'loss': 0.6941, 'grad_norm': 0.1680046170949936, 'learning_rate': 3.3750000000000003e-06, 'epoch': 0.32}
{'loss': 0.7232, 'grad_norm': 0.17457738518714905, 'learning_rate': 2.5416666666666668e-06, 'epoch': 0.36}
{'loss': 0.75, 'grad_norm': 0.15484467148780823, 'learning_rate': 1.7083333333333334e-06, 'epoch': 0.4}
{'loss': 0.6762, 'grad_norm': 0.14186422526836395, 'learning_rate': 8.75e-07, 'epoch': 0.44}
{'loss': 0.7024, 'grad_norm': 0.14609399437904358, 'learning_rate': 4.166666666666667e-08, 'epoch': 0.48}
{'train_runtime': 18478.1593, 'train_samples_per_second': 0.156, 'train_steps_per_second': 0.013, 'train_loss': 0.7309317668279012, 'epoch': 0.48}
