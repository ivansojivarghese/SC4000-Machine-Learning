[Step5] Distilling fold 0 using LLaMA-only OOF probs
{'loss': 5.2336, 'grad_norm': 58.783119201660156, 'learning_rate': 3.234451718494272e-05, 'epoch': 0.39}
{'loss': 5.0846, 'grad_norm': 78.31986236572266, 'learning_rate': 3.19353518821604e-05, 'epoch': 0.4}
{'loss': 5.1101, 'grad_norm': 85.34447479248047, 'learning_rate': 3.152618657937807e-05, 'epoch': 0.41}
{'loss': 5.2549, 'grad_norm': 23.401775360107422, 'learning_rate': 3.111702127659575e-05, 'epoch': 0.42}
{'loss': 4.7207, 'grad_norm': 24.190181732177734, 'learning_rate': 3.070785597381342e-05, 'epoch': 0.43}
{'loss': 5.3221, 'grad_norm': 21.328943252563477, 'learning_rate': 3.02986906710311e-05, 'epoch': 0.43}
{'loss': 5.1907, 'grad_norm': 75.4703369140625, 'learning_rate': 2.988952536824877e-05, 'epoch': 0.44}
{'loss': 4.9133, 'grad_norm': 82.50859069824219, 'learning_rate': 2.9480360065466452e-05, 'epoch': 0.45}
{'loss': 5.3695, 'grad_norm': 48.030067443847656, 'learning_rate': 2.9071194762684123e-05, 'epoch': 0.46}
{'loss': 4.7556, 'grad_norm': 16.975921630859375, 'learning_rate': 2.86620294599018e-05, 'epoch': 0.46}
{'loss': 5.3152, 'grad_norm': 16.71502685546875, 'learning_rate': 2.825286415711948e-05, 'epoch': 0.47}
{'loss': 5.114, 'grad_norm': 27.05744743347168, 'learning_rate': 2.7843698854337153e-05, 'epoch': 0.48}
{'loss': 4.8733, 'grad_norm': 26.74808120727539, 'learning_rate': 2.743453355155483e-05, 'epoch': 0.49}
{'loss': 5.2392, 'grad_norm': 40.54545211791992, 'learning_rate': 2.7025368248772502e-05, 'epoch': 0.49}
{'loss': 4.951, 'grad_norm': 48.71947479248047, 'learning_rate': 2.6616202945990183e-05, 'epoch': 0.5}
{'loss': 5.2511, 'grad_norm': 42.575382232666016, 'learning_rate': 2.6207037643207855e-05, 'epoch': 0.51}
{'loss': 4.9509, 'grad_norm': 25.275705337524414, 'learning_rate': 2.5797872340425532e-05, 'epoch': 0.52}
{'loss': 4.8575, 'grad_norm': 24.655420303344727, 'learning_rate': 2.538870703764321e-05, 'epoch': 0.53}
{'loss': 5.2176, 'grad_norm': 34.892066955566406, 'learning_rate': 2.4979541734860885e-05, 'epoch': 0.53}
{'loss': 4.5736, 'grad_norm': 34.923309326171875, 'learning_rate': 2.457037643207856e-05, 'epoch': 0.54}
{'loss': 5.4265, 'grad_norm': 36.62015151977539, 'learning_rate': 2.4161211129296237e-05, 'epoch': 0.55}
{'loss': 5.095, 'grad_norm': 30.124534606933594, 'learning_rate': 2.3752045826513915e-05, 'epoch': 0.56}
{'loss': 5.0892, 'grad_norm': 22.579570770263672, 'learning_rate': 2.334288052373159e-05, 'epoch': 0.56}
{'loss': 5.1202, 'grad_norm': 26.145822525024414, 'learning_rate': 2.2933715220949264e-05, 'epoch': 0.57}
{'loss': 4.6016, 'grad_norm': 28.762510299682617, 'learning_rate': 2.252454991816694e-05, 'epoch': 0.58}
{'loss': 5.1617, 'grad_norm': 74.08041381835938, 'learning_rate': 2.2115384615384616e-05, 'epoch': 0.59}
{'loss': 4.9346, 'grad_norm': 34.38613510131836, 'learning_rate': 2.170621931260229e-05, 'epoch': 0.6}
{'loss': 4.9297, 'grad_norm': 24.634294509887695, 'learning_rate': 2.129705400981997e-05, 'epoch': 0.6}
{'loss': 4.9392, 'grad_norm': 43.91292190551758, 'learning_rate': 2.0887888707037647e-05, 'epoch': 0.61}
{'loss': 4.6716, 'grad_norm': 49.45372009277344, 'learning_rate': 2.047872340425532e-05, 'epoch': 0.62}
{'loss': 5.4189, 'grad_norm': 48.89100646972656, 'learning_rate': 2.0069558101472996e-05, 'epoch': 0.63}
{'loss': 5.038, 'grad_norm': 47.965145111083984, 'learning_rate': 1.966039279869067e-05, 'epoch': 0.63}
{'loss': 4.9729, 'grad_norm': 35.005615234375, 'learning_rate': 1.9251227495908348e-05, 'epoch': 0.64}
{'loss': 5.0559, 'grad_norm': 28.499204635620117, 'learning_rate': 1.8842062193126022e-05, 'epoch': 0.65}
{'loss': 4.5942, 'grad_norm': 30.540205001831055, 'learning_rate': 1.84328968903437e-05, 'epoch': 0.66}
{'loss': 5.2226, 'grad_norm': 28.783203125, 'learning_rate': 1.8023731587561378e-05, 'epoch': 0.67}
{'loss': 4.8619, 'grad_norm': 18.668941497802734, 'learning_rate': 1.7614566284779053e-05, 'epoch': 0.67}
{'loss': 4.8188, 'grad_norm': 24.094324111938477, 'learning_rate': 1.7205400981996727e-05, 'epoch': 0.68}
{'loss': 5.1517, 'grad_norm': 34.152992248535156, 'learning_rate': 1.67962356792144e-05, 'epoch': 0.69}
{'loss': 4.8473, 'grad_norm': 23.559072494506836, 'learning_rate': 1.638707037643208e-05, 'epoch': 0.7}
{'loss': 4.9495, 'grad_norm': 60.68102264404297, 'learning_rate': 1.5977905073649754e-05, 'epoch': 0.7}
{'loss': 5.0723, 'grad_norm': 19.561073303222656, 'learning_rate': 1.556873977086743e-05, 'epoch': 0.71}
{'loss': 4.824, 'grad_norm': 23.275346755981445, 'learning_rate': 1.5159574468085108e-05, 'epoch': 0.72}
{'loss': 5.0855, 'grad_norm': 46.286197662353516, 'learning_rate': 1.4750409165302784e-05, 'epoch': 0.73}
{'loss': 4.5058, 'grad_norm': 37.60898208618164, 'learning_rate': 1.4341243862520459e-05, 'epoch': 0.73}
{'loss': 5.0883, 'grad_norm': 33.73245620727539, 'learning_rate': 1.3932078559738135e-05, 'epoch': 0.74}
{'loss': 4.9312, 'grad_norm': 19.538593292236328, 'learning_rate': 1.3522913256955811e-05, 'epoch': 0.75}
{'loss': 4.8751, 'grad_norm': 40.57658767700195, 'learning_rate': 1.3113747954173485e-05, 'epoch': 0.76}
{'loss': 5.0617, 'grad_norm': 17.746816635131836, 'learning_rate': 1.2704582651391162e-05, 'epoch': 0.77}
{'loss': 4.4111, 'grad_norm': 70.29171752929688, 'learning_rate': 1.2295417348608838e-05, 'epoch': 0.77}
{'loss': 5.1144, 'grad_norm': 32.36035919189453, 'learning_rate': 1.1886252045826514e-05, 'epoch': 0.78}
{'loss': 4.9099, 'grad_norm': 56.09429931640625, 'learning_rate': 1.147708674304419e-05, 'epoch': 0.79}
{'loss': 4.7522, 'grad_norm': 26.522220611572266, 'learning_rate': 1.1067921440261866e-05, 'epoch': 0.8}
{'loss': 5.0657, 'grad_norm': 38.47199630737305, 'learning_rate': 1.0658756137479543e-05, 'epoch': 0.8}
{'loss': 4.4767, 'grad_norm': 39.53116226196289, 'learning_rate': 1.0249590834697217e-05, 'epoch': 0.81}
{'loss': 4.9525, 'grad_norm': 48.07999038696289, 'learning_rate': 9.840425531914895e-06, 'epoch': 0.82}
{'loss': 4.7111, 'grad_norm': 57.051063537597656, 'learning_rate': 9.43126022913257e-06, 'epoch': 0.83}
{'loss': 4.6893, 'grad_norm': 20.027692794799805, 'learning_rate': 9.022094926350246e-06, 'epoch': 0.84}
{'loss': 4.9263, 'grad_norm': 68.39494323730469, 'learning_rate': 8.612929623567922e-06, 'epoch': 0.84}
{'loss': 4.5951, 'grad_norm': 24.303844451904297, 'learning_rate': 8.203764320785598e-06, 'epoch': 0.85}
{'loss': 5.0358, 'grad_norm': 44.4661750793457, 'learning_rate': 7.794599018003274e-06, 'epoch': 0.86}
{'loss': 4.9183, 'grad_norm': 35.70107650756836, 'learning_rate': 7.3854337152209495e-06, 'epoch': 0.87}
{'loss': 4.5161, 'grad_norm': 18.127601623535156, 'learning_rate': 6.976268412438626e-06, 'epoch': 0.87}
{'loss': 5.0062, 'grad_norm': 67.85773468017578, 'learning_rate': 6.567103109656302e-06, 'epoch': 0.88}
{'loss': 4.2992, 'grad_norm': 21.626670837402344, 'learning_rate': 6.157937806873977e-06, 'epoch': 0.89}
{'loss': 4.9903, 'grad_norm': 61.894683837890625, 'learning_rate': 5.748772504091653e-06, 'epoch': 0.9}
{'loss': 4.6479, 'grad_norm': 35.230934143066406, 'learning_rate': 5.3396072013093295e-06, 'epoch': 0.9}
{'loss': 4.9099, 'grad_norm': 42.724609375, 'learning_rate': 4.930441898527005e-06, 'epoch': 0.91}
{'loss': 4.8634, 'grad_norm': 38.66281509399414, 'learning_rate': 4.521276595744681e-06, 'epoch': 0.92}
{'loss': 4.6652, 'grad_norm': 20.493816375732422, 'learning_rate': 4.112111292962357e-06, 'epoch': 0.93}
{'loss': 4.9627, 'grad_norm': 26.433067321777344, 'learning_rate': 3.702945990180033e-06, 'epoch': 0.94}
{'loss': 5.0382, 'grad_norm': 39.14720916748047, 'learning_rate': 3.2937806873977087e-06, 'epoch': 0.94}
{'loss': 4.6728, 'grad_norm': 50.59090805053711, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.95}
{'loss': 4.9263, 'grad_norm': 17.801942825317383, 'learning_rate': 2.4754500818330606e-06, 'epoch': 0.96}
{'loss': 4.7547, 'grad_norm': 24.227052688598633, 'learning_rate': 2.066284779050737e-06, 'epoch': 0.97}
{'loss': 4.9892, 'grad_norm': 15.489584922790527, 'learning_rate': 1.6571194762684126e-06, 'epoch': 0.97}
{'loss': 4.7551, 'grad_norm': 26.215618133544922, 'learning_rate': 1.2479541734860885e-06, 'epoch': 0.98}
{'loss': 4.8722, 'grad_norm': 52.262977600097656, 'learning_rate': 8.387888707037644e-07, 'epoch': 0.99}
{'loss': 4.8905, 'grad_norm': 25.18559455871582, 'learning_rate': 4.2962356792144027e-07, 'epoch': 1.0}
{'loss': 4.7099, 'grad_norm': 13.137129783630371, 'learning_rate': 2.0458265139116204e-08, 'epoch': 1.01}
{'train_runtime': 18810.9971, 'train_samples_per_second': 2.211, 'train_steps_per_second': 0.138, 'train_loss': 3.0359331072293796, 'epoch': 1.01}
{'eval': {'eval_loss': 0.2961174249649048, 'eval_log_loss': 0.97530497916178, 'eval_accuracy': 0.5131550337029789, 'eval_runtime': 1021.1871, 'eval_samples_per_second': 4.504, 'eval_steps_per_second': 4.504, 'epoch': 1.0054129814895365}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
