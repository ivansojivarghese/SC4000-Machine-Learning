[Step5] Distilling fold 1 using LLaMA-only OOF probs
{'loss': 9.8927, 'grad_norm': 169.66354370117188, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.01}
{'loss': 8.5464, 'grad_norm': 102.79878997802734, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
{'loss': 6.8113, 'grad_norm': 91.1144027709961, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.02}
{'loss': 7.433, 'grad_norm': 77.44891357421875, 'learning_rate': 4.898936170212766e-05, 'epoch': 0.03}
{'loss': 6.1843, 'grad_norm': 98.48478698730469, 'learning_rate': 4.792553191489362e-05, 'epoch': 0.04}
{'loss': 6.4433, 'grad_norm': 86.3523941040039, 'learning_rate': 4.686170212765958e-05, 'epoch': 0.05}
{'loss': 5.9806, 'grad_norm': 103.41539764404297, 'learning_rate': 4.579787234042554e-05, 'epoch': 0.05}
{'loss': 5.7507, 'grad_norm': 82.2862319946289, 'learning_rate': 4.473404255319149e-05, 'epoch': 0.06}
{'loss': 5.634, 'grad_norm': 57.03568649291992, 'learning_rate': 4.367021276595745e-05, 'epoch': 0.07}
{'loss': 5.6306, 'grad_norm': 51.14066696166992, 'learning_rate': 4.2606382978723406e-05, 'epoch': 0.08}
{'loss': 5.5848, 'grad_norm': 39.798133850097656, 'learning_rate': 4.1542553191489364e-05, 'epoch': 0.09}
{'loss': 5.6738, 'grad_norm': 96.97410583496094, 'learning_rate': 4.047872340425532e-05, 'epoch': 0.09}
{'loss': 5.3802, 'grad_norm': 18.008419036865234, 'learning_rate': 3.941489361702128e-05, 'epoch': 0.1}
{'loss': 5.4103, 'grad_norm': 62.26047134399414, 'learning_rate': 3.835106382978724e-05, 'epoch': 0.11}
{'loss': 5.4574, 'grad_norm': 42.8752555847168, 'learning_rate': 3.728723404255319e-05, 'epoch': 0.12}
{'loss': 5.5037, 'grad_norm': 90.9966812133789, 'learning_rate': 3.622340425531915e-05, 'epoch': 0.12}
{'loss': 5.4329, 'grad_norm': 34.33433151245117, 'learning_rate': 3.515957446808511e-05, 'epoch': 0.13}
{'loss': 5.3252, 'grad_norm': 24.082748413085938, 'learning_rate': 3.4095744680851066e-05, 'epoch': 0.14}
{'loss': 5.3562, 'grad_norm': 75.17852783203125, 'learning_rate': 3.3031914893617025e-05, 'epoch': 0.15}
{'loss': 5.2153, 'grad_norm': 57.84050750732422, 'learning_rate': 3.1968085106382976e-05, 'epoch': 0.15}
{'loss': 5.7626, 'grad_norm': 34.28482437133789, 'learning_rate': 3.090425531914894e-05, 'epoch': 0.16}
{'loss': 5.3833, 'grad_norm': 39.00358200073242, 'learning_rate': 2.9840425531914897e-05, 'epoch': 0.17}
{'loss': 5.1949, 'grad_norm': 20.818965911865234, 'learning_rate': 2.877659574468085e-05, 'epoch': 0.18}
{'loss': 5.2985, 'grad_norm': 48.30908966064453, 'learning_rate': 2.7712765957446813e-05, 'epoch': 0.19}
{'loss': 5.101, 'grad_norm': 69.50714874267578, 'learning_rate': 2.664893617021277e-05, 'epoch': 0.19}
{'loss': 5.4088, 'grad_norm': 53.12019348144531, 'learning_rate': 2.5585106382978723e-05, 'epoch': 0.2}
{'loss': 5.1739, 'grad_norm': 49.50366973876953, 'learning_rate': 2.4521276595744682e-05, 'epoch': 0.21}
{'loss': 5.1425, 'grad_norm': 49.545677185058594, 'learning_rate': 2.345744680851064e-05, 'epoch': 0.22}
{'loss': 5.3177, 'grad_norm': 62.44621276855469, 'learning_rate': 2.23936170212766e-05, 'epoch': 0.22}
{'loss': 4.9633, 'grad_norm': 44.66704559326172, 'learning_rate': 2.1329787234042554e-05, 'epoch': 0.23}
{'loss': 5.323, 'grad_norm': 20.148954391479492, 'learning_rate': 2.0265957446808512e-05, 'epoch': 0.24}
{'loss': 5.1561, 'grad_norm': 35.21928787231445, 'learning_rate': 1.920212765957447e-05, 'epoch': 0.25}
{'loss': 5.0725, 'grad_norm': 44.467933654785156, 'learning_rate': 1.8138297872340425e-05, 'epoch': 0.26}
{'loss': 5.2447, 'grad_norm': 43.148712158203125, 'learning_rate': 1.7074468085106384e-05, 'epoch': 0.26}
{'loss': 4.9969, 'grad_norm': 37.977081298828125, 'learning_rate': 1.6010638297872342e-05, 'epoch': 0.27}
{'loss': 5.2234, 'grad_norm': 22.956069946289062, 'learning_rate': 1.4946808510638299e-05, 'epoch': 0.28}
{'loss': 5.0777, 'grad_norm': 24.786651611328125, 'learning_rate': 1.3882978723404256e-05, 'epoch': 0.29}
{'loss': 5.1277, 'grad_norm': 71.66417694091797, 'learning_rate': 1.2819148936170214e-05, 'epoch': 0.29}
{'loss': 5.2751, 'grad_norm': 33.993858337402344, 'learning_rate': 1.175531914893617e-05, 'epoch': 0.3}
{'loss': 4.7705, 'grad_norm': 27.37175941467285, 'learning_rate': 1.0691489361702128e-05, 'epoch': 0.31}
{'loss': 5.1946, 'grad_norm': 16.859567642211914, 'learning_rate': 9.627659574468086e-06, 'epoch': 0.32}
{'loss': 5.0196, 'grad_norm': 28.389217376708984, 'learning_rate': 8.563829787234043e-06, 'epoch': 0.32}
{'loss': 5.0453, 'grad_norm': 39.657222747802734, 'learning_rate': 7.5e-06, 'epoch': 0.33}
{'loss': 5.1893, 'grad_norm': 48.195308685302734, 'learning_rate': 6.436170212765958e-06, 'epoch': 0.34}
{'loss': 4.6791, 'grad_norm': 26.6109561920166, 'learning_rate': 5.372340425531915e-06, 'epoch': 0.35}
{'loss': 5.1501, 'grad_norm': 67.33528137207031, 'learning_rate': 4.308510638297873e-06, 'epoch': 0.36}
{'loss': 4.9764, 'grad_norm': 55.09800720214844, 'learning_rate': 3.2446808510638296e-06, 'epoch': 0.36}
{'loss': 4.8481, 'grad_norm': 14.449454307556152, 'learning_rate': 2.1808510638297876e-06, 'epoch': 0.37}
{'loss': 5.1831, 'grad_norm': 58.28413772583008, 'learning_rate': 1.1170212765957447e-06, 'epoch': 0.38}
{'loss': 4.6794, 'grad_norm': 19.79457664489746, 'learning_rate': 5.319148936170213e-08, 'epoch': 0.39}
{'train_runtime': 11745.3232, 'train_samples_per_second': 1.362, 'train_steps_per_second': 0.085, 'train_loss': 5.53252010345459, 'epoch': 0.39}
{'eval': {'eval_loss': 0.31086844205856323, 'eval_log_loss': 1.0288713302453594, 'eval_accuracy': 0.45792563600782776, 'eval_runtime': 1018.654, 'eval_samples_per_second': 4.515, 'eval_steps_per_second': 4.515, 'epoch': 0.3866415349668938}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
