[Step1] Resolving SCRATCH_BASE...
[Debug] SCRATCH_BASE resolved to: /scratch-shared/tc1proj005
[Step1] Working dir: /home/UG/ivansoji001/exported-assets_sc4000
[Step1] Home dir: /home/UG/ivansoji001
[Step1] SCRATCH_BASE: /scratch-shared/tc1proj005
[Step1] HF_HOME: /scratch-shared/tc1proj005
[Step1] HUGGINGFACE_HUB_CACHE: /scratch-shared/tc1proj005
[Step1] HF_DATASETS_CACHE: /scratch-shared/tc1proj005
[Step1] TRANSFORMERS_CACHE: /scratch-shared/tc1proj005
[Step1] UT_DATA: data/ultrafeedback.csv | EPOCHS: 1 | LR: 1e-5 | MAXLEN: 512 | PER_DEVICE_BS: 1 | MAX_STEPS: -1 | LORA_R: 16 | LORA_ALPHA: 32
[Step1][AutoCap] Projected 500 steps (~6h) exceeds budget 6h; capping to 450.
[Step1][Force] Overriding max steps to 240 (ignoring POSTPRE_MAX_STEPS=450)
[Step1] Deps OK
[Step1] Online mode enabled (TRANSFORMERS_OFFLINE=0)
[Step1] Downloading tokenizer files for Qwen/Qwen2.5-14B...
[Step1] Tokenizer snapshot at: /scratch-shared/tc1proj005/models--Qwen--Qwen2.5-14B/snapshots/97e1e76335b7017d8f67c08a19d103c0504298c9
[Step1] Prefetched tokenizer: vocab size 151665
[Step1] Starting post-pretraining (LoRA/QLoRA)
[Step1] Selected stage: qwen
[Step1] Disk check for /scratch-shared/tc1proj005: need 180G, avail 85391G
[Step1] Cleaning HF cache at /scratch-shared/tc1proj005
[Step1] Skipping cache clean to avoid wiping SCRATCH_BASE root
[Step1] Qwen/Qwen2.5-14B QLoRA: starting
[Step1] Qwen args: --base-model Qwen/Qwen2.5-14B --output-dir /scratch-shared/tc1proj005/post_pretrain_qwen2-72b_lora --data-path data/ultrafeedback.csv --tokenizer-path Qwen/Qwen2.5-14B --bf16 --qlora --epochs 1 --lr 1e-5 --max-length 512 --r 16 --lora-alpha 32 --per-device-batch 1 --grad-accum 12 --subset-size 6000 --max-steps 240
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged
[Step1] Stage 'qwen' completed
