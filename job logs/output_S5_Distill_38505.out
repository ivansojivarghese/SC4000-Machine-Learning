[Step5] Distilling fold 2 using LLaMA-only OOF probs
{'loss': 4.8608, 'grad_norm': 75.49067687988281, 'learning_rate': 2.0022796352583586e-05, 'epoch': 1.01}
{'loss': 4.5402, 'grad_norm': 25.00558090209961, 'learning_rate': 1.976950354609929e-05, 'epoch': 1.02}
{'loss': 5.1884, 'grad_norm': 55.56452178955078, 'learning_rate': 1.9516210739614996e-05, 'epoch': 1.03}
{'loss': 4.8094, 'grad_norm': 20.351089477539062, 'learning_rate': 1.92629179331307e-05, 'epoch': 1.04}
{'loss': 4.7392, 'grad_norm': 19.021812438964844, 'learning_rate': 1.9009625126646405e-05, 'epoch': 1.04}
{'loss': 4.8247, 'grad_norm': 18.778898239135742, 'learning_rate': 1.875633232016211e-05, 'epoch': 1.05}
{'loss': 4.5896, 'grad_norm': 37.38905715942383, 'learning_rate': 1.850303951367781e-05, 'epoch': 1.06}
{'loss': 4.8912, 'grad_norm': 45.302001953125, 'learning_rate': 1.8249746707193515e-05, 'epoch': 1.07}
{'loss': 4.9145, 'grad_norm': 53.195011138916016, 'learning_rate': 1.799645390070922e-05, 'epoch': 1.07}
{'loss': 4.7155, 'grad_norm': 24.5560245513916, 'learning_rate': 1.7743161094224924e-05, 'epoch': 1.08}
{'loss': 4.8845, 'grad_norm': 34.45242691040039, 'learning_rate': 1.748986828774063e-05, 'epoch': 1.09}
{'loss': 4.4044, 'grad_norm': 37.851348876953125, 'learning_rate': 1.7236575481256333e-05, 'epoch': 1.1}
{'loss': 4.769, 'grad_norm': 16.663780212402344, 'learning_rate': 1.6983282674772038e-05, 'epoch': 1.11}
{'loss': 4.6654, 'grad_norm': 52.75697708129883, 'learning_rate': 1.672998986828774e-05, 'epoch': 1.11}
{'loss': 4.6943, 'grad_norm': 60.29459762573242, 'learning_rate': 1.6476697061803443e-05, 'epoch': 1.12}
{'loss': 4.7463, 'grad_norm': 34.55078125, 'learning_rate': 1.6223404255319148e-05, 'epoch': 1.13}
{'loss': 4.1437, 'grad_norm': 22.311864852905273, 'learning_rate': 1.5970111448834852e-05, 'epoch': 1.14}
{'loss': 5.0217, 'grad_norm': 23.240116119384766, 'learning_rate': 1.571681864235056e-05, 'epoch': 1.14}
{'loss': 4.4039, 'grad_norm': 22.52313232421875, 'learning_rate': 1.5463525835866265e-05, 'epoch': 1.15}
{'loss': 4.5448, 'grad_norm': 18.578529357910156, 'learning_rate': 1.5210233029381964e-05, 'epoch': 1.16}
{'loss': 4.8526, 'grad_norm': 26.456552505493164, 'learning_rate': 1.4956940222897669e-05, 'epoch': 1.17}
{'loss': 4.1169, 'grad_norm': 28.455472946166992, 'learning_rate': 1.4703647416413373e-05, 'epoch': 1.18}
{'loss': 5.0321, 'grad_norm': 26.296831130981445, 'learning_rate': 1.4450354609929078e-05, 'epoch': 1.18}
{'loss': 4.8798, 'grad_norm': 17.223125457763672, 'learning_rate': 1.4197061803444783e-05, 'epoch': 1.19}
{'loss': 4.6533, 'grad_norm': 19.63561248779297, 'learning_rate': 1.3943768996960487e-05, 'epoch': 1.2}
{'loss': 4.8957, 'grad_norm': 54.399723052978516, 'learning_rate': 1.3690476190476192e-05, 'epoch': 1.21}
{'loss': 3.8743, 'grad_norm': 40.20542526245117, 'learning_rate': 1.3437183383991894e-05, 'epoch': 1.21}
{'loss': 4.7919, 'grad_norm': 37.433319091796875, 'learning_rate': 1.3183890577507599e-05, 'epoch': 1.22}
{'loss': 4.3216, 'grad_norm': 43.62470626831055, 'learning_rate': 1.2930597771023304e-05, 'epoch': 1.23}
{'loss': 4.4673, 'grad_norm': 16.665658950805664, 'learning_rate': 1.2677304964539008e-05, 'epoch': 1.24}
{'loss': 4.831, 'grad_norm': 72.90005493164062, 'learning_rate': 1.2424012158054713e-05, 'epoch': 1.24}
{'loss': 4.2321, 'grad_norm': 30.84900665283203, 'learning_rate': 1.2170719351570415e-05, 'epoch': 1.25}
{'loss': 4.8479, 'grad_norm': 48.64030075073242, 'learning_rate': 1.191742654508612e-05, 'epoch': 1.26}
{'loss': 4.3587, 'grad_norm': 33.24748229980469, 'learning_rate': 1.1664133738601825e-05, 'epoch': 1.27}
{'loss': 4.6795, 'grad_norm': 21.246685028076172, 'learning_rate': 1.1410840932117527e-05, 'epoch': 1.28}
{'loss': 4.7233, 'grad_norm': 47.12651824951172, 'learning_rate': 1.1157548125633232e-05, 'epoch': 1.28}
{'loss': 4.0319, 'grad_norm': 49.835182189941406, 'learning_rate': 1.0904255319148937e-05, 'epoch': 1.29}
{'loss': 5.193, 'grad_norm': 46.866676330566406, 'learning_rate': 1.0650962512664641e-05, 'epoch': 1.3}
{'loss': 4.6589, 'grad_norm': 53.80781173706055, 'learning_rate': 1.0397669706180346e-05, 'epoch': 1.31}
{'loss': 4.5109, 'grad_norm': 34.47504425048828, 'learning_rate': 1.014437689969605e-05, 'epoch': 1.31}
{'loss': 4.8457, 'grad_norm': 49.27951431274414, 'learning_rate': 9.891084093211753e-06, 'epoch': 1.32}
{'loss': 4.0585, 'grad_norm': 23.33005714416504, 'learning_rate': 9.637791286727458e-06, 'epoch': 1.33}
{'loss': 5.0715, 'grad_norm': 29.117820739746094, 'learning_rate': 9.384498480243162e-06, 'epoch': 1.34}
{'loss': 4.5974, 'grad_norm': 50.79248046875, 'learning_rate': 9.131205673758867e-06, 'epoch': 1.35}
{'loss': 4.6196, 'grad_norm': 27.420879364013672, 'learning_rate': 8.87791286727457e-06, 'epoch': 1.35}
{'loss': 5.1175, 'grad_norm': 25.234716415405273, 'learning_rate': 8.624620060790274e-06, 'epoch': 1.36}
{'loss': 4.273, 'grad_norm': 66.84536743164062, 'learning_rate': 8.371327254305979e-06, 'epoch': 1.37}
{'loss': 5.1872, 'grad_norm': 63.12860107421875, 'learning_rate': 8.118034447821681e-06, 'epoch': 1.38}
{'loss': 4.6745, 'grad_norm': 32.57865524291992, 'learning_rate': 7.864741641337386e-06, 'epoch': 1.38}
{'loss': 4.3959, 'grad_norm': 54.86893844604492, 'learning_rate': 7.611448834853091e-06, 'epoch': 1.39}
{'loss': 4.7116, 'grad_norm': 23.566577911376953, 'learning_rate': 7.358156028368794e-06, 'epoch': 1.4}
{'loss': 3.2458, 'grad_norm': 19.03424072265625, 'learning_rate': 7.104863221884499e-06, 'epoch': 1.41}
{'loss': 4.6008, 'grad_norm': 41.515071868896484, 'learning_rate': 6.851570415400203e-06, 'epoch': 1.41}
{'loss': 4.3104, 'grad_norm': 33.05814743041992, 'learning_rate': 6.598277608915906e-06, 'epoch': 1.42}
{'loss': 4.7215, 'grad_norm': 50.10868453979492, 'learning_rate': 6.344984802431611e-06, 'epoch': 1.43}
{'loss': 5.1829, 'grad_norm': 60.598243713378906, 'learning_rate': 6.091691995947316e-06, 'epoch': 1.44}
{'loss': 3.4408, 'grad_norm': 33.51473617553711, 'learning_rate': 5.83839918946302e-06, 'epoch': 1.45}
{'loss': 4.6641, 'grad_norm': 25.459178924560547, 'learning_rate': 5.5851063829787235e-06, 'epoch': 1.45}
{'loss': 4.0473, 'grad_norm': 40.51984786987305, 'learning_rate': 5.331813576494428e-06, 'epoch': 1.46}
{'loss': 4.4804, 'grad_norm': 20.421531677246094, 'learning_rate': 5.078520770010132e-06, 'epoch': 1.47}
{'loss': 4.7598, 'grad_norm': 56.84092712402344, 'learning_rate': 4.825227963525836e-06, 'epoch': 1.48}
{'loss': 3.8803, 'grad_norm': 26.63229751586914, 'learning_rate': 4.57193515704154e-06, 'epoch': 1.48}
{'loss': 4.4813, 'grad_norm': 52.778751373291016, 'learning_rate': 4.3186423505572445e-06, 'epoch': 1.49}
{'loss': 4.0826, 'grad_norm': 35.44241714477539, 'learning_rate': 4.065349544072949e-06, 'epoch': 1.5}
{'loss': 4.3698, 'grad_norm': 21.65673828125, 'learning_rate': 3.8120567375886527e-06, 'epoch': 1.51}
{'loss': 4.9082, 'grad_norm': 52.65668487548828, 'learning_rate': 3.5587639311043564e-06, 'epoch': 1.52}
{'loss': 3.4402, 'grad_norm': 40.400001525878906, 'learning_rate': 3.305471124620061e-06, 'epoch': 1.52}
{'loss': 4.6189, 'grad_norm': 42.179386138916016, 'learning_rate': 3.052178318135765e-06, 'epoch': 1.53}
{'loss': 4.0093, 'grad_norm': 30.05498504638672, 'learning_rate': 2.798885511651469e-06, 'epoch': 1.54}
{'loss': 4.1324, 'grad_norm': 41.7008171081543, 'learning_rate': 2.5455927051671733e-06, 'epoch': 1.55}
{'loss': 4.504, 'grad_norm': 40.17513656616211, 'learning_rate': 2.2922998986828774e-06, 'epoch': 1.55}
{'loss': 3.7748, 'grad_norm': 30.685583114624023, 'learning_rate': 2.039007092198582e-06, 'epoch': 1.56}
{'loss': 4.5801, 'grad_norm': 39.12769317626953, 'learning_rate': 1.7857142857142857e-06, 'epoch': 1.57}
{'loss': 4.2652, 'grad_norm': 29.24038314819336, 'learning_rate': 1.53242147922999e-06, 'epoch': 1.58}
{'loss': 4.237, 'grad_norm': 27.556209564208984, 'learning_rate': 1.2791286727456941e-06, 'epoch': 1.58}
{'loss': 4.4925, 'grad_norm': 43.043277740478516, 'learning_rate': 1.0258358662613983e-06, 'epoch': 1.59}
{'loss': 3.9011, 'grad_norm': 19.394147872924805, 'learning_rate': 7.725430597771024e-07, 'epoch': 1.6}
{'loss': 4.6931, 'grad_norm': 39.59522247314453, 'learning_rate': 5.192502532928066e-07, 'epoch': 1.61}
{'loss': 4.1082, 'grad_norm': 46.7867317199707, 'learning_rate': 2.6595744680851066e-07, 'epoch': 1.62}
{'loss': 4.1306, 'grad_norm': 33.12485122680664, 'learning_rate': 1.2664640324214793e-08, 'epoch': 1.62}
{'train_runtime': 18727.3986, 'train_samples_per_second': 3.588, 'train_steps_per_second': 0.224, 'train_loss': 1.7234267952328637, 'epoch': 1.62}
{'eval': {'eval_loss': 0.3092783987522125, 'eval_log_loss': 1.0125392958830897, 'eval_accuracy': 0.49967384213959554, 'eval_runtime': 1022.1596, 'eval_samples_per_second': 4.499, 'eval_steps_per_second': 4.499, 'epoch': 1.6236377256361307}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
