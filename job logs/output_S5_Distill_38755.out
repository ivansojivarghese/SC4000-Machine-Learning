[Step5] Distilling fold 2 using LLaMA-only OOF probs
{'loss': 4.6455, 'grad_norm': 66.91410827636719, 'learning_rate': 1.4499266324284665e-05, 'epoch': 1.63}
{'loss': 3.6976, 'grad_norm': 48.288753509521484, 'learning_rate': 1.4315847395451212e-05, 'epoch': 1.64}
{'loss': 4.8786, 'grad_norm': 32.2287712097168, 'learning_rate': 1.4132428466617756e-05, 'epoch': 1.65}
{'loss': 4.2959, 'grad_norm': 62.33889389038086, 'learning_rate': 1.3949009537784299e-05, 'epoch': 1.65}
{'loss': 4.0604, 'grad_norm': 47.2105827331543, 'learning_rate': 1.3765590608950846e-05, 'epoch': 1.66}
{'loss': 4.4826, 'grad_norm': 36.13532638549805, 'learning_rate': 1.358217168011739e-05, 'epoch': 1.67}
{'loss': 3.7619, 'grad_norm': 35.6705436706543, 'learning_rate': 1.3398752751283933e-05, 'epoch': 1.68}
{'loss': 4.3069, 'grad_norm': 37.104976654052734, 'learning_rate': 1.3215333822450476e-05, 'epoch': 1.69}
{'loss': 4.0719, 'grad_norm': 61.07640838623047, 'learning_rate': 1.3031914893617023e-05, 'epoch': 1.69}
{'loss': 4.1428, 'grad_norm': 44.12091827392578, 'learning_rate': 1.2848495964783567e-05, 'epoch': 1.7}
{'loss': 4.8689, 'grad_norm': 37.756744384765625, 'learning_rate': 1.266507703595011e-05, 'epoch': 1.71}
{'loss': 3.6693, 'grad_norm': 30.0579833984375, 'learning_rate': 1.2481658107116655e-05, 'epoch': 1.72}
{'loss': 4.3828, 'grad_norm': 52.386138916015625, 'learning_rate': 1.22982391782832e-05, 'epoch': 1.72}
{'loss': 4.5269, 'grad_norm': 72.48007202148438, 'learning_rate': 1.2114820249449744e-05, 'epoch': 1.73}
{'loss': 4.3489, 'grad_norm': 26.359067916870117, 'learning_rate': 1.1931401320616289e-05, 'epoch': 1.74}
{'loss': 4.5766, 'grad_norm': 43.01655197143555, 'learning_rate': 1.1747982391782832e-05, 'epoch': 1.75}
{'loss': 3.7649, 'grad_norm': 40.27719497680664, 'learning_rate': 1.1564563462949376e-05, 'epoch': 1.76}
{'loss': 4.7052, 'grad_norm': 35.197818756103516, 'learning_rate': 1.1381144534115921e-05, 'epoch': 1.76}
{'loss': 4.4727, 'grad_norm': 47.65662384033203, 'learning_rate': 1.1197725605282464e-05, 'epoch': 1.77}
{'loss': 4.6036, 'grad_norm': 24.755605697631836, 'learning_rate': 1.101430667644901e-05, 'epoch': 1.78}
{'loss': 4.6652, 'grad_norm': 58.351707458496094, 'learning_rate': 1.0830887747615555e-05, 'epoch': 1.79}
{'loss': 3.5277, 'grad_norm': 34.91561508178711, 'learning_rate': 1.0647468818782098e-05, 'epoch': 1.79}
{'loss': 4.8209, 'grad_norm': 40.14801025390625, 'learning_rate': 1.0464049889948643e-05, 'epoch': 1.8}
{'loss': 4.1929, 'grad_norm': 34.08087158203125, 'learning_rate': 1.0280630961115188e-05, 'epoch': 1.81}
{'loss': 4.4666, 'grad_norm': 67.14451599121094, 'learning_rate': 1.0097212032281732e-05, 'epoch': 1.82}
{'loss': 4.7042, 'grad_norm': 46.44322967529297, 'learning_rate': 9.913793103448277e-06, 'epoch': 1.82}
{'loss': 3.7047, 'grad_norm': 22.30190658569336, 'learning_rate': 9.73037417461482e-06, 'epoch': 1.83}
{'loss': 4.9317, 'grad_norm': 37.78815841674805, 'learning_rate': 9.546955245781365e-06, 'epoch': 1.84}
{'loss': 4.1627, 'grad_norm': 61.12287139892578, 'learning_rate': 9.36353631694791e-06, 'epoch': 1.85}
{'loss': 4.3303, 'grad_norm': 27.98579216003418, 'learning_rate': 9.180117388114454e-06, 'epoch': 1.86}
{'loss': 4.834, 'grad_norm': 34.57452392578125, 'learning_rate': 8.996698459281e-06, 'epoch': 1.86}
{'loss': 3.7415, 'grad_norm': 31.62059783935547, 'learning_rate': 8.813279530447543e-06, 'epoch': 1.87}
{'loss': 4.7909, 'grad_norm': 28.80760383605957, 'learning_rate': 8.629860601614088e-06, 'epoch': 1.88}
{'loss': 4.2191, 'grad_norm': 43.96454620361328, 'learning_rate': 8.446441672780631e-06, 'epoch': 1.89}
{'loss': 4.4149, 'grad_norm': 50.827518463134766, 'learning_rate': 8.263022743947176e-06, 'epoch': 1.89}
{'loss': 4.7435, 'grad_norm': 47.598777770996094, 'learning_rate': 8.07960381511372e-06, 'epoch': 1.9}
{'loss': 4.3433, 'grad_norm': 63.209312438964844, 'learning_rate': 7.896184886280263e-06, 'epoch': 1.91}
{'loss': 5.0675, 'grad_norm': 71.88554382324219, 'learning_rate': 7.712765957446808e-06, 'epoch': 1.92}
{'loss': 4.4487, 'grad_norm': 65.92597961425781, 'learning_rate': 7.5293470286133535e-06, 'epoch': 1.93}
{'loss': 4.5903, 'grad_norm': 31.467981338500977, 'learning_rate': 7.345928099779898e-06, 'epoch': 1.93}
{'loss': 4.8353, 'grad_norm': 27.18778419494629, 'learning_rate': 7.162509170946442e-06, 'epoch': 1.94}
{'loss': 4.3431, 'grad_norm': 41.50761795043945, 'learning_rate': 6.9790902421129855e-06, 'epoch': 1.95}
{'loss': 5.0468, 'grad_norm': 42.07729721069336, 'learning_rate': 6.795671313279531e-06, 'epoch': 1.96}
{'loss': 4.5683, 'grad_norm': 56.52760696411133, 'learning_rate': 6.612252384446076e-06, 'epoch': 1.96}
{'loss': 4.5996, 'grad_norm': 25.477800369262695, 'learning_rate': 6.428833455612619e-06, 'epoch': 1.97}
{'loss': 4.9673, 'grad_norm': 21.614276885986328, 'learning_rate': 6.245414526779164e-06, 'epoch': 1.98}
{'loss': 4.3883, 'grad_norm': 34.4908447265625, 'learning_rate': 6.061995597945709e-06, 'epoch': 1.99}
{'loss': 4.929, 'grad_norm': 49.45127487182617, 'learning_rate': 5.878576669112253e-06, 'epoch': 1.99}
{'loss': 4.4835, 'grad_norm': 58.430973052978516, 'learning_rate': 5.695157740278797e-06, 'epoch': 2.0}
{'loss': 4.5191, 'grad_norm': 26.7020263671875, 'learning_rate': 5.5117388114453415e-06, 'epoch': 2.01}
{'loss': 3.5336, 'grad_norm': 38.931610107421875, 'learning_rate': 5.328319882611886e-06, 'epoch': 2.02}
{'loss': 4.212, 'grad_norm': 38.193050384521484, 'learning_rate': 5.14490095377843e-06, 'epoch': 2.03}
{'loss': 4.215, 'grad_norm': 41.242942810058594, 'learning_rate': 4.961482024944974e-06, 'epoch': 2.03}
{'loss': 3.4153, 'grad_norm': 33.68057632446289, 'learning_rate': 4.778063096111519e-06, 'epoch': 2.04}
{'loss': 4.3864, 'grad_norm': 37.744384765625, 'learning_rate': 4.594644167278064e-06, 'epoch': 2.05}
{'loss': 3.2688, 'grad_norm': 33.89020538330078, 'learning_rate': 4.411225238444608e-06, 'epoch': 2.06}
{'loss': 4.1403, 'grad_norm': 61.275909423828125, 'learning_rate': 4.227806309611152e-06, 'epoch': 2.06}
{'loss': 4.0104, 'grad_norm': 58.567718505859375, 'learning_rate': 4.044387380777697e-06, 'epoch': 2.07}
{'loss': 3.2796, 'grad_norm': 49.48857498168945, 'learning_rate': 3.860968451944241e-06, 'epoch': 2.08}
{'loss': 4.2715, 'grad_norm': 52.19542694091797, 'learning_rate': 3.6775495231107857e-06, 'epoch': 2.09}
{'loss': 3.2904, 'grad_norm': 79.0255126953125, 'learning_rate': 3.4941305942773295e-06, 'epoch': 2.1}
{'loss': 3.9042, 'grad_norm': 32.26668930053711, 'learning_rate': 3.310711665443874e-06, 'epoch': 2.1}
{'loss': 4.1703, 'grad_norm': 33.79456329345703, 'learning_rate': 3.127292736610418e-06, 'epoch': 2.11}
{'loss': 3.1843, 'grad_norm': 55.95399856567383, 'learning_rate': 2.943873807776963e-06, 'epoch': 2.12}
{'loss': 4.3838, 'grad_norm': 40.53031539916992, 'learning_rate': 2.760454878943507e-06, 'epoch': 2.13}
{'loss': 3.3057, 'grad_norm': 84.54257202148438, 'learning_rate': 2.5770359501100514e-06, 'epoch': 2.13}
{'loss': 3.9923, 'grad_norm': 30.575855255126953, 'learning_rate': 2.3936170212765957e-06, 'epoch': 2.14}
{'loss': 4.114, 'grad_norm': 81.09954071044922, 'learning_rate': 2.21019809244314e-06, 'epoch': 2.15}
{'loss': 3.3671, 'grad_norm': 46.210792541503906, 'learning_rate': 2.0267791636096847e-06, 'epoch': 2.16}
{'loss': 4.2731, 'grad_norm': 41.287601470947266, 'learning_rate': 1.8433602347762288e-06, 'epoch': 2.17}
{'loss': 3.3805, 'grad_norm': 44.40351104736328, 'learning_rate': 1.6599413059427735e-06, 'epoch': 2.17}
{'loss': 3.889, 'grad_norm': 77.16503143310547, 'learning_rate': 1.4765223771093178e-06, 'epoch': 2.18}
{'loss': 4.0296, 'grad_norm': 37.484413146972656, 'learning_rate': 1.293103448275862e-06, 'epoch': 2.19}
{'loss': 3.299, 'grad_norm': 48.814090728759766, 'learning_rate': 1.1096845194424066e-06, 'epoch': 2.2}
{'loss': 4.072, 'grad_norm': 47.19612503051758, 'learning_rate': 9.26265590608951e-07, 'epoch': 2.2}
{'loss': 3.3011, 'grad_norm': 28.23904037475586, 'learning_rate': 7.428466617754953e-07, 'epoch': 2.21}
{'loss': 4.0363, 'grad_norm': 42.34903335571289, 'learning_rate': 5.594277329420397e-07, 'epoch': 2.22}
{'loss': 3.9548, 'grad_norm': 67.46894836425781, 'learning_rate': 3.76008804108584e-07, 'epoch': 2.23}
{'loss': 3.4877, 'grad_norm': 51.47886657714844, 'learning_rate': 1.925898752751284e-07, 'epoch': 2.23}
{'loss': 4.2072, 'grad_norm': 45.143619537353516, 'learning_rate': 9.170946441672781e-09, 'epoch': 2.24}
{'train_runtime': 19307.2712, 'train_samples_per_second': 4.806, 'train_steps_per_second': 0.3, 'train_loss': 1.158690107937517, 'epoch': 2.24}
{'eval': {'eval_loss': 0.32209497690200806, 'eval_log_loss': 1.051549541148119, 'eval_accuracy': 0.4979343335507719, 'eval_runtime': 1062.589, 'eval_samples_per_second': 4.328, 'eval_steps_per_second': 4.328, 'epoch': 2.242418384360728}}
