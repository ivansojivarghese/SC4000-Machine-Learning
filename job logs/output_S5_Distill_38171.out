[Step5] Distilling fold 1 using LLaMA-only OOF probs
{'loss': 5.3779, 'grad_norm': 40.017391204833984, 'learning_rate': 3.234451718494272e-05, 'epoch': 0.39}
{'loss': 5.2628, 'grad_norm': 52.62248992919922, 'learning_rate': 3.19353518821604e-05, 'epoch': 0.4}
{'loss': 5.1485, 'grad_norm': 57.54389190673828, 'learning_rate': 3.152618657937807e-05, 'epoch': 0.41}
{'loss': 5.2108, 'grad_norm': 15.939162254333496, 'learning_rate': 3.111702127659575e-05, 'epoch': 0.42}
{'loss': 4.7747, 'grad_norm': 41.23941421508789, 'learning_rate': 3.070785597381342e-05, 'epoch': 0.43}
{'loss': 5.243, 'grad_norm': 18.03719139099121, 'learning_rate': 3.02986906710311e-05, 'epoch': 0.43}
{'loss': 5.2017, 'grad_norm': 15.661787033081055, 'learning_rate': 2.988952536824877e-05, 'epoch': 0.44}
{'loss': 4.9829, 'grad_norm': 44.04790496826172, 'learning_rate': 2.9480360065466452e-05, 'epoch': 0.45}
{'loss': 5.1527, 'grad_norm': 65.13850402832031, 'learning_rate': 2.9071194762684123e-05, 'epoch': 0.46}
{'loss': 4.758, 'grad_norm': 43.6126708984375, 'learning_rate': 2.86620294599018e-05, 'epoch': 0.46}
{'loss': 5.2972, 'grad_norm': 52.38603591918945, 'learning_rate': 2.825286415711948e-05, 'epoch': 0.47}
{'loss': 5.1554, 'grad_norm': 95.66307830810547, 'learning_rate': 2.7843698854337153e-05, 'epoch': 0.48}
{'loss': 4.8524, 'grad_norm': 41.39654541015625, 'learning_rate': 2.743453355155483e-05, 'epoch': 0.49}
{'loss': 5.1944, 'grad_norm': 29.932945251464844, 'learning_rate': 2.7025368248772502e-05, 'epoch': 0.49}
{'loss': 4.7353, 'grad_norm': 83.80484008789062, 'learning_rate': 2.6616202945990183e-05, 'epoch': 0.5}
{'loss': 5.1188, 'grad_norm': 52.33511734008789, 'learning_rate': 2.6207037643207855e-05, 'epoch': 0.51}
{'loss': 4.954, 'grad_norm': 36.042484283447266, 'learning_rate': 2.5797872340425532e-05, 'epoch': 0.52}
{'loss': 5.1374, 'grad_norm': 48.45473098754883, 'learning_rate': 2.538870703764321e-05, 'epoch': 0.53}
{'loss': 5.2795, 'grad_norm': 28.6295223236084, 'learning_rate': 2.4979541734860885e-05, 'epoch': 0.53}
{'loss': 4.9023, 'grad_norm': 35.87084197998047, 'learning_rate': 2.457037643207856e-05, 'epoch': 0.54}
{'loss': 5.0697, 'grad_norm': 36.21085739135742, 'learning_rate': 2.4161211129296237e-05, 'epoch': 0.55}
{'loss': 5.0006, 'grad_norm': 61.359413146972656, 'learning_rate': 2.3752045826513915e-05, 'epoch': 0.56}
{'loss': 4.8948, 'grad_norm': 41.45281982421875, 'learning_rate': 2.334288052373159e-05, 'epoch': 0.56}
{'loss': 5.2386, 'grad_norm': 52.89302444458008, 'learning_rate': 2.2933715220949264e-05, 'epoch': 0.57}
{'loss': 4.6387, 'grad_norm': 52.38920593261719, 'learning_rate': 2.252454991816694e-05, 'epoch': 0.58}
{'loss': 5.1551, 'grad_norm': 62.55153274536133, 'learning_rate': 2.2115384615384616e-05, 'epoch': 0.59}
{'loss': 5.0759, 'grad_norm': 79.03048706054688, 'learning_rate': 2.170621931260229e-05, 'epoch': 0.6}
{'loss': 5.3022, 'grad_norm': 39.698028564453125, 'learning_rate': 2.129705400981997e-05, 'epoch': 0.6}
{'loss': 5.0446, 'grad_norm': 24.99913787841797, 'learning_rate': 2.0887888707037647e-05, 'epoch': 0.61}
{'loss': 4.872, 'grad_norm': 33.64366149902344, 'learning_rate': 2.047872340425532e-05, 'epoch': 0.62}
{'loss': 5.2352, 'grad_norm': 38.57416915893555, 'learning_rate': 2.0069558101472996e-05, 'epoch': 0.63}
{'loss': 4.8545, 'grad_norm': 31.208824157714844, 'learning_rate': 1.966039279869067e-05, 'epoch': 0.63}
{'loss': 4.9363, 'grad_norm': 19.84848976135254, 'learning_rate': 1.9251227495908348e-05, 'epoch': 0.64}
{'loss': 5.1257, 'grad_norm': 18.483543395996094, 'learning_rate': 1.8842062193126022e-05, 'epoch': 0.65}
{'loss': 4.6481, 'grad_norm': 24.192121505737305, 'learning_rate': 1.84328968903437e-05, 'epoch': 0.66}
{'loss': 5.1538, 'grad_norm': 38.141212463378906, 'learning_rate': 1.8023731587561378e-05, 'epoch': 0.67}
{'loss': 4.7696, 'grad_norm': 72.4135971069336, 'learning_rate': 1.7614566284779053e-05, 'epoch': 0.67}
{'loss': 4.7457, 'grad_norm': 57.8050651550293, 'learning_rate': 1.7205400981996727e-05, 'epoch': 0.68}
{'loss': 5.1587, 'grad_norm': 59.961463928222656, 'learning_rate': 1.67962356792144e-05, 'epoch': 0.69}
{'loss': 4.619, 'grad_norm': 43.324501037597656, 'learning_rate': 1.638707037643208e-05, 'epoch': 0.7}
{'loss': 5.1209, 'grad_norm': 14.571743965148926, 'learning_rate': 1.5977905073649754e-05, 'epoch': 0.7}
{'loss': 4.9292, 'grad_norm': 51.98872756958008, 'learning_rate': 1.556873977086743e-05, 'epoch': 0.71}
{'loss': 4.7359, 'grad_norm': 23.13512420654297, 'learning_rate': 1.5159574468085108e-05, 'epoch': 0.72}
{'loss': 5.1865, 'grad_norm': 72.80686950683594, 'learning_rate': 1.4750409165302784e-05, 'epoch': 0.73}
{'loss': 4.8912, 'grad_norm': 26.599559783935547, 'learning_rate': 1.4341243862520459e-05, 'epoch': 0.73}
{'loss': 5.3394, 'grad_norm': 55.903133392333984, 'learning_rate': 1.3932078559738135e-05, 'epoch': 0.74}
{'loss': 5.0363, 'grad_norm': 31.4388427734375, 'learning_rate': 1.3522913256955811e-05, 'epoch': 0.75}
{'loss': 4.9125, 'grad_norm': 34.488887786865234, 'learning_rate': 1.3113747954173485e-05, 'epoch': 0.76}
{'loss': 5.1031, 'grad_norm': 61.65629959106445, 'learning_rate': 1.2704582651391162e-05, 'epoch': 0.77}
{'loss': 4.7594, 'grad_norm': 48.03575897216797, 'learning_rate': 1.2295417348608838e-05, 'epoch': 0.77}
{'loss': 4.9794, 'grad_norm': 49.325599670410156, 'learning_rate': 1.1886252045826514e-05, 'epoch': 0.78}
{'loss': 4.8686, 'grad_norm': 19.481136322021484, 'learning_rate': 1.147708674304419e-05, 'epoch': 0.79}
{'loss': 4.7616, 'grad_norm': 15.730340957641602, 'learning_rate': 1.1067921440261866e-05, 'epoch': 0.8}
{'loss': 5.1403, 'grad_norm': 30.828691482543945, 'learning_rate': 1.0658756137479543e-05, 'epoch': 0.8}
{'loss': 4.5239, 'grad_norm': 33.98688888549805, 'learning_rate': 1.0249590834697217e-05, 'epoch': 0.81}
{'loss': 5.0003, 'grad_norm': 92.55340576171875, 'learning_rate': 9.840425531914895e-06, 'epoch': 0.82}
{'loss': 4.9122, 'grad_norm': 37.77970504760742, 'learning_rate': 9.43126022913257e-06, 'epoch': 0.83}
{'loss': 4.7627, 'grad_norm': 44.779029846191406, 'learning_rate': 9.022094926350246e-06, 'epoch': 0.84}
{'loss': 5.0598, 'grad_norm': 22.176729202270508, 'learning_rate': 8.612929623567922e-06, 'epoch': 0.84}
{'loss': 4.4794, 'grad_norm': 16.213781356811523, 'learning_rate': 8.203764320785598e-06, 'epoch': 0.85}
{'loss': 4.9873, 'grad_norm': 35.12954330444336, 'learning_rate': 7.794599018003274e-06, 'epoch': 0.86}
{'loss': 5.0142, 'grad_norm': 40.96660232543945, 'learning_rate': 7.3854337152209495e-06, 'epoch': 0.87}
{'loss': 4.6476, 'grad_norm': 46.369022369384766, 'learning_rate': 6.976268412438626e-06, 'epoch': 0.87}
{'loss': 5.1531, 'grad_norm': 26.595186233520508, 'learning_rate': 6.567103109656302e-06, 'epoch': 0.88}
{'loss': 4.5711, 'grad_norm': 27.00716209411621, 'learning_rate': 6.157937806873977e-06, 'epoch': 0.89}
{'loss': 5.1987, 'grad_norm': 45.97188186645508, 'learning_rate': 5.748772504091653e-06, 'epoch': 0.9}
{'loss': 4.8963, 'grad_norm': 33.367069244384766, 'learning_rate': 5.3396072013093295e-06, 'epoch': 0.9}
{'loss': 4.685, 'grad_norm': 27.09303092956543, 'learning_rate': 4.930441898527005e-06, 'epoch': 0.91}
{'loss': 5.0098, 'grad_norm': 80.63114929199219, 'learning_rate': 4.521276595744681e-06, 'epoch': 0.92}
{'loss': 4.6396, 'grad_norm': 30.98582649230957, 'learning_rate': 4.112111292962357e-06, 'epoch': 0.93}
{'loss': 5.0238, 'grad_norm': 16.933029174804688, 'learning_rate': 3.702945990180033e-06, 'epoch': 0.94}
{'loss': 4.7849, 'grad_norm': 23.755348205566406, 'learning_rate': 3.2937806873977087e-06, 'epoch': 0.94}
{'loss': 4.7627, 'grad_norm': 50.574466705322266, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.95}
{'loss': 5.0357, 'grad_norm': 20.340576171875, 'learning_rate': 2.4754500818330606e-06, 'epoch': 0.96}
{'loss': 4.5588, 'grad_norm': 19.357391357421875, 'learning_rate': 2.066284779050737e-06, 'epoch': 0.97}
{'loss': 5.1407, 'grad_norm': 30.191383361816406, 'learning_rate': 1.6571194762684126e-06, 'epoch': 0.97}
{'loss': 4.7966, 'grad_norm': 46.73043441772461, 'learning_rate': 1.2479541734860885e-06, 'epoch': 0.98}
{'loss': 4.6769, 'grad_norm': 34.77708435058594, 'learning_rate': 8.387888707037644e-07, 'epoch': 0.99}
{'loss': 4.7524, 'grad_norm': 38.35121154785156, 'learning_rate': 4.2962356792144027e-07, 'epoch': 1.0}
{'loss': 4.7674, 'grad_norm': 20.347734451293945, 'learning_rate': 2.0458265139116204e-08, 'epoch': 1.01}
{'train_runtime': 18792.4122, 'train_samples_per_second': 2.214, 'train_steps_per_second': 0.138, 'train_loss': 3.0529497381357045, 'epoch': 1.01}
{'eval': {'eval_loss': 0.29891523718833923, 'eval_log_loss': 0.9849431474859448, 'eval_accuracy': 0.5044574907588606, 'eval_runtime': 1026.7963, 'eval_samples_per_second': 4.479, 'eval_steps_per_second': 4.479, 'epoch': 1.0054129814895365}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
