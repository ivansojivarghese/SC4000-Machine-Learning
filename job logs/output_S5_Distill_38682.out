[Step5] Distilling fold 1 using LLaMA-only OOF probs
{'loss': 4.9725, 'grad_norm': 57.17635726928711, 'learning_rate': 1.1747982391782832e-05, 'epoch': 1.75}
{'loss': 3.9667, 'grad_norm': 70.81012725830078, 'learning_rate': 1.1564563462949376e-05, 'epoch': 1.76}
{'loss': 4.8664, 'grad_norm': 29.13591957092285, 'learning_rate': 1.1381144534115921e-05, 'epoch': 1.76}
{'loss': 4.1806, 'grad_norm': 52.4118537902832, 'learning_rate': 1.1197725605282464e-05, 'epoch': 1.77}
{'loss': 4.3954, 'grad_norm': 27.395938873291016, 'learning_rate': 1.101430667644901e-05, 'epoch': 1.78}
{'loss': 4.6688, 'grad_norm': 53.936744689941406, 'learning_rate': 1.0830887747615555e-05, 'epoch': 1.79}
{'loss': 3.6555, 'grad_norm': 34.679927825927734, 'learning_rate': 1.0647468818782098e-05, 'epoch': 1.79}
{'loss': 4.6269, 'grad_norm': 30.441551208496094, 'learning_rate': 1.0464049889948643e-05, 'epoch': 1.8}
{'loss': 4.3203, 'grad_norm': 70.69091033935547, 'learning_rate': 1.0280630961115188e-05, 'epoch': 1.81}
{'loss': 4.4388, 'grad_norm': 39.880130767822266, 'learning_rate': 1.0097212032281732e-05, 'epoch': 1.82}
{'loss': 4.782, 'grad_norm': 32.2986946105957, 'learning_rate': 9.913793103448277e-06, 'epoch': 1.82}
{'loss': 3.805, 'grad_norm': 43.52150344848633, 'learning_rate': 9.73037417461482e-06, 'epoch': 1.83}
{'loss': 4.6512, 'grad_norm': 56.1656608581543, 'learning_rate': 9.546955245781365e-06, 'epoch': 1.84}
{'loss': 4.5125, 'grad_norm': 59.83784484863281, 'learning_rate': 9.36353631694791e-06, 'epoch': 1.85}
{'loss': 4.1783, 'grad_norm': 34.954227447509766, 'learning_rate': 9.180117388114454e-06, 'epoch': 1.86}
{'loss': 4.9343, 'grad_norm': 41.303260803222656, 'learning_rate': 8.996698459281e-06, 'epoch': 1.86}
{'loss': 4.0723, 'grad_norm': 37.58126449584961, 'learning_rate': 8.813279530447543e-06, 'epoch': 1.87}
{'loss': 4.8733, 'grad_norm': 47.14975357055664, 'learning_rate': 8.629860601614088e-06, 'epoch': 1.88}
{'loss': 4.5053, 'grad_norm': 43.062652587890625, 'learning_rate': 8.446441672780631e-06, 'epoch': 1.89}
{'loss': 4.7124, 'grad_norm': 83.93406677246094, 'learning_rate': 8.263022743947176e-06, 'epoch': 1.89}
{'loss': 4.8819, 'grad_norm': 38.13133239746094, 'learning_rate': 8.07960381511372e-06, 'epoch': 1.9}
{'loss': 4.2588, 'grad_norm': 42.511417388916016, 'learning_rate': 7.896184886280263e-06, 'epoch': 1.91}
{'loss': 4.8381, 'grad_norm': 34.128822326660156, 'learning_rate': 7.712765957446808e-06, 'epoch': 1.92}
{'loss': 4.7458, 'grad_norm': 59.84456253051758, 'learning_rate': 7.5293470286133535e-06, 'epoch': 1.93}
{'loss': 4.3704, 'grad_norm': 38.68223190307617, 'learning_rate': 7.345928099779898e-06, 'epoch': 1.93}
{'loss': 4.9047, 'grad_norm': 29.71858024597168, 'learning_rate': 7.162509170946442e-06, 'epoch': 1.94}
{'loss': 4.34, 'grad_norm': 45.908836364746094, 'learning_rate': 6.9790902421129855e-06, 'epoch': 1.95}
{'loss': 4.9399, 'grad_norm': 72.00721740722656, 'learning_rate': 6.795671313279531e-06, 'epoch': 1.96}
{'loss': 4.7665, 'grad_norm': 37.49196243286133, 'learning_rate': 6.612252384446076e-06, 'epoch': 1.96}
{'loss': 4.8833, 'grad_norm': 27.184656143188477, 'learning_rate': 6.428833455612619e-06, 'epoch': 1.97}
{'loss': 4.9777, 'grad_norm': 43.91691207885742, 'learning_rate': 6.245414526779164e-06, 'epoch': 1.98}
{'loss': 4.466, 'grad_norm': 33.670257568359375, 'learning_rate': 6.061995597945709e-06, 'epoch': 1.99}
{'loss': 4.9115, 'grad_norm': 23.941757202148438, 'learning_rate': 5.878576669112253e-06, 'epoch': 1.99}
{'loss': 4.4346, 'grad_norm': 56.188209533691406, 'learning_rate': 5.695157740278797e-06, 'epoch': 2.0}
{'loss': 4.4276, 'grad_norm': 39.81449890136719, 'learning_rate': 5.5117388114453415e-06, 'epoch': 2.01}
{'loss': 3.7527, 'grad_norm': 47.4142951965332, 'learning_rate': 5.328319882611886e-06, 'epoch': 2.02}
{'loss': 4.2813, 'grad_norm': 34.920738220214844, 'learning_rate': 5.14490095377843e-06, 'epoch': 2.03}
{'loss': 4.0015, 'grad_norm': 30.589466094970703, 'learning_rate': 4.961482024944974e-06, 'epoch': 2.03}
{'loss': 3.3432, 'grad_norm': 32.81467056274414, 'learning_rate': 4.778063096111519e-06, 'epoch': 2.04}
{'loss': 4.2414, 'grad_norm': 30.047937393188477, 'learning_rate': 4.594644167278064e-06, 'epoch': 2.05}
{'loss': 3.1185, 'grad_norm': 47.4351692199707, 'learning_rate': 4.411225238444608e-06, 'epoch': 2.06}
{'loss': 4.3325, 'grad_norm': 49.51390075683594, 'learning_rate': 4.227806309611152e-06, 'epoch': 2.06}
{'loss': 4.0472, 'grad_norm': 46.61931228637695, 'learning_rate': 4.044387380777697e-06, 'epoch': 2.07}
{'loss': 3.7618, 'grad_norm': 36.59840393066406, 'learning_rate': 3.860968451944241e-06, 'epoch': 2.08}
{'loss': 4.1919, 'grad_norm': 63.20197296142578, 'learning_rate': 3.6775495231107857e-06, 'epoch': 2.09}
{'loss': 3.7151, 'grad_norm': 72.49832153320312, 'learning_rate': 3.4941305942773295e-06, 'epoch': 2.1}
{'loss': 4.2232, 'grad_norm': 47.4685173034668, 'learning_rate': 3.310711665443874e-06, 'epoch': 2.1}
{'loss': 3.9951, 'grad_norm': 45.103389739990234, 'learning_rate': 3.127292736610418e-06, 'epoch': 2.11}
{'loss': 3.5276, 'grad_norm': 68.99115753173828, 'learning_rate': 2.943873807776963e-06, 'epoch': 2.12}
{'loss': 4.1793, 'grad_norm': 35.658233642578125, 'learning_rate': 2.760454878943507e-06, 'epoch': 2.13}
{'loss': 3.6319, 'grad_norm': 52.47007751464844, 'learning_rate': 2.5770359501100514e-06, 'epoch': 2.13}
{'loss': 3.902, 'grad_norm': 50.084251403808594, 'learning_rate': 2.3936170212765957e-06, 'epoch': 2.14}
{'loss': 4.3166, 'grad_norm': 45.25160217285156, 'learning_rate': 2.21019809244314e-06, 'epoch': 2.15}
{'loss': 3.8004, 'grad_norm': 43.35099411010742, 'learning_rate': 2.0267791636096847e-06, 'epoch': 2.16}
{'loss': 4.2732, 'grad_norm': 76.50666046142578, 'learning_rate': 1.8433602347762288e-06, 'epoch': 2.17}
{'loss': 3.0224, 'grad_norm': 52.33823776245117, 'learning_rate': 1.6599413059427735e-06, 'epoch': 2.17}
{'loss': 4.2808, 'grad_norm': 50.57040023803711, 'learning_rate': 1.4765223771093178e-06, 'epoch': 2.18}
{'loss': 4.1239, 'grad_norm': 33.12377166748047, 'learning_rate': 1.293103448275862e-06, 'epoch': 2.19}
{'loss': 3.5031, 'grad_norm': 52.32946014404297, 'learning_rate': 1.1096845194424066e-06, 'epoch': 2.2}
{'loss': 4.2841, 'grad_norm': 63.55716323852539, 'learning_rate': 9.26265590608951e-07, 'epoch': 2.2}
{'loss': 3.2928, 'grad_norm': 47.99018859863281, 'learning_rate': 7.428466617754953e-07, 'epoch': 2.21}
{'loss': 3.9888, 'grad_norm': 48.76449203491211, 'learning_rate': 5.594277329420397e-07, 'epoch': 2.22}
{'loss': 4.3398, 'grad_norm': 50.92384719848633, 'learning_rate': 3.76008804108584e-07, 'epoch': 2.23}
{'loss': 3.5649, 'grad_norm': 62.3130989074707, 'learning_rate': 1.925898752751284e-07, 'epoch': 2.23}
{'loss': 4.1737, 'grad_norm': 34.64976119995117, 'learning_rate': 9.170946441672781e-09, 'epoch': 2.24}
{'train_runtime': 15409.2748, 'train_samples_per_second': 6.022, 'train_steps_per_second': 0.376, 'train_loss': 0.9533652075405779, 'epoch': 2.24}
{'eval': {'eval_loss': 0.3133171796798706, 'eval_log_loss': 1.0238574468952109, 'eval_accuracy': 0.5103283322461405, 'eval_runtime': 1038.4849, 'eval_samples_per_second': 4.429, 'eval_steps_per_second': 4.429, 'epoch': 2.242424242424242}}
