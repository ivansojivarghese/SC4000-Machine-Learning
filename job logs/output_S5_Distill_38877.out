[Step5] Distilling fold 4 using LLaMA-only OOF probs
{'loss': 10.3554, 'grad_norm': 44.377925872802734, 'learning_rate': 1.4499266324284665e-05, 'epoch': 1.63}
{'loss': 9.136, 'grad_norm': 93.43913269042969, 'learning_rate': 1.4315847395451212e-05, 'epoch': 1.64}
{'loss': 11.1927, 'grad_norm': 33.53208541870117, 'learning_rate': 1.4132428466617756e-05, 'epoch': 1.65}
{'loss': 11.8518, 'grad_norm': 90.80332946777344, 'learning_rate': 1.3949009537784299e-05, 'epoch': 1.65}
{'loss': 11.405, 'grad_norm': 127.38025665283203, 'learning_rate': 1.3765590608950846e-05, 'epoch': 1.66}
{'loss': 13.3504, 'grad_norm': 221.35260009765625, 'learning_rate': 1.358217168011739e-05, 'epoch': 1.67}
{'loss': 13.1568, 'grad_norm': 120.46098327636719, 'learning_rate': 1.3398752751283933e-05, 'epoch': 1.68}
{'loss': 11.8822, 'grad_norm': 108.19056701660156, 'learning_rate': 1.3215333822450476e-05, 'epoch': 1.69}
{'loss': 9.4402, 'grad_norm': 84.09439849853516, 'learning_rate': 1.3031914893617023e-05, 'epoch': 1.69}
{'loss': 11.6183, 'grad_norm': 86.27477264404297, 'learning_rate': 1.2848495964783567e-05, 'epoch': 1.7}
{'loss': 10.9773, 'grad_norm': 58.58156204223633, 'learning_rate': 1.266507703595011e-05, 'epoch': 1.71}
{'loss': 9.6859, 'grad_norm': 99.12262725830078, 'learning_rate': 1.2481658107116655e-05, 'epoch': 1.72}
{'loss': 10.1161, 'grad_norm': 98.12301635742188, 'learning_rate': 1.22982391782832e-05, 'epoch': 1.72}
{'loss': 13.3772, 'grad_norm': 100.98189544677734, 'learning_rate': 1.2114820249449744e-05, 'epoch': 1.73}
{'loss': 11.4525, 'grad_norm': 153.5950469970703, 'learning_rate': 1.1931401320616289e-05, 'epoch': 1.74}
{'loss': 12.2442, 'grad_norm': 32.813865661621094, 'learning_rate': 1.1747982391782832e-05, 'epoch': 1.75}
{'loss': 12.813, 'grad_norm': 64.96710968017578, 'learning_rate': 1.1564563462949376e-05, 'epoch': 1.76}
{'loss': 12.8448, 'grad_norm': 50.26438903808594, 'learning_rate': 1.1381144534115921e-05, 'epoch': 1.76}
{'loss': 8.3722, 'grad_norm': 38.8944091796875, 'learning_rate': 1.1197725605282464e-05, 'epoch': 1.77}
{'loss': 12.1181, 'grad_norm': 57.031490325927734, 'learning_rate': 1.101430667644901e-05, 'epoch': 1.78}
{'loss': 12.5562, 'grad_norm': 146.46031188964844, 'learning_rate': 1.0830887747615555e-05, 'epoch': 1.79}
{'loss': 12.071, 'grad_norm': 61.0889892578125, 'learning_rate': 1.0647468818782098e-05, 'epoch': 1.79}
{'loss': 13.1037, 'grad_norm': 79.71593475341797, 'learning_rate': 1.0464049889948643e-05, 'epoch': 1.8}
{'loss': 10.3666, 'grad_norm': 100.03710174560547, 'learning_rate': 1.0280630961115188e-05, 'epoch': 1.81}
{'loss': 12.5197, 'grad_norm': 131.29296875, 'learning_rate': 1.0097212032281732e-05, 'epoch': 1.82}
{'loss': 16.1355, 'grad_norm': 80.00543212890625, 'learning_rate': 9.913793103448277e-06, 'epoch': 1.82}
{'loss': 8.707, 'grad_norm': 25.82263946533203, 'learning_rate': 9.73037417461482e-06, 'epoch': 1.83}
{'loss': 11.4686, 'grad_norm': 66.50028991699219, 'learning_rate': 9.546955245781365e-06, 'epoch': 1.84}
{'loss': 13.2594, 'grad_norm': 96.29694366455078, 'learning_rate': 9.36353631694791e-06, 'epoch': 1.85}
{'loss': 13.1901, 'grad_norm': 79.59513854980469, 'learning_rate': 9.180117388114454e-06, 'epoch': 1.86}
{'loss': 10.3691, 'grad_norm': 132.57298278808594, 'learning_rate': 8.996698459281e-06, 'epoch': 1.86}
{'loss': 10.849, 'grad_norm': 24.453706741333008, 'learning_rate': 8.813279530447543e-06, 'epoch': 1.87}
{'loss': 14.4118, 'grad_norm': 65.7623062133789, 'learning_rate': 8.629860601614088e-06, 'epoch': 1.88}
{'loss': 13.1245, 'grad_norm': 91.0760498046875, 'learning_rate': 8.446441672780631e-06, 'epoch': 1.89}
{'loss': 13.1917, 'grad_norm': 52.68128967285156, 'learning_rate': 8.263022743947176e-06, 'epoch': 1.89}
{'loss': 13.35, 'grad_norm': 100.25115203857422, 'learning_rate': 8.07960381511372e-06, 'epoch': 1.9}
{'loss': 11.6272, 'grad_norm': 36.620025634765625, 'learning_rate': 7.896184886280263e-06, 'epoch': 1.91}
{'loss': 12.8186, 'grad_norm': 36.555606842041016, 'learning_rate': 7.712765957446808e-06, 'epoch': 1.92}
{'loss': 11.7289, 'grad_norm': 108.02710723876953, 'learning_rate': 7.5293470286133535e-06, 'epoch': 1.93}
{'loss': 13.4344, 'grad_norm': 79.2735366821289, 'learning_rate': 7.345928099779898e-06, 'epoch': 1.93}
{'loss': 14.6841, 'grad_norm': 96.34110260009766, 'learning_rate': 7.162509170946442e-06, 'epoch': 1.94}
{'loss': 11.0003, 'grad_norm': 73.27405548095703, 'learning_rate': 6.9790902421129855e-06, 'epoch': 1.95}
{'loss': 12.2689, 'grad_norm': 58.90530776977539, 'learning_rate': 6.795671313279531e-06, 'epoch': 1.96}
{'loss': 13.1251, 'grad_norm': 197.54257202148438, 'learning_rate': 6.612252384446076e-06, 'epoch': 1.96}
{'loss': 11.0399, 'grad_norm': 28.74980926513672, 'learning_rate': 6.428833455612619e-06, 'epoch': 1.97}
{'loss': 12.7818, 'grad_norm': 52.64278030395508, 'learning_rate': 6.245414526779164e-06, 'epoch': 1.98}
{'loss': 10.5294, 'grad_norm': 44.17844009399414, 'learning_rate': 6.061995597945709e-06, 'epoch': 1.99}
{'loss': 12.0268, 'grad_norm': 127.31329345703125, 'learning_rate': 5.878576669112253e-06, 'epoch': 1.99}
{'loss': 12.5726, 'grad_norm': 39.569828033447266, 'learning_rate': 5.695157740278797e-06, 'epoch': 2.0}
{'loss': 9.2881, 'grad_norm': 62.02729797363281, 'learning_rate': 5.5117388114453415e-06, 'epoch': 2.01}
{'loss': 13.1211, 'grad_norm': 61.946388244628906, 'learning_rate': 5.328319882611886e-06, 'epoch': 2.02}
{'loss': 10.0419, 'grad_norm': 36.11430358886719, 'learning_rate': 5.14490095377843e-06, 'epoch': 2.03}
{'loss': 10.1579, 'grad_norm': 37.92877960205078, 'learning_rate': 4.961482024944974e-06, 'epoch': 2.03}
{'loss': 8.8009, 'grad_norm': 115.58368682861328, 'learning_rate': 4.778063096111519e-06, 'epoch': 2.04}
{'loss': 11.9688, 'grad_norm': 66.0793228149414, 'learning_rate': 4.594644167278064e-06, 'epoch': 2.05}
{'loss': 9.5723, 'grad_norm': 47.60980224609375, 'learning_rate': 4.411225238444608e-06, 'epoch': 2.06}
{'loss': 11.6297, 'grad_norm': 225.8710174560547, 'learning_rate': 4.227806309611152e-06, 'epoch': 2.06}
{'loss': 12.5625, 'grad_norm': 56.307979583740234, 'learning_rate': 4.044387380777697e-06, 'epoch': 2.07}
{'loss': 13.268, 'grad_norm': 78.41075897216797, 'learning_rate': 3.860968451944241e-06, 'epoch': 2.08}
{'loss': 13.1187, 'grad_norm': 141.8933563232422, 'learning_rate': 3.6775495231107857e-06, 'epoch': 2.09}
{'loss': 9.6899, 'grad_norm': 224.75013732910156, 'learning_rate': 3.4941305942773295e-06, 'epoch': 2.1}
{'loss': 10.7004, 'grad_norm': 66.4625473022461, 'learning_rate': 3.310711665443874e-06, 'epoch': 2.1}
{'loss': 12.4597, 'grad_norm': 95.13368225097656, 'learning_rate': 3.127292736610418e-06, 'epoch': 2.11}
{'loss': 10.663, 'grad_norm': 60.905357360839844, 'learning_rate': 2.943873807776963e-06, 'epoch': 2.12}
{'loss': 13.4892, 'grad_norm': 123.06527709960938, 'learning_rate': 2.760454878943507e-06, 'epoch': 2.13}
{'loss': 9.6779, 'grad_norm': 46.33030319213867, 'learning_rate': 2.5770359501100514e-06, 'epoch': 2.13}
{'loss': 9.9284, 'grad_norm': 109.10845947265625, 'learning_rate': 2.3936170212765957e-06, 'epoch': 2.14}
{'loss': 11.2349, 'grad_norm': 149.9901123046875, 'learning_rate': 2.21019809244314e-06, 'epoch': 2.15}
{'loss': 10.213, 'grad_norm': 107.05593872070312, 'learning_rate': 2.0267791636096847e-06, 'epoch': 2.16}
{'loss': 9.2646, 'grad_norm': 124.7607192993164, 'learning_rate': 1.8433602347762288e-06, 'epoch': 2.17}
{'loss': 10.4672, 'grad_norm': 202.6898651123047, 'learning_rate': 1.6599413059427735e-06, 'epoch': 2.17}
{'loss': 10.8693, 'grad_norm': 95.9288558959961, 'learning_rate': 1.4765223771093178e-06, 'epoch': 2.18}
{'loss': 11.9039, 'grad_norm': 92.35712432861328, 'learning_rate': 1.293103448275862e-06, 'epoch': 2.19}
{'loss': 12.2213, 'grad_norm': 145.6426544189453, 'learning_rate': 1.1096845194424066e-06, 'epoch': 2.2}
{'loss': 9.2242, 'grad_norm': 84.89978790283203, 'learning_rate': 9.26265590608951e-07, 'epoch': 2.2}
{'loss': 11.7146, 'grad_norm': 48.674285888671875, 'learning_rate': 7.428466617754953e-07, 'epoch': 2.21}
{'loss': 10.2408, 'grad_norm': 107.93383026123047, 'learning_rate': 5.594277329420397e-07, 'epoch': 2.22}
{'loss': 9.2275, 'grad_norm': 59.145965576171875, 'learning_rate': 3.76008804108584e-07, 'epoch': 2.23}
{'loss': 9.2011, 'grad_norm': 42.8101921081543, 'learning_rate': 1.925898752751284e-07, 'epoch': 2.23}
{'loss': 10.3408, 'grad_norm': 82.27445220947266, 'learning_rate': 9.170946441672781e-09, 'epoch': 2.24}
{'train_runtime': 18951.955, 'train_samples_per_second': 4.897, 'train_steps_per_second': 0.306, 'train_loss': 3.179186564478381, 'epoch': 2.24}
{'eval': {'eval_loss': 0.8070201873779297, 'eval_log_loss': 1.1331775458747733, 'eval_accuracy': 0.4809741248097412, 'eval_runtime': 1024.3318, 'eval_samples_per_second': 4.49, 'eval_steps_per_second': 4.49, 'epoch': 2.242418384360728}}
