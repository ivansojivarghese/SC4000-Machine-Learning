[Step5] Distilling fold 3 using LLaMA-only OOF probs
{'loss': 5.18, 'grad_norm': 64.79077911376953, 'learning_rate': 3.234451718494272e-05, 'epoch': 0.39}
{'loss': 5.1206, 'grad_norm': 34.51902389526367, 'learning_rate': 3.19353518821604e-05, 'epoch': 0.4}
{'loss': 5.0779, 'grad_norm': 17.40528678894043, 'learning_rate': 3.152618657937807e-05, 'epoch': 0.41}
{'loss': 5.4448, 'grad_norm': 39.25047302246094, 'learning_rate': 3.111702127659575e-05, 'epoch': 0.42}
{'loss': 4.727, 'grad_norm': 75.97573852539062, 'learning_rate': 3.070785597381342e-05, 'epoch': 0.43}
{'loss': 5.3032, 'grad_norm': 12.851205825805664, 'learning_rate': 3.02986906710311e-05, 'epoch': 0.43}
{'loss': 5.2541, 'grad_norm': 45.492271423339844, 'learning_rate': 2.988952536824877e-05, 'epoch': 0.44}
{'loss': 4.9819, 'grad_norm': 39.24336242675781, 'learning_rate': 2.9480360065466452e-05, 'epoch': 0.45}
{'loss': 5.2096, 'grad_norm': 25.376922607421875, 'learning_rate': 2.9071194762684123e-05, 'epoch': 0.46}
{'loss': 4.6264, 'grad_norm': 17.763797760009766, 'learning_rate': 2.86620294599018e-05, 'epoch': 0.46}
{'loss': 5.3543, 'grad_norm': 55.123779296875, 'learning_rate': 2.825286415711948e-05, 'epoch': 0.47}
{'loss': 5.0855, 'grad_norm': 28.4231014251709, 'learning_rate': 2.7843698854337153e-05, 'epoch': 0.48}
{'loss': 4.8852, 'grad_norm': 42.28953170776367, 'learning_rate': 2.743453355155483e-05, 'epoch': 0.49}
{'loss': 5.2297, 'grad_norm': 61.94321060180664, 'learning_rate': 2.7025368248772502e-05, 'epoch': 0.49}
{'loss': 5.1359, 'grad_norm': 36.38633346557617, 'learning_rate': 2.6616202945990183e-05, 'epoch': 0.5}
{'loss': 5.3113, 'grad_norm': 40.15260314941406, 'learning_rate': 2.6207037643207855e-05, 'epoch': 0.51}
{'loss': 5.1056, 'grad_norm': 35.5548210144043, 'learning_rate': 2.5797872340425532e-05, 'epoch': 0.52}
{'loss': 4.868, 'grad_norm': 42.856380462646484, 'learning_rate': 2.538870703764321e-05, 'epoch': 0.53}
{'loss': 5.1751, 'grad_norm': 68.01351928710938, 'learning_rate': 2.4979541734860885e-05, 'epoch': 0.53}
{'loss': 4.931, 'grad_norm': 44.17290496826172, 'learning_rate': 2.457037643207856e-05, 'epoch': 0.54}
{'loss': 5.03, 'grad_norm': 23.99884605407715, 'learning_rate': 2.4161211129296237e-05, 'epoch': 0.55}
{'loss': 5.1255, 'grad_norm': 13.716215133666992, 'learning_rate': 2.3752045826513915e-05, 'epoch': 0.56}
{'loss': 4.9918, 'grad_norm': 42.35268020629883, 'learning_rate': 2.334288052373159e-05, 'epoch': 0.56}
{'loss': 5.2587, 'grad_norm': 17.434844970703125, 'learning_rate': 2.2933715220949264e-05, 'epoch': 0.57}
{'loss': 4.6077, 'grad_norm': 29.225379943847656, 'learning_rate': 2.252454991816694e-05, 'epoch': 0.58}
{'loss': 5.0996, 'grad_norm': 29.439563751220703, 'learning_rate': 2.2115384615384616e-05, 'epoch': 0.59}
{'loss': 5.1243, 'grad_norm': 37.137821197509766, 'learning_rate': 2.170621931260229e-05, 'epoch': 0.6}
{'loss': 4.8004, 'grad_norm': 17.923782348632812, 'learning_rate': 2.129705400981997e-05, 'epoch': 0.6}
{'loss': 5.1937, 'grad_norm': 53.225589752197266, 'learning_rate': 2.0887888707037647e-05, 'epoch': 0.61}
{'loss': 4.6356, 'grad_norm': 21.311006546020508, 'learning_rate': 2.047872340425532e-05, 'epoch': 0.62}
{'loss': 5.1112, 'grad_norm': 21.73312759399414, 'learning_rate': 2.0069558101472996e-05, 'epoch': 0.63}
{'loss': 4.9452, 'grad_norm': 38.094974517822266, 'learning_rate': 1.966039279869067e-05, 'epoch': 0.63}
{'loss': 5.102, 'grad_norm': 51.04389572143555, 'learning_rate': 1.9251227495908348e-05, 'epoch': 0.64}
{'loss': 5.0875, 'grad_norm': 70.50196838378906, 'learning_rate': 1.8842062193126022e-05, 'epoch': 0.65}
{'loss': 5.1105, 'grad_norm': 23.006351470947266, 'learning_rate': 1.84328968903437e-05, 'epoch': 0.66}
{'loss': 5.2176, 'grad_norm': 40.681068420410156, 'learning_rate': 1.8023731587561378e-05, 'epoch': 0.67}
{'loss': 5.1199, 'grad_norm': 37.79234313964844, 'learning_rate': 1.7614566284779053e-05, 'epoch': 0.67}
{'loss': 5.003, 'grad_norm': 36.05002975463867, 'learning_rate': 1.7205400981996727e-05, 'epoch': 0.68}
{'loss': 5.1894, 'grad_norm': 35.84703826904297, 'learning_rate': 1.67962356792144e-05, 'epoch': 0.69}
{'loss': 4.7958, 'grad_norm': 14.529831886291504, 'learning_rate': 1.638707037643208e-05, 'epoch': 0.7}
{'loss': 5.14, 'grad_norm': 20.34694480895996, 'learning_rate': 1.5977905073649754e-05, 'epoch': 0.7}
{'loss': 5.0143, 'grad_norm': 45.84878921508789, 'learning_rate': 1.556873977086743e-05, 'epoch': 0.71}
{'loss': 4.9741, 'grad_norm': 16.718284606933594, 'learning_rate': 1.5159574468085108e-05, 'epoch': 0.72}
{'loss': 5.1899, 'grad_norm': 41.37717819213867, 'learning_rate': 1.4750409165302784e-05, 'epoch': 0.73}
{'loss': 4.4493, 'grad_norm': 18.909170150756836, 'learning_rate': 1.4341243862520459e-05, 'epoch': 0.73}
{'loss': 5.1395, 'grad_norm': 52.46332550048828, 'learning_rate': 1.3932078559738135e-05, 'epoch': 0.74}
{'loss': 4.9826, 'grad_norm': 27.998689651489258, 'learning_rate': 1.3522913256955811e-05, 'epoch': 0.75}
{'loss': 4.5527, 'grad_norm': 29.695812225341797, 'learning_rate': 1.3113747954173485e-05, 'epoch': 0.76}
{'loss': 5.0232, 'grad_norm': 50.85252380371094, 'learning_rate': 1.2704582651391162e-05, 'epoch': 0.77}
{'loss': 4.297, 'grad_norm': 66.57423400878906, 'learning_rate': 1.2295417348608838e-05, 'epoch': 0.77}
{'loss': 5.1731, 'grad_norm': 30.945505142211914, 'learning_rate': 1.1886252045826514e-05, 'epoch': 0.78}
{'loss': 4.8825, 'grad_norm': 39.777915954589844, 'learning_rate': 1.147708674304419e-05, 'epoch': 0.79}
{'loss': 4.5501, 'grad_norm': 51.30883026123047, 'learning_rate': 1.1067921440261866e-05, 'epoch': 0.8}
{'loss': 4.9895, 'grad_norm': 43.52559280395508, 'learning_rate': 1.0658756137479543e-05, 'epoch': 0.8}
{'loss': 4.5482, 'grad_norm': 43.05506896972656, 'learning_rate': 1.0249590834697217e-05, 'epoch': 0.81}
{'loss': 5.1133, 'grad_norm': 45.47377395629883, 'learning_rate': 9.840425531914895e-06, 'epoch': 0.82}
{'loss': 4.805, 'grad_norm': 22.223236083984375, 'learning_rate': 9.43126022913257e-06, 'epoch': 0.83}
{'loss': 4.842, 'grad_norm': 72.20800018310547, 'learning_rate': 9.022094926350246e-06, 'epoch': 0.84}
{'loss': 5.1144, 'grad_norm': 55.09685134887695, 'learning_rate': 8.612929623567922e-06, 'epoch': 0.84}
{'loss': 4.5198, 'grad_norm': 58.151485443115234, 'learning_rate': 8.203764320785598e-06, 'epoch': 0.85}
{'loss': 5.0527, 'grad_norm': 30.665359497070312, 'learning_rate': 7.794599018003274e-06, 'epoch': 0.86}
{'loss': 4.9747, 'grad_norm': 33.74756622314453, 'learning_rate': 7.3854337152209495e-06, 'epoch': 0.87}
{'loss': 4.7782, 'grad_norm': 24.149349212646484, 'learning_rate': 6.976268412438626e-06, 'epoch': 0.87}
{'loss': 5.1135, 'grad_norm': 19.553680419921875, 'learning_rate': 6.567103109656302e-06, 'epoch': 0.88}
{'loss': 4.5428, 'grad_norm': 39.7232551574707, 'learning_rate': 6.157937806873977e-06, 'epoch': 0.89}
{'loss': 5.0513, 'grad_norm': 19.55240249633789, 'learning_rate': 5.748772504091653e-06, 'epoch': 0.9}
{'loss': 4.9302, 'grad_norm': 18.757274627685547, 'learning_rate': 5.3396072013093295e-06, 'epoch': 0.9}
{'loss': 4.6805, 'grad_norm': 54.48518753051758, 'learning_rate': 4.930441898527005e-06, 'epoch': 0.91}
{'loss': 5.1492, 'grad_norm': 44.148284912109375, 'learning_rate': 4.521276595744681e-06, 'epoch': 0.92}
{'loss': 4.4279, 'grad_norm': 71.47386169433594, 'learning_rate': 4.112111292962357e-06, 'epoch': 0.93}
{'loss': 4.9339, 'grad_norm': 21.3760986328125, 'learning_rate': 3.702945990180033e-06, 'epoch': 0.94}
{'loss': 4.9107, 'grad_norm': 35.03813934326172, 'learning_rate': 3.2937806873977087e-06, 'epoch': 0.94}
{'loss': 4.6498, 'grad_norm': 26.87327766418457, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.95}
{'loss': 5.0005, 'grad_norm': 44.198387145996094, 'learning_rate': 2.4754500818330606e-06, 'epoch': 0.96}
{'loss': 4.4377, 'grad_norm': 32.285179138183594, 'learning_rate': 2.066284779050737e-06, 'epoch': 0.97}
{'loss': 5.0522, 'grad_norm': 36.878753662109375, 'learning_rate': 1.6571194762684126e-06, 'epoch': 0.97}
{'loss': 4.8519, 'grad_norm': 43.71234130859375, 'learning_rate': 1.2479541734860885e-06, 'epoch': 0.98}
{'loss': 4.6353, 'grad_norm': 49.242103576660156, 'learning_rate': 8.387888707037644e-07, 'epoch': 0.99}
{'loss': 4.7886, 'grad_norm': 35.67366027832031, 'learning_rate': 4.2962356792144027e-07, 'epoch': 1.0}
{'loss': 4.4653, 'grad_norm': 16.098413467407227, 'learning_rate': 2.0458265139116204e-08, 'epoch': 1.01}
{'train_runtime': 19042.7601, 'train_samples_per_second': 2.185, 'train_steps_per_second': 0.137, 'train_loss': 3.0483396060650163, 'epoch': 1.01}
{'eval': {'eval_loss': 0.299511194229126, 'eval_log_loss': 0.9866974162652292, 'eval_accuracy': 0.5020656664492281, 'eval_runtime': 1025.0088, 'eval_samples_per_second': 4.487, 'eval_steps_per_second': 4.487, 'epoch': 1.0054128506874804}}
[Step5] Done fold 3 -> model_save/distilled_gemma2-9b_fold_3
