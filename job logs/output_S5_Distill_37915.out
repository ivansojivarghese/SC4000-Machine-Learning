[Step5] Distilling fold 2 using LLaMA-only OOF probs
{'loss': 9.8326, 'grad_norm': 143.16709899902344, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.01}
{'loss': 7.4922, 'grad_norm': 97.23886108398438, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
{'loss': 7.2327, 'grad_norm': 100.57260131835938, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.02}
{'loss': 8.183, 'grad_norm': 87.2729721069336, 'learning_rate': 4.898936170212766e-05, 'epoch': 0.03}
{'loss': 6.206, 'grad_norm': 215.72035217285156, 'learning_rate': 4.792553191489362e-05, 'epoch': 0.04}
{'loss': 6.9317, 'grad_norm': 74.0467529296875, 'learning_rate': 4.686170212765958e-05, 'epoch': 0.05}
{'loss': 6.1095, 'grad_norm': 43.041419982910156, 'learning_rate': 4.579787234042554e-05, 'epoch': 0.05}
{'loss': 5.8667, 'grad_norm': 80.82857513427734, 'learning_rate': 4.473404255319149e-05, 'epoch': 0.06}
{'loss': 5.9982, 'grad_norm': 59.59989929199219, 'learning_rate': 4.367021276595745e-05, 'epoch': 0.07}
{'loss': 5.6009, 'grad_norm': 48.64869689941406, 'learning_rate': 4.2606382978723406e-05, 'epoch': 0.08}
{'loss': 5.6628, 'grad_norm': 40.334373474121094, 'learning_rate': 4.1542553191489364e-05, 'epoch': 0.09}
{'loss': 5.4993, 'grad_norm': 22.657907485961914, 'learning_rate': 4.047872340425532e-05, 'epoch': 0.09}
{'loss': 5.5823, 'grad_norm': 47.1527214050293, 'learning_rate': 3.941489361702128e-05, 'epoch': 0.1}
{'loss': 5.4192, 'grad_norm': 30.143014907836914, 'learning_rate': 3.835106382978724e-05, 'epoch': 0.11}
{'loss': 5.2757, 'grad_norm': 32.3420524597168, 'learning_rate': 3.728723404255319e-05, 'epoch': 0.12}
{'loss': 5.7195, 'grad_norm': 49.37255859375, 'learning_rate': 3.622340425531915e-05, 'epoch': 0.12}
{'loss': 5.238, 'grad_norm': 53.59344482421875, 'learning_rate': 3.515957446808511e-05, 'epoch': 0.13}
{'loss': 5.2088, 'grad_norm': 25.74852752685547, 'learning_rate': 3.4095744680851066e-05, 'epoch': 0.14}
{'loss': 5.581, 'grad_norm': 22.55484390258789, 'learning_rate': 3.3031914893617025e-05, 'epoch': 0.15}
{'loss': 4.9658, 'grad_norm': 58.643131256103516, 'learning_rate': 3.1968085106382976e-05, 'epoch': 0.15}
{'loss': 5.4717, 'grad_norm': 13.193029403686523, 'learning_rate': 3.090425531914894e-05, 'epoch': 0.16}
{'loss': 5.3052, 'grad_norm': 61.47556686401367, 'learning_rate': 2.9840425531914897e-05, 'epoch': 0.17}
{'loss': 5.0338, 'grad_norm': 27.357091903686523, 'learning_rate': 2.877659574468085e-05, 'epoch': 0.18}
{'loss': 5.3693, 'grad_norm': 31.892967224121094, 'learning_rate': 2.7712765957446813e-05, 'epoch': 0.19}
{'loss': 5.3534, 'grad_norm': 97.00434112548828, 'learning_rate': 2.664893617021277e-05, 'epoch': 0.19}
{'loss': 5.4522, 'grad_norm': 82.84333801269531, 'learning_rate': 2.5585106382978723e-05, 'epoch': 0.2}
{'loss': 5.1985, 'grad_norm': 52.745521545410156, 'learning_rate': 2.4521276595744682e-05, 'epoch': 0.21}
{'loss': 4.8237, 'grad_norm': 21.031352996826172, 'learning_rate': 2.345744680851064e-05, 'epoch': 0.22}
{'loss': 5.3329, 'grad_norm': 68.24263000488281, 'learning_rate': 2.23936170212766e-05, 'epoch': 0.22}
{'loss': 4.9158, 'grad_norm': 76.95633697509766, 'learning_rate': 2.1329787234042554e-05, 'epoch': 0.23}
{'loss': 5.2028, 'grad_norm': 24.242036819458008, 'learning_rate': 2.0265957446808512e-05, 'epoch': 0.24}
{'loss': 5.3351, 'grad_norm': 26.129329681396484, 'learning_rate': 1.920212765957447e-05, 'epoch': 0.25}
{'loss': 5.1764, 'grad_norm': 31.939420700073242, 'learning_rate': 1.8138297872340425e-05, 'epoch': 0.26}
{'loss': 5.223, 'grad_norm': 49.27237319946289, 'learning_rate': 1.7074468085106384e-05, 'epoch': 0.26}
{'loss': 4.6427, 'grad_norm': 39.2662353515625, 'learning_rate': 1.6010638297872342e-05, 'epoch': 0.27}
{'loss': 5.3727, 'grad_norm': 28.581565856933594, 'learning_rate': 1.4946808510638299e-05, 'epoch': 0.28}
{'loss': 4.8709, 'grad_norm': 42.79243850708008, 'learning_rate': 1.3882978723404256e-05, 'epoch': 0.29}
{'loss': 5.0058, 'grad_norm': 38.837093353271484, 'learning_rate': 1.2819148936170214e-05, 'epoch': 0.29}
{'loss': 5.4239, 'grad_norm': 76.53551483154297, 'learning_rate': 1.175531914893617e-05, 'epoch': 0.3}
{'loss': 4.7958, 'grad_norm': 43.30465316772461, 'learning_rate': 1.0691489361702128e-05, 'epoch': 0.31}
{'loss': 5.192, 'grad_norm': 39.678428649902344, 'learning_rate': 9.627659574468086e-06, 'epoch': 0.32}
{'loss': 5.0583, 'grad_norm': 31.593015670776367, 'learning_rate': 8.563829787234043e-06, 'epoch': 0.32}
{'loss': 4.8699, 'grad_norm': 44.3591194152832, 'learning_rate': 7.5e-06, 'epoch': 0.33}
{'loss': 5.1477, 'grad_norm': 26.980636596679688, 'learning_rate': 6.436170212765958e-06, 'epoch': 0.34}
{'loss': 4.6435, 'grad_norm': 23.96632194519043, 'learning_rate': 5.372340425531915e-06, 'epoch': 0.35}
{'loss': 5.1437, 'grad_norm': 34.050357818603516, 'learning_rate': 4.308510638297873e-06, 'epoch': 0.36}
{'loss': 4.9022, 'grad_norm': 56.83074951171875, 'learning_rate': 3.2446808510638296e-06, 'epoch': 0.36}
{'loss': 4.9971, 'grad_norm': 25.41900062561035, 'learning_rate': 2.1808510638297876e-06, 'epoch': 0.37}
{'loss': 5.1253, 'grad_norm': 41.39799118041992, 'learning_rate': 1.1170212765957447e-06, 'epoch': 0.38}
{'loss': 4.7334, 'grad_norm': 46.50868606567383, 'learning_rate': 5.319148936170213e-08, 'epoch': 0.39}
{'train_runtime': 11797.9884, 'train_samples_per_second': 1.356, 'train_steps_per_second': 0.085, 'train_loss': 5.534490562438965, 'epoch': 0.39}
{'eval': {'eval_loss': 0.31067734956741333, 'eval_log_loss': 1.0278831159788528, 'eval_accuracy': 0.4622744074798869, 'eval_runtime': 1014.6777, 'eval_samples_per_second': 4.532, 'eval_steps_per_second': 4.532, 'epoch': 0.3866321919628833}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
