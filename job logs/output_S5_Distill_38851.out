[Step5] Distilling fold 3 using LLaMA-only OOF probs
{'loss': 4.5954, 'grad_norm': 50.93381118774414, 'learning_rate': 1.4499266324284665e-05, 'epoch': 1.63}
{'loss': 3.6793, 'grad_norm': 67.09095001220703, 'learning_rate': 1.4315847395451212e-05, 'epoch': 1.64}
{'loss': 4.9553, 'grad_norm': 63.69533157348633, 'learning_rate': 1.4132428466617756e-05, 'epoch': 1.65}
{'loss': 4.6229, 'grad_norm': 64.46732330322266, 'learning_rate': 1.3949009537784299e-05, 'epoch': 1.65}
{'loss': 4.543, 'grad_norm': 60.347965240478516, 'learning_rate': 1.3765590608950846e-05, 'epoch': 1.66}
{'loss': 4.7976, 'grad_norm': 33.87582778930664, 'learning_rate': 1.358217168011739e-05, 'epoch': 1.67}
{'loss': 3.7795, 'grad_norm': 50.06361770629883, 'learning_rate': 1.3398752751283933e-05, 'epoch': 1.68}
{'loss': 4.9247, 'grad_norm': 46.91020202636719, 'learning_rate': 1.3215333822450476e-05, 'epoch': 1.69}
{'loss': 4.3061, 'grad_norm': 51.247798919677734, 'learning_rate': 1.3031914893617023e-05, 'epoch': 1.69}
{'loss': 4.7184, 'grad_norm': 24.0113468170166, 'learning_rate': 1.2848495964783567e-05, 'epoch': 1.7}
{'loss': 4.8161, 'grad_norm': 37.080780029296875, 'learning_rate': 1.266507703595011e-05, 'epoch': 1.71}
{'loss': 4.1198, 'grad_norm': 24.220075607299805, 'learning_rate': 1.2481658107116655e-05, 'epoch': 1.72}
{'loss': 4.8032, 'grad_norm': 41.57477569580078, 'learning_rate': 1.22982391782832e-05, 'epoch': 1.72}
{'loss': 4.2656, 'grad_norm': 42.52839279174805, 'learning_rate': 1.2114820249449744e-05, 'epoch': 1.73}
{'loss': 4.0971, 'grad_norm': 40.76893615722656, 'learning_rate': 1.1931401320616289e-05, 'epoch': 1.74}
{'loss': 4.7664, 'grad_norm': 28.86395263671875, 'learning_rate': 1.1747982391782832e-05, 'epoch': 1.75}
{'loss': 3.7978, 'grad_norm': 34.22019958496094, 'learning_rate': 1.1564563462949376e-05, 'epoch': 1.76}
{'loss': 4.6807, 'grad_norm': 58.463539123535156, 'learning_rate': 1.1381144534115921e-05, 'epoch': 1.76}
{'loss': 3.9352, 'grad_norm': 50.942996978759766, 'learning_rate': 1.1197725605282464e-05, 'epoch': 1.77}
{'loss': 4.2088, 'grad_norm': 50.67710876464844, 'learning_rate': 1.101430667644901e-05, 'epoch': 1.78}
{'loss': 4.7157, 'grad_norm': 36.27945327758789, 'learning_rate': 1.0830887747615555e-05, 'epoch': 1.79}
{'loss': 3.4726, 'grad_norm': 37.79210662841797, 'learning_rate': 1.0647468818782098e-05, 'epoch': 1.79}
{'loss': 4.6034, 'grad_norm': 27.4325008392334, 'learning_rate': 1.0464049889948643e-05, 'epoch': 1.8}
{'loss': 4.3236, 'grad_norm': 24.600276947021484, 'learning_rate': 1.0280630961115188e-05, 'epoch': 1.81}
{'loss': 4.3269, 'grad_norm': 20.856521606445312, 'learning_rate': 1.0097212032281732e-05, 'epoch': 1.82}
{'loss': 4.9152, 'grad_norm': 57.65233612060547, 'learning_rate': 9.913793103448277e-06, 'epoch': 1.82}
{'loss': 3.8545, 'grad_norm': 48.493003845214844, 'learning_rate': 9.73037417461482e-06, 'epoch': 1.83}
{'loss': 5.0351, 'grad_norm': 45.163543701171875, 'learning_rate': 9.546955245781365e-06, 'epoch': 1.84}
{'loss': 4.3334, 'grad_norm': 38.8947639465332, 'learning_rate': 9.36353631694791e-06, 'epoch': 1.85}
{'loss': 4.5684, 'grad_norm': 56.3184700012207, 'learning_rate': 9.180117388114454e-06, 'epoch': 1.86}
{'loss': 4.922, 'grad_norm': 50.89472579956055, 'learning_rate': 8.996698459281e-06, 'epoch': 1.86}
{'loss': 4.1782, 'grad_norm': 74.0811538696289, 'learning_rate': 8.813279530447543e-06, 'epoch': 1.87}
{'loss': 4.9648, 'grad_norm': 44.375953674316406, 'learning_rate': 8.629860601614088e-06, 'epoch': 1.88}
{'loss': 4.3543, 'grad_norm': 33.58576202392578, 'learning_rate': 8.446441672780631e-06, 'epoch': 1.89}
{'loss': 4.5839, 'grad_norm': 38.0953369140625, 'learning_rate': 8.263022743947176e-06, 'epoch': 1.89}
{'loss': 4.9631, 'grad_norm': 33.903038024902344, 'learning_rate': 8.07960381511372e-06, 'epoch': 1.9}
{'loss': 4.2028, 'grad_norm': 31.90885353088379, 'learning_rate': 7.896184886280263e-06, 'epoch': 1.91}
{'loss': 4.9137, 'grad_norm': 37.555931091308594, 'learning_rate': 7.712765957446808e-06, 'epoch': 1.92}
{'loss': 4.6175, 'grad_norm': 32.8330192565918, 'learning_rate': 7.5293470286133535e-06, 'epoch': 1.93}
{'loss': 4.461, 'grad_norm': 30.245817184448242, 'learning_rate': 7.345928099779898e-06, 'epoch': 1.93}
{'loss': 4.8672, 'grad_norm': 30.529659271240234, 'learning_rate': 7.162509170946442e-06, 'epoch': 1.94}
{'loss': 4.4778, 'grad_norm': 59.19401550292969, 'learning_rate': 6.9790902421129855e-06, 'epoch': 1.95}
{'loss': 4.7906, 'grad_norm': 36.51669692993164, 'learning_rate': 6.795671313279531e-06, 'epoch': 1.96}
{'loss': 4.6927, 'grad_norm': 72.14788818359375, 'learning_rate': 6.612252384446076e-06, 'epoch': 1.96}
{'loss': 4.6961, 'grad_norm': 56.108760833740234, 'learning_rate': 6.428833455612619e-06, 'epoch': 1.97}
{'loss': 4.8298, 'grad_norm': 67.96102142333984, 'learning_rate': 6.245414526779164e-06, 'epoch': 1.98}
{'loss': 4.5311, 'grad_norm': 50.34267044067383, 'learning_rate': 6.061995597945709e-06, 'epoch': 1.99}
{'loss': 4.8069, 'grad_norm': 33.50819778442383, 'learning_rate': 5.878576669112253e-06, 'epoch': 1.99}
{'loss': 4.2891, 'grad_norm': 27.3809814453125, 'learning_rate': 5.695157740278797e-06, 'epoch': 2.0}
{'loss': 4.3542, 'grad_norm': 35.287471771240234, 'learning_rate': 5.5117388114453415e-06, 'epoch': 2.01}
{'loss': 3.7199, 'grad_norm': 22.796222686767578, 'learning_rate': 5.328319882611886e-06, 'epoch': 2.02}
{'loss': 4.3292, 'grad_norm': 47.86580276489258, 'learning_rate': 5.14490095377843e-06, 'epoch': 2.03}
{'loss': 4.276, 'grad_norm': 75.96601104736328, 'learning_rate': 4.961482024944974e-06, 'epoch': 2.03}
{'loss': 3.696, 'grad_norm': 33.68543243408203, 'learning_rate': 4.778063096111519e-06, 'epoch': 2.04}
{'loss': 4.602, 'grad_norm': 26.686628341674805, 'learning_rate': 4.594644167278064e-06, 'epoch': 2.05}
{'loss': 3.62, 'grad_norm': 62.66749954223633, 'learning_rate': 4.411225238444608e-06, 'epoch': 2.06}
{'loss': 4.108, 'grad_norm': 93.64250946044922, 'learning_rate': 4.227806309611152e-06, 'epoch': 2.06}
{'loss': 4.0459, 'grad_norm': 41.11219787597656, 'learning_rate': 4.044387380777697e-06, 'epoch': 2.07}
{'loss': 3.5782, 'grad_norm': 30.72473907470703, 'learning_rate': 3.860968451944241e-06, 'epoch': 2.08}
{'loss': 4.3607, 'grad_norm': 55.07495880126953, 'learning_rate': 3.6775495231107857e-06, 'epoch': 2.09}
{'loss': 3.7404, 'grad_norm': 54.319122314453125, 'learning_rate': 3.4941305942773295e-06, 'epoch': 2.1}
{'loss': 3.9709, 'grad_norm': 36.24697494506836, 'learning_rate': 3.310711665443874e-06, 'epoch': 2.1}
{'loss': 4.2382, 'grad_norm': 43.349365234375, 'learning_rate': 3.127292736610418e-06, 'epoch': 2.11}
{'loss': 3.4146, 'grad_norm': 29.28925323486328, 'learning_rate': 2.943873807776963e-06, 'epoch': 2.12}
{'loss': 4.2829, 'grad_norm': 57.422359466552734, 'learning_rate': 2.760454878943507e-06, 'epoch': 2.13}
{'loss': 3.6191, 'grad_norm': 34.26184844970703, 'learning_rate': 2.5770359501100514e-06, 'epoch': 2.13}
{'loss': 4.2025, 'grad_norm': 38.962162017822266, 'learning_rate': 2.3936170212765957e-06, 'epoch': 2.14}
{'loss': 4.1382, 'grad_norm': 34.59020233154297, 'learning_rate': 2.21019809244314e-06, 'epoch': 2.15}
{'loss': 3.4386, 'grad_norm': 41.11906814575195, 'learning_rate': 2.0267791636096847e-06, 'epoch': 2.16}
{'loss': 4.1075, 'grad_norm': 41.032798767089844, 'learning_rate': 1.8433602347762288e-06, 'epoch': 2.17}
{'loss': 3.5029, 'grad_norm': 42.81684112548828, 'learning_rate': 1.6599413059427735e-06, 'epoch': 2.17}
{'loss': 3.8221, 'grad_norm': 45.11260223388672, 'learning_rate': 1.4765223771093178e-06, 'epoch': 2.18}
{'loss': 4.2058, 'grad_norm': 54.67654037475586, 'learning_rate': 1.293103448275862e-06, 'epoch': 2.19}
{'loss': 3.4462, 'grad_norm': 47.680606842041016, 'learning_rate': 1.1096845194424066e-06, 'epoch': 2.2}
{'loss': 4.2647, 'grad_norm': 50.003013610839844, 'learning_rate': 9.26265590608951e-07, 'epoch': 2.2}
{'loss': 3.1889, 'grad_norm': 43.2178955078125, 'learning_rate': 7.428466617754953e-07, 'epoch': 2.21}
{'loss': 4.0546, 'grad_norm': 41.73713302612305, 'learning_rate': 5.594277329420397e-07, 'epoch': 2.22}
{'loss': 4.0832, 'grad_norm': 39.35430145263672, 'learning_rate': 3.76008804108584e-07, 'epoch': 2.23}
{'loss': 3.1411, 'grad_norm': 29.502229690551758, 'learning_rate': 1.925898752751284e-07, 'epoch': 2.23}
{'loss': 4.1998, 'grad_norm': 55.30076599121094, 'learning_rate': 9.170946441672781e-09, 'epoch': 2.24}
{'train_runtime': 18866.6668, 'train_samples_per_second': 4.919, 'train_steps_per_second': 0.307, 'train_loss': 1.1808863988415947, 'epoch': 2.24}
{'eval': {'eval_loss': 0.3136753737926483, 'eval_log_loss': 1.0247267040881172, 'eval_accuracy': 0.5161991737334203, 'eval_runtime': 1028.1511, 'eval_samples_per_second': 4.473, 'eval_steps_per_second': 4.473, 'epoch': 2.242418384360728}}
