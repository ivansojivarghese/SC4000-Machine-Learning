[Step5] Distilling fold 0 using LLaMA-only OOF probs
{'loss': 5.1928, 'grad_norm': 64.43439483642578, 'learning_rate': 2.0022796352583586e-05, 'epoch': 1.01}
{'loss': 4.647, 'grad_norm': 27.45728302001953, 'learning_rate': 1.976950354609929e-05, 'epoch': 1.02}
{'loss': 4.9794, 'grad_norm': 38.11205291748047, 'learning_rate': 1.9516210739614996e-05, 'epoch': 1.03}
{'loss': 5.0083, 'grad_norm': 36.975921630859375, 'learning_rate': 1.92629179331307e-05, 'epoch': 1.04}
{'loss': 4.8902, 'grad_norm': 41.689735412597656, 'learning_rate': 1.9009625126646405e-05, 'epoch': 1.04}
{'loss': 4.9774, 'grad_norm': 39.82377243041992, 'learning_rate': 1.875633232016211e-05, 'epoch': 1.05}
{'loss': 4.6148, 'grad_norm': 27.674840927124023, 'learning_rate': 1.850303951367781e-05, 'epoch': 1.06}
{'loss': 4.9039, 'grad_norm': 28.023717880249023, 'learning_rate': 1.8249746707193515e-05, 'epoch': 1.07}
{'loss': 4.7913, 'grad_norm': 30.953840255737305, 'learning_rate': 1.799645390070922e-05, 'epoch': 1.07}
{'loss': 4.759, 'grad_norm': 48.610130310058594, 'learning_rate': 1.7743161094224924e-05, 'epoch': 1.08}
{'loss': 4.8097, 'grad_norm': 21.278671264648438, 'learning_rate': 1.748986828774063e-05, 'epoch': 1.09}
{'loss': 4.4513, 'grad_norm': 29.15226936340332, 'learning_rate': 1.7236575481256333e-05, 'epoch': 1.1}
{'loss': 4.9413, 'grad_norm': 27.26885223388672, 'learning_rate': 1.6983282674772038e-05, 'epoch': 1.11}
{'loss': 4.6987, 'grad_norm': 45.70131301879883, 'learning_rate': 1.672998986828774e-05, 'epoch': 1.11}
{'loss': 4.5545, 'grad_norm': 62.771202087402344, 'learning_rate': 1.6476697061803443e-05, 'epoch': 1.12}
{'loss': 4.8209, 'grad_norm': 39.4020881652832, 'learning_rate': 1.6223404255319148e-05, 'epoch': 1.13}
{'loss': 4.3113, 'grad_norm': 51.95104217529297, 'learning_rate': 1.5970111448834852e-05, 'epoch': 1.14}
{'loss': 4.6733, 'grad_norm': 39.22212219238281, 'learning_rate': 1.571681864235056e-05, 'epoch': 1.14}
{'loss': 4.508, 'grad_norm': 33.58982849121094, 'learning_rate': 1.5463525835866265e-05, 'epoch': 1.15}
{'loss': 4.8838, 'grad_norm': 31.39784812927246, 'learning_rate': 1.5210233029381964e-05, 'epoch': 1.16}
{'loss': 4.7733, 'grad_norm': 32.65618896484375, 'learning_rate': 1.4956940222897669e-05, 'epoch': 1.17}
{'loss': 4.4569, 'grad_norm': 75.89849090576172, 'learning_rate': 1.4703647416413373e-05, 'epoch': 1.18}
{'loss': 4.8647, 'grad_norm': 20.349864959716797, 'learning_rate': 1.4450354609929078e-05, 'epoch': 1.18}
{'loss': 4.5161, 'grad_norm': 40.27280044555664, 'learning_rate': 1.4197061803444783e-05, 'epoch': 1.19}
{'loss': 4.6465, 'grad_norm': 39.357093811035156, 'learning_rate': 1.3943768996960487e-05, 'epoch': 1.2}
{'loss': 4.8582, 'grad_norm': 36.62760543823242, 'learning_rate': 1.3690476190476192e-05, 'epoch': 1.21}
{'loss': 4.0075, 'grad_norm': 52.451904296875, 'learning_rate': 1.3437183383991894e-05, 'epoch': 1.21}
{'loss': 4.9073, 'grad_norm': 28.517568588256836, 'learning_rate': 1.3183890577507599e-05, 'epoch': 1.22}
{'loss': 4.0835, 'grad_norm': 35.771575927734375, 'learning_rate': 1.2930597771023304e-05, 'epoch': 1.23}
{'loss': 4.221, 'grad_norm': 49.475425720214844, 'learning_rate': 1.2677304964539008e-05, 'epoch': 1.24}
{'loss': 4.7737, 'grad_norm': 63.95473861694336, 'learning_rate': 1.2424012158054713e-05, 'epoch': 1.24}
{'loss': 4.126, 'grad_norm': 30.483783721923828, 'learning_rate': 1.2170719351570415e-05, 'epoch': 1.25}
{'loss': 5.119, 'grad_norm': 19.641870498657227, 'learning_rate': 1.191742654508612e-05, 'epoch': 1.26}
{'loss': 4.5054, 'grad_norm': 46.49995803833008, 'learning_rate': 1.1664133738601825e-05, 'epoch': 1.27}
{'loss': 4.9196, 'grad_norm': 28.746267318725586, 'learning_rate': 1.1410840932117527e-05, 'epoch': 1.28}
{'loss': 4.7843, 'grad_norm': 28.90353775024414, 'learning_rate': 1.1157548125633232e-05, 'epoch': 1.28}
{'loss': 4.2014, 'grad_norm': 18.49409294128418, 'learning_rate': 1.0904255319148937e-05, 'epoch': 1.29}
{'loss': 4.7796, 'grad_norm': 21.454998016357422, 'learning_rate': 1.0650962512664641e-05, 'epoch': 1.3}
{'loss': 4.7446, 'grad_norm': 32.224998474121094, 'learning_rate': 1.0397669706180346e-05, 'epoch': 1.31}
{'loss': 4.7153, 'grad_norm': 45.12941360473633, 'learning_rate': 1.014437689969605e-05, 'epoch': 1.31}
{'loss': 4.8642, 'grad_norm': 28.392576217651367, 'learning_rate': 9.891084093211753e-06, 'epoch': 1.32}
{'loss': 4.0468, 'grad_norm': 44.39493179321289, 'learning_rate': 9.637791286727458e-06, 'epoch': 1.33}
{'loss': 5.0518, 'grad_norm': 28.21890640258789, 'learning_rate': 9.384498480243162e-06, 'epoch': 1.34}
{'loss': 4.5808, 'grad_norm': 23.750808715820312, 'learning_rate': 9.131205673758867e-06, 'epoch': 1.35}
{'loss': 4.6727, 'grad_norm': 42.353328704833984, 'learning_rate': 8.87791286727457e-06, 'epoch': 1.35}
{'loss': 5.0503, 'grad_norm': 23.360429763793945, 'learning_rate': 8.624620060790274e-06, 'epoch': 1.36}
{'loss': 4.5085, 'grad_norm': 24.317115783691406, 'learning_rate': 8.371327254305979e-06, 'epoch': 1.37}
{'loss': 4.9308, 'grad_norm': 49.78044128417969, 'learning_rate': 8.118034447821681e-06, 'epoch': 1.38}
{'loss': 4.7607, 'grad_norm': 29.262855529785156, 'learning_rate': 7.864741641337386e-06, 'epoch': 1.38}
{'loss': 4.3358, 'grad_norm': 24.0731143951416, 'learning_rate': 7.611448834853091e-06, 'epoch': 1.39}
{'loss': 4.5165, 'grad_norm': 35.45122146606445, 'learning_rate': 7.358156028368794e-06, 'epoch': 1.4}
{'loss': 3.8596, 'grad_norm': 17.84792709350586, 'learning_rate': 7.104863221884499e-06, 'epoch': 1.41}
{'loss': 4.6742, 'grad_norm': 46.626590728759766, 'learning_rate': 6.851570415400203e-06, 'epoch': 1.41}
{'loss': 4.3297, 'grad_norm': 36.28813171386719, 'learning_rate': 6.598277608915906e-06, 'epoch': 1.42}
{'loss': 4.0068, 'grad_norm': 55.72978973388672, 'learning_rate': 6.344984802431611e-06, 'epoch': 1.43}
{'loss': 4.6387, 'grad_norm': 19.612825393676758, 'learning_rate': 6.091691995947316e-06, 'epoch': 1.44}
{'loss': 3.606, 'grad_norm': 40.47759246826172, 'learning_rate': 5.83839918946302e-06, 'epoch': 1.45}
{'loss': 4.6571, 'grad_norm': 20.292537689208984, 'learning_rate': 5.5851063829787235e-06, 'epoch': 1.45}
{'loss': 4.1265, 'grad_norm': 28.27553939819336, 'learning_rate': 5.331813576494428e-06, 'epoch': 1.46}
{'loss': 4.1854, 'grad_norm': 37.13249969482422, 'learning_rate': 5.078520770010132e-06, 'epoch': 1.47}
{'loss': 4.6071, 'grad_norm': 27.400218963623047, 'learning_rate': 4.825227963525836e-06, 'epoch': 1.48}
{'loss': 3.6102, 'grad_norm': 38.20467758178711, 'learning_rate': 4.57193515704154e-06, 'epoch': 1.48}
{'loss': 4.6492, 'grad_norm': 31.213064193725586, 'learning_rate': 4.3186423505572445e-06, 'epoch': 1.49}
{'loss': 4.2983, 'grad_norm': 35.23072814941406, 'learning_rate': 4.065349544072949e-06, 'epoch': 1.5}
{'loss': 4.2851, 'grad_norm': 51.53449249267578, 'learning_rate': 3.8120567375886527e-06, 'epoch': 1.51}
{'loss': 4.6564, 'grad_norm': 34.750732421875, 'learning_rate': 3.5587639311043564e-06, 'epoch': 1.52}
{'loss': 3.5785, 'grad_norm': 31.98526382446289, 'learning_rate': 3.305471124620061e-06, 'epoch': 1.52}
{'loss': 4.7143, 'grad_norm': 21.814523696899414, 'learning_rate': 3.052178318135765e-06, 'epoch': 1.53}
{'loss': 4.0651, 'grad_norm': 37.13748550415039, 'learning_rate': 2.798885511651469e-06, 'epoch': 1.54}
{'loss': 4.1461, 'grad_norm': 45.681644439697266, 'learning_rate': 2.5455927051671733e-06, 'epoch': 1.55}
{'loss': 4.4594, 'grad_norm': 39.50074005126953, 'learning_rate': 2.2922998986828774e-06, 'epoch': 1.55}
{'loss': 3.8905, 'grad_norm': 28.96547508239746, 'learning_rate': 2.039007092198582e-06, 'epoch': 1.56}
{'loss': 4.6007, 'grad_norm': 35.66060256958008, 'learning_rate': 1.7857142857142857e-06, 'epoch': 1.57}
{'loss': 4.3949, 'grad_norm': 66.91216278076172, 'learning_rate': 1.53242147922999e-06, 'epoch': 1.58}
{'loss': 3.9525, 'grad_norm': 45.05287551879883, 'learning_rate': 1.2791286727456941e-06, 'epoch': 1.58}
{'loss': 4.4351, 'grad_norm': 26.717655181884766, 'learning_rate': 1.0258358662613983e-06, 'epoch': 1.59}
{'loss': 3.7528, 'grad_norm': 33.84568405151367, 'learning_rate': 7.725430597771024e-07, 'epoch': 1.6}
{'loss': 4.4837, 'grad_norm': 27.80093002319336, 'learning_rate': 5.192502532928066e-07, 'epoch': 1.61}
{'loss': 4.176, 'grad_norm': 27.544275283813477, 'learning_rate': 2.6595744680851066e-07, 'epoch': 1.62}
{'loss': 4.0352, 'grad_norm': 50.95505905151367, 'learning_rate': 1.2664640324214793e-08, 'epoch': 1.62}
{'train_runtime': 18786.4443, 'train_samples_per_second': 3.577, 'train_steps_per_second': 0.224, 'train_loss': 1.721977277483259, 'epoch': 1.62}
{'eval': {'eval_loss': 0.3029170036315918, 'eval_log_loss': 0.9903737751267565, 'eval_accuracy': 0.5198956294846706, 'eval_runtime': 1043.4694, 'eval_samples_per_second': 4.407, 'eval_steps_per_second': 4.407, 'epoch': 1.6236527959015996}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
