[Step5] Distilling fold 4 using LLaMA-only OOF probs
{'loss': 17.9878, 'grad_norm': 109.45025634765625, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.01}
{'loss': 16.0184, 'grad_norm': 273.57861328125, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
{'loss': 14.4493, 'grad_norm': 102.8779067993164, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.02}
{'loss': 21.0373, 'grad_norm': 179.53846740722656, 'learning_rate': 4.898936170212766e-05, 'epoch': 0.03}
{'loss': 14.7686, 'grad_norm': 177.9767303466797, 'learning_rate': 4.792553191489362e-05, 'epoch': 0.04}
{'loss': 17.3073, 'grad_norm': 371.44830322265625, 'learning_rate': 4.686170212765958e-05, 'epoch': 0.05}
{'loss': 12.5779, 'grad_norm': 120.15235137939453, 'learning_rate': 4.579787234042554e-05, 'epoch': 0.05}
{'loss': 14.5145, 'grad_norm': 185.55809020996094, 'learning_rate': 4.473404255319149e-05, 'epoch': 0.06}
{'loss': 17.3564, 'grad_norm': 181.4838104248047, 'learning_rate': 4.367021276595745e-05, 'epoch': 0.07}
{'loss': 14.9616, 'grad_norm': 90.76773071289062, 'learning_rate': 4.2606382978723406e-05, 'epoch': 0.08}
{'loss': 12.2092, 'grad_norm': 148.28933715820312, 'learning_rate': 4.1542553191489364e-05, 'epoch': 0.09}
{'loss': 13.6905, 'grad_norm': 65.885009765625, 'learning_rate': 4.047872340425532e-05, 'epoch': 0.09}
{'loss': 15.7632, 'grad_norm': 145.04127502441406, 'learning_rate': 3.941489361702128e-05, 'epoch': 0.1}
{'loss': 14.5741, 'grad_norm': 111.99650573730469, 'learning_rate': 3.835106382978724e-05, 'epoch': 0.11}
{'loss': 12.9618, 'grad_norm': 81.95108795166016, 'learning_rate': 3.728723404255319e-05, 'epoch': 0.12}
{'loss': 15.6899, 'grad_norm': 82.78279113769531, 'learning_rate': 3.622340425531915e-05, 'epoch': 0.12}
{'loss': 13.5185, 'grad_norm': 156.1639862060547, 'learning_rate': 3.515957446808511e-05, 'epoch': 0.13}
{'loss': 14.8961, 'grad_norm': 71.5279541015625, 'learning_rate': 3.4095744680851066e-05, 'epoch': 0.14}
{'loss': 12.7119, 'grad_norm': 81.0345687866211, 'learning_rate': 3.3031914893617025e-05, 'epoch': 0.15}
{'loss': 11.8763, 'grad_norm': 228.41769409179688, 'learning_rate': 3.1968085106382976e-05, 'epoch': 0.15}
{'loss': 12.6778, 'grad_norm': 35.934810638427734, 'learning_rate': 3.090425531914894e-05, 'epoch': 0.16}
{'loss': 11.7105, 'grad_norm': 91.17263793945312, 'learning_rate': 2.9840425531914897e-05, 'epoch': 0.17}
{'loss': 15.8234, 'grad_norm': 61.70143127441406, 'learning_rate': 2.877659574468085e-05, 'epoch': 0.18}
{'loss': 13.6401, 'grad_norm': 85.955078125, 'learning_rate': 2.7712765957446813e-05, 'epoch': 0.19}
{'loss': 11.6597, 'grad_norm': 85.21460723876953, 'learning_rate': 2.664893617021277e-05, 'epoch': 0.19}
{'loss': 11.6563, 'grad_norm': 125.45970916748047, 'learning_rate': 2.5585106382978723e-05, 'epoch': 0.2}
{'loss': 11.9083, 'grad_norm': 54.05826950073242, 'learning_rate': 2.4521276595744682e-05, 'epoch': 0.21}
{'loss': 14.0485, 'grad_norm': 34.87657165527344, 'learning_rate': 2.345744680851064e-05, 'epoch': 0.22}
{'loss': 11.3802, 'grad_norm': 31.870351791381836, 'learning_rate': 2.23936170212766e-05, 'epoch': 0.22}
{'loss': 14.9995, 'grad_norm': 21.65926170349121, 'learning_rate': 2.1329787234042554e-05, 'epoch': 0.23}
{'loss': 11.6856, 'grad_norm': 66.50714874267578, 'learning_rate': 2.0265957446808512e-05, 'epoch': 0.24}
{'loss': 15.1393, 'grad_norm': 156.25677490234375, 'learning_rate': 1.920212765957447e-05, 'epoch': 0.25}
{'loss': 14.1998, 'grad_norm': 16.25628662109375, 'learning_rate': 1.8138297872340425e-05, 'epoch': 0.26}
{'loss': 14.8159, 'grad_norm': 160.39263916015625, 'learning_rate': 1.7074468085106384e-05, 'epoch': 0.26}
{'loss': 12.3443, 'grad_norm': 152.62945556640625, 'learning_rate': 1.6010638297872342e-05, 'epoch': 0.27}
{'loss': 11.6225, 'grad_norm': 65.88513946533203, 'learning_rate': 1.4946808510638299e-05, 'epoch': 0.28}
{'loss': 16.5187, 'grad_norm': 257.3622131347656, 'learning_rate': 1.3882978723404256e-05, 'epoch': 0.29}
{'loss': 9.9541, 'grad_norm': 57.800960540771484, 'learning_rate': 1.2819148936170214e-05, 'epoch': 0.29}
{'loss': 11.2771, 'grad_norm': 138.41317749023438, 'learning_rate': 1.175531914893617e-05, 'epoch': 0.3}
{'loss': 12.442, 'grad_norm': 80.43669891357422, 'learning_rate': 1.0691489361702128e-05, 'epoch': 0.31}
{'loss': 13.2355, 'grad_norm': 58.127342224121094, 'learning_rate': 9.627659574468086e-06, 'epoch': 0.32}
{'loss': 13.8661, 'grad_norm': 293.27685546875, 'learning_rate': 8.563829787234043e-06, 'epoch': 0.32}
{'loss': 12.2908, 'grad_norm': 53.28660583496094, 'learning_rate': 7.5e-06, 'epoch': 0.33}
{'loss': 15.4474, 'grad_norm': 129.87637329101562, 'learning_rate': 6.436170212765958e-06, 'epoch': 0.34}
{'loss': 12.7913, 'grad_norm': 55.47572326660156, 'learning_rate': 5.372340425531915e-06, 'epoch': 0.35}
{'loss': 11.5123, 'grad_norm': 55.42429733276367, 'learning_rate': 4.308510638297873e-06, 'epoch': 0.36}
{'loss': 13.5649, 'grad_norm': 38.463836669921875, 'learning_rate': 3.2446808510638296e-06, 'epoch': 0.36}
{'loss': 12.9834, 'grad_norm': 113.4072494506836, 'learning_rate': 2.1808510638297876e-06, 'epoch': 0.37}
{'loss': 12.5399, 'grad_norm': 56.92491912841797, 'learning_rate': 1.1170212765957447e-06, 'epoch': 0.38}
{'loss': 12.1552, 'grad_norm': 165.37704467773438, 'learning_rate': 5.319148936170213e-08, 'epoch': 0.39}
{'train_runtime': 11739.2085, 'train_samples_per_second': 1.363, 'train_steps_per_second': 0.085, 'train_loss': 13.77521954345703, 'epoch': 0.39}
{'eval': {'eval_loss': 0.7986236214637756, 'eval_log_loss': 1.0925643477891898, 'eval_accuracy': 0.43052837573385516, 'eval_runtime': 1010.591, 'eval_samples_per_second': 4.551, 'eval_steps_per_second': 4.551, 'epoch': 0.3866321919628833}}
[Step5] Done fold 4 -> model_save/distilled_gemma2-9b_fold_4
