[Step1] Resolving SCRATCH_BASE...
[Debug] SCRATCH_BASE resolved to: /scratch-shared/tc1proj005
[Step1] Working dir: /home/UG/ivansoji001/exported-assets_sc4000
[Step1] Home dir: /home/UG/ivansoji001
[Step1] SCRATCH_BASE: /scratch-shared/tc1proj005
[Step1] HF_HOME: /scratch-shared/tc1proj005
[Step1] HUGGINGFACE_HUB_CACHE: /scratch-shared/tc1proj005
[Step1] HF_DATASETS_CACHE: /scratch-shared/tc1proj005
[Step1] TRANSFORMERS_CACHE: /scratch-shared/tc1proj005
[Step1] UT_DATA: data/ultrafeedback.csv | EPOCHS: 1 | LR: 1e-5 | MAXLEN: 512 | PER_DEVICE_BS: 1 | MAX_STEPS: -1 | LORA_R: 32 | LORA_ALPHA: 64
[Step1][AutoCap] Projected 500 steps (~6h) exceeds budget 6h; capping to 432.
[Step1][Force] Overriding max steps to 370 (ignoring POSTPRE_MAX_STEPS=432)
[Step1] Deps OK
[Step1] Online mode enabled (TRANSFORMERS_OFFLINE=0)
[Step1] Downloading tokenizer files for Qwen/Qwen2.5-14B...
[Step1] Tokenizer snapshot at: /scratch-shared/tc1proj005/models--Qwen--Qwen2.5-14B/snapshots/97e1e76335b7017d8f67c08a19d103c0504298c9
[Step1] Prefetched tokenizer: vocab size 151665
[Step1] Starting post-pretraining (LoRA/QLoRA)
[Step1] Selected stage: llama
[Step1] Disk check for /scratch-shared/tc1proj005: need 180G, avail 85363G
[Step1] Cleaning HF cache at /scratch-shared/tc1proj005
[Step1] Skipping cache clean to avoid wiping SCRATCH_BASE root
[Step1] meta-llama/Meta-Llama-3.1-8B QLoRA: starting
[Step1] LLaMA tokenizer snapshot at: /scratch-shared/tc1proj005/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b
[Step1] LLaMA args: --base-model meta-llama/Meta-Llama-3.1-8B --output-dir /scratch-shared/tc1proj005/post_pretrain_llama3-8b_lora --data-path data/ultrafeedback.csv --tokenizer-path /scratch-shared/tc1proj005/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b --bf16 --qlora --epochs 1 --lr 1e-5 --max-length 512 --r 32 --lora-alpha 64 --per-device-batch 1 --grad-accum 12 --subset-size 6000 --max-steps 370 --grad-checkpoint
[DEBUG] Trainable params: 83,886,080 / 4,624,486,400 (1.8140%)
[INFO] Subsetting training data: 6000 / 157675 examples (requested 6000)
{'loss': 0.979, 'grad_norm': 0.555529773235321, 'learning_rate': 9.486486486486487e-06, 'epoch': 0.04}
{'loss': 0.8983, 'grad_norm': 0.40820804238319397, 'learning_rate': 8.945945945945946e-06, 'epoch': 0.08}
{'loss': 0.8596, 'grad_norm': 0.3913278579711914, 'learning_rate': 8.405405405405406e-06, 'epoch': 0.12}
{'loss': 0.854, 'grad_norm': 0.4913256764411926, 'learning_rate': 7.864864864864866e-06, 'epoch': 0.16}
{'loss': 0.845, 'grad_norm': 0.5135613679885864, 'learning_rate': 7.324324324324325e-06, 'epoch': 0.2}
{'loss': 0.8526, 'grad_norm': 0.4642426371574402, 'learning_rate': 6.783783783783784e-06, 'epoch': 0.24}
{'loss': 0.8265, 'grad_norm': 0.42425498366355896, 'learning_rate': 6.243243243243243e-06, 'epoch': 0.28}
{'loss': 0.7997, 'grad_norm': 0.47830235958099365, 'learning_rate': 5.702702702702702e-06, 'epoch': 0.32}
{'loss': 0.8266, 'grad_norm': 0.49089670181274414, 'learning_rate': 5.162162162162162e-06, 'epoch': 0.36}
{'loss': 0.8534, 'grad_norm': 0.5166736245155334, 'learning_rate': 4.621621621621622e-06, 'epoch': 0.4}
{'loss': 0.7685, 'grad_norm': 0.4583915174007416, 'learning_rate': 4.0810810810810815e-06, 'epoch': 0.44}
{'loss': 0.8025, 'grad_norm': 0.5515820980072021, 'learning_rate': 3.5405405405405408e-06, 'epoch': 0.48}
{'loss': 0.7999, 'grad_norm': 0.5601966381072998, 'learning_rate': 3e-06, 'epoch': 0.52}
{'loss': 0.8379, 'grad_norm': 0.5332500338554382, 'learning_rate': 2.45945945945946e-06, 'epoch': 0.56}
{'loss': 0.8029, 'grad_norm': 0.5267394185066223, 'learning_rate': 1.918918918918919e-06, 'epoch': 0.6}
{'loss': 0.7895, 'grad_norm': 0.5675305724143982, 'learning_rate': 1.3783783783783786e-06, 'epoch': 0.64}
{'loss': 0.7822, 'grad_norm': 0.5228391885757446, 'learning_rate': 8.37837837837838e-07, 'epoch': 0.68}
{'loss': 0.7958, 'grad_norm': 0.5756637454032898, 'learning_rate': 2.972972972972973e-07, 'epoch': 0.72}
{'train_runtime': 16948.017, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.022, 'train_loss': 0.830865595791791, 'epoch': 0.74}
[Merge] Saving merged model to /scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged
[Step1] Stage 'llama' completed
