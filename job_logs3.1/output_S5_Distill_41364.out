[Step5] Distilling fold 4 using LLaMA-only OOF probs
{'loss': 1.287, 'grad_norm': 10.036688804626465, 'learning_rate': 1.7338591342626558e-05, 'epoch': 0.26}
{'loss': 1.2872, 'grad_norm': 9.342948913574219, 'learning_rate': 1.7063462949376377e-05, 'epoch': 0.26}
{'loss': 1.2429, 'grad_norm': 23.926700592041016, 'learning_rate': 1.6788334556126193e-05, 'epoch': 0.27}
{'loss': 1.1303, 'grad_norm': 11.863813400268555, 'learning_rate': 1.651320616287601e-05, 'epoch': 0.27}
{'loss': 1.3624, 'grad_norm': 11.357643127441406, 'learning_rate': 1.6238077769625825e-05, 'epoch': 0.28}
{'loss': 1.3141, 'grad_norm': 18.712596893310547, 'learning_rate': 1.596294937637564e-05, 'epoch': 0.28}
{'loss': 1.3211, 'grad_norm': 18.60700035095215, 'learning_rate': 1.568782098312546e-05, 'epoch': 0.29}
{'loss': 1.2964, 'grad_norm': 18.01850128173828, 'learning_rate': 1.5412692589875276e-05, 'epoch': 0.29}
{'loss': 1.3195, 'grad_norm': 39.392967224121094, 'learning_rate': 1.513756419662509e-05, 'epoch': 0.29}
{'loss': 1.2448, 'grad_norm': 22.184917449951172, 'learning_rate': 1.4862435803374908e-05, 'epoch': 0.3}
{'loss': 1.2402, 'grad_norm': 31.656211853027344, 'learning_rate': 1.4587307410124724e-05, 'epoch': 0.3}
{'loss': 1.1284, 'grad_norm': 24.877206802368164, 'learning_rate': 1.4312179016874542e-05, 'epoch': 0.31}
{'loss': 1.3458, 'grad_norm': 21.15425682067871, 'learning_rate': 1.403705062362436e-05, 'epoch': 0.31}
{'loss': 1.293, 'grad_norm': 12.891315460205078, 'learning_rate': 1.3761922230374174e-05, 'epoch': 0.32}
{'loss': 1.3064, 'grad_norm': 27.0860595703125, 'learning_rate': 1.3486793837123992e-05, 'epoch': 0.32}
{'loss': 1.2993, 'grad_norm': 6.541109085083008, 'learning_rate': 1.3211665443873808e-05, 'epoch': 0.33}
{'loss': 1.3325, 'grad_norm': 8.401623725891113, 'learning_rate': 1.2936537050623626e-05, 'epoch': 0.33}
{'loss': 1.2682, 'grad_norm': 18.888856887817383, 'learning_rate': 1.266140865737344e-05, 'epoch': 0.34}
{'loss': 1.2386, 'grad_norm': 18.507530212402344, 'learning_rate': 1.2386280264123258e-05, 'epoch': 0.34}
{'loss': 1.1533, 'grad_norm': 6.9218339920043945, 'learning_rate': 1.2111151870873075e-05, 'epoch': 0.35}
{'loss': 1.2352, 'grad_norm': 11.848899841308594, 'learning_rate': 1.1836023477622891e-05, 'epoch': 0.35}
{'loss': 1.2795, 'grad_norm': 10.75385856628418, 'learning_rate': 1.1560895084372707e-05, 'epoch': 0.36}
{'loss': 1.291, 'grad_norm': 14.922247886657715, 'learning_rate': 1.1285766691122523e-05, 'epoch': 0.36}
{'loss': 1.2771, 'grad_norm': 33.02358627319336, 'learning_rate': 1.1010638297872341e-05, 'epoch': 0.37}
{'loss': 1.2888, 'grad_norm': 16.20008087158203, 'learning_rate': 1.0735509904622157e-05, 'epoch': 0.37}
{'loss': 1.2242, 'grad_norm': 13.432618141174316, 'learning_rate': 1.0460381511371973e-05, 'epoch': 0.38}
{'loss': 1.2264, 'grad_norm': 20.597190856933594, 'learning_rate': 1.018525311812179e-05, 'epoch': 0.38}
{'loss': 1.1943, 'grad_norm': 11.400456428527832, 'learning_rate': 9.910124724871607e-06, 'epoch': 0.39}
{'loss': 1.2228, 'grad_norm': 11.157526016235352, 'learning_rate': 9.634996331621424e-06, 'epoch': 0.39}
{'loss': 1.2758, 'grad_norm': 45.606834411621094, 'learning_rate': 9.359867938371239e-06, 'epoch': 0.4}
{'loss': 1.2587, 'grad_norm': 26.66791534423828, 'learning_rate': 9.084739545121056e-06, 'epoch': 0.4}
{'loss': 1.2755, 'grad_norm': 15.613382339477539, 'learning_rate': 8.809611151870872e-06, 'epoch': 0.41}
{'loss': 1.2979, 'grad_norm': 9.014426231384277, 'learning_rate': 8.53448275862069e-06, 'epoch': 0.41}
{'loss': 1.2598, 'grad_norm': 11.592894554138184, 'learning_rate': 8.259354365370508e-06, 'epoch': 0.42}
{'loss': 1.1727, 'grad_norm': 8.134242057800293, 'learning_rate': 7.984225972120322e-06, 'epoch': 0.42}
{'loss': 1.1185, 'grad_norm': 6.9806928634643555, 'learning_rate': 7.70909757887014e-06, 'epoch': 0.43}
{'loss': 1.3049, 'grad_norm': 14.418379783630371, 'learning_rate': 7.433969185619957e-06, 'epoch': 0.43}
{'loss': 1.2944, 'grad_norm': 13.70157527923584, 'learning_rate': 7.158840792369773e-06, 'epoch': 0.43}
{'loss': 1.2328, 'grad_norm': 15.666878700256348, 'learning_rate': 6.88371239911959e-06, 'epoch': 0.44}
{'loss': 1.2487, 'grad_norm': 12.967972755432129, 'learning_rate': 6.608584005869406e-06, 'epoch': 0.44}
{'loss': 1.2408, 'grad_norm': 17.87685203552246, 'learning_rate': 6.3334556126192225e-06, 'epoch': 0.45}
{'loss': 1.165, 'grad_norm': 26.032703399658203, 'learning_rate': 6.0583272193690385e-06, 'epoch': 0.45}
{'loss': 1.1952, 'grad_norm': 16.781240463256836, 'learning_rate': 5.783198826118856e-06, 'epoch': 0.46}
{'loss': 1.1168, 'grad_norm': 16.17180824279785, 'learning_rate': 5.508070432868672e-06, 'epoch': 0.46}
{'loss': 1.2224, 'grad_norm': 23.995756149291992, 'learning_rate': 5.232942039618489e-06, 'epoch': 0.47}
{'loss': 1.2138, 'grad_norm': 25.16258430480957, 'learning_rate': 4.957813646368305e-06, 'epoch': 0.47}
{'loss': 1.3224, 'grad_norm': 12.766989707946777, 'learning_rate': 4.682685253118122e-06, 'epoch': 0.48}
{'loss': 1.2389, 'grad_norm': 15.076092720031738, 'learning_rate': 4.407556859867938e-06, 'epoch': 0.48}
{'loss': 1.2315, 'grad_norm': 12.084293365478516, 'learning_rate': 4.132428466617756e-06, 'epoch': 0.49}
{'loss': 1.1621, 'grad_norm': 12.940094947814941, 'learning_rate': 3.857300073367572e-06, 'epoch': 0.49}
{'loss': 1.1421, 'grad_norm': 18.512733459472656, 'learning_rate': 3.582171680117388e-06, 'epoch': 0.5}
{'loss': 1.121, 'grad_norm': 17.90509605407715, 'learning_rate': 3.3070432868672046e-06, 'epoch': 0.5}
{'loss': 1.2064, 'grad_norm': 14.864315032958984, 'learning_rate': 3.031914893617021e-06, 'epoch': 0.51}
{'loss': 1.267, 'grad_norm': 14.928359985351562, 'learning_rate': 2.7567865003668383e-06, 'epoch': 0.51}
{'loss': 1.1989, 'grad_norm': 36.269561767578125, 'learning_rate': 2.4816581071166547e-06, 'epoch': 0.52}
{'loss': 1.2024, 'grad_norm': 23.333404541015625, 'learning_rate': 2.206529713866471e-06, 'epoch': 0.52}
{'loss': 1.246, 'grad_norm': 13.644782066345215, 'learning_rate': 1.931401320616288e-06, 'epoch': 0.53}
{'loss': 1.121, 'grad_norm': 21.418155670166016, 'learning_rate': 1.6562729273661042e-06, 'epoch': 0.53}
{'loss': 1.1545, 'grad_norm': 9.904226303100586, 'learning_rate': 1.3811445341159207e-06, 'epoch': 0.54}
{'loss': 1.1015, 'grad_norm': 24.918798446655273, 'learning_rate': 1.1060161408657373e-06, 'epoch': 0.54}
{'loss': 1.3172, 'grad_norm': 24.600261688232422, 'learning_rate': 8.30887747615554e-07, 'epoch': 0.55}
{'loss': 1.2556, 'grad_norm': 28.373254776000977, 'learning_rate': 5.557593543653705e-07, 'epoch': 0.55}
{'loss': 1.2224, 'grad_norm': 15.212663650512695, 'learning_rate': 2.806309611151871e-07, 'epoch': 0.56}
{'loss': 1.2116, 'grad_norm': 23.483915328979492, 'learning_rate': 5.502567865003668e-09, 'epoch': 0.56}
{'train_runtime': 13082.3751, 'train_samples_per_second': 1.773, 'train_steps_per_second': 0.443, 'train_loss': 0.6833356666564941, 'epoch': 0.56}
{'eval': {'eval_loss': 0.30512604117393494, 'eval_log_loss': 1.0044500499506468, 'eval_accuracy': 0.5003261578604045, 'eval_runtime': 1362.4452, 'eval_samples_per_second': 3.376, 'eval_steps_per_second': 0.422, 'epoch': 0.5606166783461808}}
[Step5] Done fold 4 -> model_save/distilled_gemma2-9b_fold_4
