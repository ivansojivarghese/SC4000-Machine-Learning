[Step1] Resolving SCRATCH_BASE...
[Debug] SCRATCH_BASE resolved to: /scratch-shared/tc1proj005
[Step1] Working dir: /home/UG/ivansoji001/exported-assets_sc4000
[Step1] Home dir: /home/UG/ivansoji001
[Step1] SCRATCH_BASE: /scratch-shared/tc1proj005
[Step1] HF_HOME: /scratch-shared/tc1proj005
[Step1] HUGGINGFACE_HUB_CACHE: /scratch-shared/tc1proj005
[Step1] HF_DATASETS_CACHE: /scratch-shared/tc1proj005
[Step1] TRANSFORMERS_CACHE: /scratch-shared/tc1proj005
[Step1] UT_DATA: data/ultrafeedback.csv | EPOCHS: 1 | LR: 1e-5 | MAXLEN: 1024 | PER_DEVICE_BS: 1 | MAX_STEPS: -1 | LORA_R: 64 | LORA_ALPHA: 128
[Step1][AutoCap] Projected 84471 steps (~516h) exceeds budget 6h; capping to 981.
[Step1] Deps OK
[Step1] Online mode enabled (TRANSFORMERS_OFFLINE=0)
[Step1] Downloading tokenizer files for Qwen/Qwen2.5-7B-Instruct...
[Step1] Tokenizer snapshot at: /scratch-shared/tc1proj005/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28
[Step1] Prefetched tokenizer: vocab size 151665
[Step1] Starting post-pretraining (LoRA/QLoRA)
[Step1] Selected stage: gemma
[Step1] Disk check for /scratch-shared/tc1proj005: need 40G, avail 82446G
[Step1] Cleaning HF cache at /scratch-shared/tc1proj005
[Step1] Skipping cache clean to avoid wiping SCRATCH_BASE root
[Step1] Gemma2-9B LoRA: starting
[Step1] Gemma args: --base-model google/gemma-2-9b --output-dir /scratch-shared/tc1proj005/post_pretrain_gemma2-9b_lora --data-path data/ultrafeedback.csv --tokenizer-path google/gemma-2-9b --bf16 --attn-impl eager --load-8bit --grad-accum 16 --subset-size 0 --epochs 1 --lr 1e-5 --max-length 1024 --r 64 --lora-alpha 128 --per-device-batch 1 --grad-checkpoint --max-steps 100
[DEBUG] Trainable params: 216,082,944 / 9,457,799,680 (2.2847%)
[INFO] subset_size=0 -> ignoring and using full dataset of 157675 examples
{'loss': 18.0391, 'grad_norm': 731.33349609375, 'learning_rate': 8.1e-06, 'epoch': 0.0}
{'loss': 14.3134, 'grad_norm': 411.6390075683594, 'learning_rate': 6.1e-06, 'epoch': 0.0}
{'loss': 13.8549, 'grad_norm': 291.0690612792969, 'learning_rate': 4.1e-06, 'epoch': 0.0}
{'loss': 13.9307, 'grad_norm': 569.5524291992188, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
{'loss': 12.6872, 'grad_norm': 492.89599609375, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.01}
{'train_runtime': 11412.2055, 'train_samples_per_second': 0.14, 'train_steps_per_second': 0.009, 'train_loss': 14.565045166015626, 'epoch': 0.01}
[INFO] Wrote training_summary.json
[Merge] Saving merged model to /scratch-shared/tc1proj005/post_pretrain_gemma2-9b_merged
[Step1] Stage 'gemma' completed
