/home/UG/ivansoji001/.local/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
/home/UG/ivansoji001/.local/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]/home/UG/ivansoji001/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 662.16it/s]
/home/UG/ivansoji001/.local/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:13,  2.00s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:06<00:19,  3.26s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:10<00:18,  3.66s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:14<00:15,  3.84s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:18<00:11,  3.96s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:22<00:08,  4.03s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:27<00:04,  4.24s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:29<00:00,  3.56s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:29<00:00,  3.69s/it]
Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-9b and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/100 [00:00<?, ?it/s]/home/UG/ivansoji001/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/UG/ivansoji001/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  1%|          | 1/100 [01:17<2:07:56, 77.54s/it]  2%|▏         | 2/100 [02:58<2:29:02, 91.25s/it]  3%|▎         | 3/100 [04:42<2:36:59, 97.11s/it]  4%|▍         | 4/100 [06:26<2:39:56, 99.96s/it]  5%|▌         | 5/100 [08:10<2:40:09, 101.16s/it]  6%|▌         | 6/100 [10:02<2:44:36, 105.07s/it]  7%|▋         | 7/100 [11:52<2:45:15, 106.62s/it]  8%|▊         | 8/100 [13:39<2:43:28, 106.61s/it]  9%|▉         | 9/100 [15:27<2:42:36, 107.21s/it] 10%|█         | 10/100 [17:17<2:41:58, 107.99s/it] 11%|█         | 11/100 [19:07<2:40:54, 108.48s/it] 12%|█▏        | 12/100 [20:57<2:40:01, 109.11s/it] 13%|█▎        | 13/100 [22:45<2:37:47, 108.82s/it] 14%|█▍        | 14/100 [24:36<2:36:38, 109.29s/it] 15%|█▌        | 15/100 [26:21<2:33:21, 108.25s/it] 16%|█▌        | 16/100 [28:02<2:28:15, 105.90s/it] 17%|█▋        | 17/100 [29:48<2:26:31, 105.92s/it] 18%|█▊        | 18/100 [31:34<2:25:01, 106.11s/it] 19%|█▉        | 19/100 [33:21<2:23:36, 106.37s/it] 20%|██        | 20/100 [35:08<2:21:45, 106.32s/it]                                                    20%|██        | 20/100 [35:08<2:21:45, 106.32s/it] 21%|██        | 21/100 [36:55<2:20:35, 106.78s/it] 22%|██▏       | 22/100 [38:41<2:18:21, 106.43s/it] 23%|██▎       | 23/100 [40:29<2:17:21, 107.03s/it] 24%|██▍       | 24/100 [42:17<2:15:37, 107.07s/it] 25%|██▌       | 25/100 [44:05<2:14:27, 107.57s/it] 26%|██▌       | 26/100 [45:54<2:12:58, 107.82s/it] 27%|██▋       | 27/100 [47:35<2:08:47, 105.86s/it] 28%|██▊       | 28/100 [49:21<2:07:11, 105.99s/it] 29%|██▉       | 29/100 [51:10<2:06:14, 106.68s/it] 30%|███       | 30/100 [52:56<2:04:13, 106.47s/it] 31%|███       | 31/100 [54:44<2:03:05, 107.04s/it] 32%|███▏      | 32/100 [56:33<2:01:57, 107.62s/it] 33%|███▎      | 33/100 [58:16<1:58:41, 106.29s/it] 34%|███▍      | 34/100 [1:00:05<1:57:47, 107.08s/it] 35%|███▌      | 35/100 [1:01:52<1:56:02, 107.12s/it] 36%|███▌      | 36/100 [1:03:34<1:52:34, 105.54s/it] 37%|███▋      | 37/100 [1:05:29<1:53:36, 108.19s/it] 38%|███▊      | 38/100 [1:07:15<1:51:14, 107.65s/it] 39%|███▉      | 39/100 [1:09:07<1:50:54, 109.09s/it] 40%|████      | 40/100 [1:11:03<1:51:09, 111.15s/it]                                                      40%|████      | 40/100 [1:11:03<1:51:09, 111.15s/it] 41%|████      | 41/100 [1:12:58<1:50:13, 112.10s/it] 42%|████▏     | 42/100 [1:14:57<1:50:28, 114.28s/it] 43%|████▎     | 43/100 [1:16:58<1:50:23, 116.20s/it] 44%|████▍     | 44/100 [1:18:53<1:48:03, 115.78s/it] 45%|████▌     | 45/100 [1:20:52<1:47:15, 117.01s/it] 46%|████▌     | 46/100 [1:22:41<1:43:08, 114.60s/it] 47%|████▋     | 47/100 [1:24:42<1:42:52, 116.47s/it] 48%|████▊     | 48/100 [1:26:41<1:41:31, 117.14s/it] 49%|████▉     | 49/100 [1:28:30<1:37:32, 114.75s/it] 50%|█████     | 50/100 [1:30:33<1:37:44, 117.30s/it] 51%|█████     | 51/100 [1:32:25<1:34:22, 115.57s/it] 52%|█████▏    | 52/100 [1:34:24<1:33:21, 116.69s/it] 53%|█████▎    | 53/100 [1:36:31<1:33:40, 119.59s/it] 54%|█████▍    | 54/100 [1:38:22<1:29:53, 117.26s/it] 55%|█████▌    | 55/100 [1:40:23<1:28:38, 118.19s/it] 56%|█████▌    | 56/100 [1:42:19<1:26:19, 117.71s/it] 57%|█████▋    | 57/100 [1:44:22<1:25:22, 119.12s/it] 58%|█████▊    | 58/100 [1:46:23<1:23:49, 119.76s/it] 59%|█████▉    | 59/100 [1:48:09<1:19:01, 115.64s/it] 60%|██████    | 60/100 [1:50:09<1:17:55, 116.90s/it]                                                      60%|██████    | 60/100 [1:50:09<1:17:55, 116.90s/it] 61%|██████    | 61/100 [1:52:06<1:16:08, 117.13s/it] 62%|██████▏   | 62/100 [1:54:07<1:14:52, 118.23s/it] 63%|██████▎   | 63/100 [1:56:06<1:12:55, 118.24s/it] 64%|██████▍   | 64/100 [1:58:05<1:11:10, 118.62s/it] 65%|██████▌   | 65/100 [2:00:10<1:10:14, 120.41s/it] 66%|██████▌   | 66/100 [2:02:12<1:08:34, 121.02s/it] 67%|██████▋   | 67/100 [2:04:11<1:06:08, 120.26s/it] 68%|██████▊   | 68/100 [2:06:12<1:04:22, 120.71s/it] 69%|██████▉   | 69/100 [2:08:07<1:01:23, 118.82s/it] 70%|███████   | 70/100 [2:10:09<59:53, 119.79s/it]   71%|███████   | 71/100 [2:12:06<57:34, 119.11s/it] 72%|███████▏  | 72/100 [2:14:00<54:52, 117.58s/it] 73%|███████▎  | 73/100 [2:16:01<53:18, 118.48s/it] 74%|███████▍  | 74/100 [2:18:02<51:43, 119.37s/it] 75%|███████▌  | 75/100 [2:20:00<49:30, 118.83s/it] 76%|███████▌  | 76/100 [2:22:03<48:02, 120.11s/it] 77%|███████▋  | 77/100 [2:24:00<45:38, 119.05s/it] 78%|███████▊  | 78/100 [2:26:01<43:52, 119.67s/it] 79%|███████▉  | 79/100 [2:28:04<42:14, 120.68s/it] 80%|████████  | 80/100 [2:30:09<40:38, 121.92s/it]                                                    80%|████████  | 80/100 [2:30:09<40:38, 121.92s/it] 81%|████████  | 81/100 [2:32:05<38:04, 120.22s/it] 82%|████████▏ | 82/100 [2:33:59<35:31, 118.43s/it] 83%|████████▎ | 83/100 [2:36:02<33:55, 119.75s/it] 84%|████████▍ | 84/100 [2:37:57<31:32, 118.31s/it] 85%|████████▌ | 85/100 [2:40:00<29:55, 119.73s/it] 86%|████████▌ | 86/100 [2:42:02<28:06, 120.49s/it] 87%|████████▋ | 87/100 [2:44:03<26:08, 120.67s/it] 88%|████████▊ | 88/100 [2:46:05<24:11, 120.94s/it] 89%|████████▉ | 89/100 [2:47:51<21:21, 116.50s/it] 90%|█████████ | 90/100 [2:49:56<19:50, 119.09s/it] 91%|█████████ | 91/100 [2:51:57<17:56, 119.61s/it] 92%|█████████▏| 92/100 [2:53:56<15:54, 119.31s/it] 93%|█████████▎| 93/100 [2:55:58<14:02, 120.35s/it] 94%|█████████▍| 94/100 [2:57:46<11:40, 116.69s/it] 95%|█████████▌| 95/100 [2:59:46<09:48, 117.67s/it] 96%|█████████▌| 96/100 [3:01:46<07:52, 118.15s/it] 97%|█████████▋| 97/100 [3:03:48<05:58, 119.52s/it] 98%|█████████▊| 98/100 [3:05:52<04:01, 120.62s/it] 99%|█████████▉| 99/100 [3:07:44<01:58, 118.27s/it]100%|██████████| 100/100 [3:09:48<00:00, 119.89s/it]                                                    100%|██████████| 100/100 [3:09:48<00:00, 119.89s/it]                                                    100%|██████████| 100/100 [3:10:12<00:00, 119.89s/it]100%|██████████| 100/100 [3:10:12<00:00, 114.12s/it]
/home/UG/ivansoji001/.local/lib/python3.9/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:08,  1.27s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:07,  1.31s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:06,  1.32s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:05<00:05,  1.33s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:06<00:03,  1.33s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.04it/s]
Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at google/gemma-2-9b and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some parameters are on the meta device because they were offloaded to the cpu.
/home/UG/ivansoji001/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:4089: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)
  warnings.warn(
Saving checkpoint shards:   0%|          | 0/18 [00:00<?, ?it/s]Saving checkpoint shards:   6%|▌         | 1/18 [00:25<07:11, 25.38s/it]Saving checkpoint shards:  11%|█         | 2/18 [00:37<04:43, 17.71s/it]Saving checkpoint shards:  17%|█▋        | 3/18 [00:51<03:58, 15.93s/it]Saving checkpoint shards:  22%|██▏       | 4/18 [01:04<03:26, 14.77s/it]Saving checkpoint shards:  28%|██▊       | 5/18 [01:15<02:54, 13.40s/it]Saving checkpoint shards:  33%|███▎      | 6/18 [01:35<03:06, 15.53s/it]Saving checkpoint shards:  39%|███▉      | 7/18 [01:47<02:40, 14.55s/it]Saving checkpoint shards:  44%|████▍     | 8/18 [02:00<02:19, 14.00s/it]Saving checkpoint shards:  50%|█████     | 9/18 [02:20<02:23, 15.91s/it]Saving checkpoint shards:  56%|█████▌    | 10/18 [02:34<02:03, 15.42s/it]Saving checkpoint shards:  61%|██████    | 11/18 [02:55<01:58, 16.89s/it]Saving checkpoint shards:  67%|██████▋   | 12/18 [03:35<02:23, 23.99s/it]Saving checkpoint shards:  72%|███████▏  | 13/18 [03:48<01:43, 20.79s/it]Saving checkpoint shards:  78%|███████▊  | 14/18 [04:06<01:19, 19.80s/it]Saving checkpoint shards:  83%|████████▎ | 15/18 [04:21<00:55, 18.44s/it]Saving checkpoint shards:  89%|████████▉ | 16/18 [04:38<00:36, 18.07s/it]Saving checkpoint shards:  94%|█████████▍| 17/18 [04:56<00:17, 17.87s/it]Saving checkpoint shards: 100%|██████████| 18/18 [05:07<00:00, 15.92s/it]Saving checkpoint shards: 100%|██████████| 18/18 [05:07<00:00, 17.09s/it]
