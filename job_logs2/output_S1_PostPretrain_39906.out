[Step1] Resolving SCRATCH_BASE...
[Debug] SCRATCH_BASE resolved to: /scratch-shared/tc1proj005
[Step1] Working dir: /home/UG/ivansoji001/exported-assets_sc4000
[Step1] Home dir: /home/UG/ivansoji001
[Step1] SCRATCH_BASE: /scratch-shared/tc1proj005
[Step1] HF_HOME: /scratch-shared/tc1proj005
[Step1] HUGGINGFACE_HUB_CACHE: /scratch-shared/tc1proj005
[Step1] HF_DATASETS_CACHE: /scratch-shared/tc1proj005
[Step1] TRANSFORMERS_CACHE: /scratch-shared/tc1proj005
[Step1] UT_DATA: data/ultrafeedback.csv | EPOCHS: 1 | LR: 1e-5 | MAXLEN: 1024 | PER_DEVICE_BS: 1 | MAX_STEPS: -1 | LORA_R: 64 | LORA_ALPHA: 128
[Step1][AutoCap] Projected 84471 steps (~1173h) exceeds budget 6h; capping to 432.
[Step1] Deps OK
[Step1] Online mode enabled (TRANSFORMERS_OFFLINE=0)
[Step1] Downloading tokenizer files for Qwen/Qwen2.5-7B-Instruct...
[Step1] Tokenizer snapshot at: /scratch-shared/tc1proj005/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28
[Step1] Prefetched tokenizer: vocab size 151665
[Step1] Starting post-pretraining (LoRA/QLoRA)
[Step1] Selected stage: llama
[Step1] Disk check for /scratch-shared/tc1proj005: need 180G, avail 82565G
[Step1] Cleaning HF cache at /scratch-shared/tc1proj005
[Step1] Skipping cache clean to avoid wiping SCRATCH_BASE root
[Step1] meta-llama/Llama-3.1-8B-Instruct QLoRA: starting
[Step1] LLaMA tokenizer snapshot at: /scratch-shared/tc1proj005/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
[Step1] LLaMA args: --base-model meta-llama/Llama-3.1-8B-Instruct --output-dir /scratch-shared/tc1proj005/post_pretrain_llama3-8b_lora --data-path data/ultrafeedback.csv --tokenizer-path /scratch-shared/tc1proj005/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659 --bf16 --qlora --epochs 1 --lr 1e-5 --max-length 1024 --r 64 --lora-alpha 128 --per-device-batch 1 --grad-accum 16 --subset-size 0 --max-steps 200 --grad-checkpoint
[DEBUG] Trainable params: 167,784,448 / 4,183,060,480 (4.0110%)
[INFO] subset_size=0 -> ignoring and using full dataset of 157675 examples
{'loss': 20.893, 'grad_norm': 661.6259155273438, 'learning_rate': 9.050000000000001e-06, 'epoch': 0.0}
{'loss': 19.3863, 'grad_norm': 468.4781494140625, 'learning_rate': 8.050000000000001e-06, 'epoch': 0.0}
{'loss': 17.3571, 'grad_norm': 1166.81298828125, 'learning_rate': 7.05e-06, 'epoch': 0.0}
{'loss': 15.7405, 'grad_norm': 935.944091796875, 'learning_rate': 6.0500000000000005e-06, 'epoch': 0.0}
{'loss': 15.5298, 'grad_norm': 933.1146850585938, 'learning_rate': 5.050000000000001e-06, 'epoch': 0.01}
{'loss': 12.6965, 'grad_norm': 428.80902099609375, 'learning_rate': 4.05e-06, 'epoch': 0.01}
{'loss': 13.4736, 'grad_norm': 481.2126770019531, 'learning_rate': 3.05e-06, 'epoch': 0.01}
{'loss': 16.3307, 'grad_norm': 414.2791442871094, 'learning_rate': 2.05e-06, 'epoch': 0.01}
{'loss': 14.24, 'grad_norm': 455.2029113769531, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.01}
{'loss': 13.8723, 'grad_norm': 258.5983581542969, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.01}
{'train_runtime': 20415.9121, 'train_samples_per_second': 0.157, 'train_steps_per_second': 0.01, 'train_loss': 15.951970291137695, 'epoch': 0.01}
[INFO] Wrote training_summary.json
[Merge] Saving merged model to /scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged
[Step1] Stage 'llama' completed
