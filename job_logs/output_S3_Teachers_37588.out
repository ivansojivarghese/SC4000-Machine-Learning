[Step3] Resolving SCRATCH_BASE...
[Step3] Using SCRATCH_BASE=/scratch-shared/tc1proj005
[Step3] Hugging Face login succeeded (token provided).
[Step3] Using Kaggle train: data/train.csv | External: data/ultrafeedback.csv
[Step3][Prep] Starting fold CSV generation...
[Step3][Prep] Loaded folds json with keys: ['0', '1', '2', '3', '4']
[Step3][Prep] Reading Kaggle train CSV...
[Step3][Prep] Loaded Kaggle train df: (57477, 9)
[Step3][Prep] Reading external CSV...
[Step3][Prep] Loaded external df: (157675, 8)
[Step3][Prep] Writing fold CSVs initially to /scratch-shared/tc1proj005/fold_data (will mirror to data/fold_data)
[Step3][Prep][Fold 0] Start generation (val idx count=11496)
[Step3][Prep][Fold 0] Built val set shape=(11496, 9)
[Step3][Prep][Fold 0] Train subset shape=(45981, 9)
[Step3][Prep][Fold 0] Combined shape=(203656, 17)
[Step3][Prep][Fold 0] Composition: kaggle_train_rows=45981 external_rows=157675 expected_total=203656
[Step3][Prep][Fold 0] Wrote to scratch: /scratch-shared/tc1proj005/fold_data
[Step3][Prep][Fold 0][Warn] Copy to project dir failed: [Errno 5] Input/output error
[Step3][Prep][Fold 0] Wrote train=1259.20MB val=37.07MB | 1/5 | 48.77s
[Step3][Prep][Fold 1] Start generation (val idx count=11496)
[Step3][Prep][Fold 1] Built val set shape=(11496, 9)
[Step3][Prep][Fold 1] Train subset shape=(45981, 9)
[Step3][Prep][Fold 1] Combined shape=(203656, 17)
[Step3][Prep][Fold 1] Composition: kaggle_train_rows=45981 external_rows=157675 expected_total=203656
[Step3][Prep][Fold 1] Wrote to scratch: /scratch-shared/tc1proj005/fold_data
[Step3][Prep][Fold 1] Wrote train=1259.47MB val=36.79MB | 2/5 | 49.51s
[Step3][Prep][Fold 2] Start generation (val idx count=11495)
[Step3][Prep][Fold 2] Built val set shape=(11495, 9)
[Step3][Prep][Fold 2] Train subset shape=(45982, 9)
[Step3][Prep][Fold 2] Combined shape=(203657, 17)
[Step3][Prep][Fold 2] Composition: kaggle_train_rows=45982 external_rows=157675 expected_total=203657
[Step3][Prep][Fold 2] Wrote to scratch: /scratch-shared/tc1proj005/fold_data
[Step3][Prep][Fold 2] Wrote train=1259.54MB val=36.72MB | 3/5 | 44.17s
[Step3][Prep][Fold 3] Start generation (val idx count=11495)
[Step3][Prep][Fold 3] Built val set shape=(11495, 9)
[Step3][Prep][Fold 3] Train subset shape=(45982, 9)
[Step3][Prep][Fold 3] Combined shape=(203657, 17)
[Step3][Prep][Fold 3] Composition: kaggle_train_rows=45982 external_rows=157675 expected_total=203657
[Step3][Prep][Fold 3] Wrote to scratch: /scratch-shared/tc1proj005/fold_data
[Step3][Prep][Fold 3] Wrote train=1259.85MB val=36.42MB | 4/5 | 45.15s
[Step3][Prep][Fold 4] Start generation (val idx count=11495)
[Step3][Prep][Fold 4] Built val set shape=(11495, 9)
[Step3][Prep][Fold 4] Train subset shape=(45982, 9)
[Step3][Prep][Fold 4] Combined shape=(203657, 17)
[Step3][Prep][Fold 4] Composition: kaggle_train_rows=45982 external_rows=157675 expected_total=203657
[Step3][Prep][Fold 4] Wrote to scratch: /scratch-shared/tc1proj005/fold_data
[Step3][Prep][Fold 4] Wrote train=1259.09MB val=37.18MB | 5/5 | 48.75s
[Step3][Prep] Completed fold CSV generation in 243.9s
[Step3][Sync] Copied data/fold_data/fold_0_train.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_0_val.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_1_train.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_1_val.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_2_train.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_2_val.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_3_train.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_3_val.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_4_train.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3][Sync] Copied data/fold_data/fold_4_val.csv -> /scratch-shared/tc1proj005/fold_data/
[Step3] LLaMA base: /scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged | tokenizer: meta-llama/Meta-Llama-3.1-8B | skip=0
[Step3] Qwen base:  /scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged | tokenizer: Qwen/Qwen2.5-14B | skip=0
[Step3] Teacher artifacts will be saved under: /scratch-shared/tc1proj005/folds
[Step3] Selected folds: 0 1 2 3 4 (from TEACHER_FOLDS='all')
[Step3] Root-level symlinks for fold models will be created (TEACHER_EXPOSE_ROOT=1).
[Step3] LLaMA fold 0 train csv: data/fold_data/fold_0_train.csv
[Step3][SkipTrain] Skipping LLaMA training for fold 0 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/llama_fold_0
[Step3][OK] Sharded safetensors detected in LLaMA merged fold 0 (/scratch-shared/tc1proj005/folds/llama_fold_0)
[Step3][Info] LLaMA merged fold 0 shard count: 7
[Step3][OK] Weights present (sharded) in LLaMA merged fold 0 (/scratch-shared/tc1proj005/folds/llama_fold_0)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with llama fold 0
[Step3] Qwen fold 0 train csv: data/fold_data/fold_0_train.csv
[Step3][SkipTrain] Skipping Qwen training for fold 0 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/qwen_fold_0
[Step3][OK] Sharded safetensors detected in Qwen merged fold 0 (/scratch-shared/tc1proj005/folds/qwen_fold_0)
[Step3][Info] Qwen merged fold 0 shard count: 6
[Step3][OK] Weights present (sharded) in Qwen merged fold 0 (/scratch-shared/tc1proj005/folds/qwen_fold_0)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with qwen fold 0
[Step3] LLaMA fold 1 train csv: data/fold_data/fold_1_train.csv
[Step3][SkipTrain] Skipping LLaMA training for fold 1 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/llama_fold_1
[Step3][OK] Sharded safetensors detected in LLaMA merged fold 1 (/scratch-shared/tc1proj005/folds/llama_fold_1)
[Step3][Info] LLaMA merged fold 1 shard count: 7
[Step3][OK] Weights present (sharded) in LLaMA merged fold 1 (/scratch-shared/tc1proj005/folds/llama_fold_1)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with llama fold 1
[Step3] Qwen fold 1 train csv: data/fold_data/fold_1_train.csv
[Step3][SkipTrain] Skipping Qwen training for fold 1 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/qwen_fold_1
[Step3][OK] Sharded safetensors detected in Qwen merged fold 1 (/scratch-shared/tc1proj005/folds/qwen_fold_1)
[Step3][Info] Qwen merged fold 1 shard count: 6
[Step3][OK] Weights present (sharded) in Qwen merged fold 1 (/scratch-shared/tc1proj005/folds/qwen_fold_1)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with qwen fold 1
[Step3] LLaMA fold 2 train csv: data/fold_data/fold_2_train.csv
[Step3][SkipTrain] Skipping LLaMA training for fold 2 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/llama_fold_2
[Step3][OK] Sharded safetensors detected in LLaMA merged fold 2 (/scratch-shared/tc1proj005/folds/llama_fold_2)
[Step3][Info] LLaMA merged fold 2 shard count: 7
[Step3][OK] Weights present (sharded) in LLaMA merged fold 2 (/scratch-shared/tc1proj005/folds/llama_fold_2)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with llama fold 2
[Step3] Qwen fold 2 train csv: data/fold_data/fold_2_train.csv
[Step3][SkipTrain] Skipping Qwen training for fold 2 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/qwen_fold_2
[Step3][OK] Sharded safetensors detected in Qwen merged fold 2 (/scratch-shared/tc1proj005/folds/qwen_fold_2)
[Step3][Info] Qwen merged fold 2 shard count: 6
[Step3][OK] Weights present (sharded) in Qwen merged fold 2 (/scratch-shared/tc1proj005/folds/qwen_fold_2)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with qwen fold 2
[Step3] LLaMA fold 3 train csv: data/fold_data/fold_3_train.csv
[Step3][SkipTrain] Skipping LLaMA training for fold 3 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/llama_fold_3
[Step3][OK] Sharded safetensors detected in LLaMA merged fold 3 (/scratch-shared/tc1proj005/folds/llama_fold_3)
[Step3][Info] LLaMA merged fold 3 shard count: 7
[Step3][OK] Weights present (sharded) in LLaMA merged fold 3 (/scratch-shared/tc1proj005/folds/llama_fold_3)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with llama fold 3
[Step3] Qwen fold 3 train csv: data/fold_data/fold_3_train.csv
[Step3][SkipTrain] Skipping Qwen training for fold 3 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/qwen_fold_3
[Step3][OK] Sharded safetensors detected in Qwen merged fold 3 (/scratch-shared/tc1proj005/folds/qwen_fold_3)
[Step3][Info] Qwen merged fold 3 shard count: 6
[Step3][OK] Weights present (sharded) in Qwen merged fold 3 (/scratch-shared/tc1proj005/folds/qwen_fold_3)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with qwen fold 3
[Step3] LLaMA fold 4 train csv: data/fold_data/fold_4_train.csv
[Step3][SkipTrain] Skipping LLaMA training for fold 4 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/llama_fold_4
[Step3][OK] Sharded safetensors detected in LLaMA merged fold 4 (/scratch-shared/tc1proj005/folds/llama_fold_4)
[Step3][Info] LLaMA merged fold 4 shard count: 7
[Step3][OK] Weights present (sharded) in LLaMA merged fold 4 (/scratch-shared/tc1proj005/folds/llama_fold_4)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with llama fold 4
[Step3] Qwen fold 4 train csv: data/fold_data/fold_4_train.csv
[Step3][SkipTrain] Skipping Qwen training for fold 4 (TEACHER_SKIP_TRAIN=1 and LoRA dir exists)
[Merge][Warn] GPU OOM during merge; retrying with low-memory CPU path...
[Merge] Low-memory CPU merge path engaged (forcing CPU load, minimal dtype)
[Merge] Saving merged model to /scratch-shared/tc1proj005/folds/qwen_fold_4
[Step3][OK] Sharded safetensors detected in Qwen merged fold 4 (/scratch-shared/tc1proj005/folds/qwen_fold_4)
[Step3][Info] Qwen merged fold 4 shard count: 6
[Step3][OK] Weights present (sharded) in Qwen merged fold 4 (/scratch-shared/tc1proj005/folds/qwen_fold_4)
[Step3][Manifest] Updated /scratch-shared/tc1proj005/folds/manifest.json with qwen fold 4
[Step3] Done
