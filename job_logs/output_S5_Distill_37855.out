[Step5] Distilling fold 0 using LLaMA-only OOF probs
{'loss': 8.9696, 'grad_norm': 163.36599731445312, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.01}
{'loss': 8.1752, 'grad_norm': 141.63194274902344, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
{'loss': 7.4383, 'grad_norm': 136.97669982910156, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.02}
{'loss': 8.1865, 'grad_norm': 135.6033172607422, 'learning_rate': 4.898936170212766e-05, 'epoch': 0.03}
{'loss': 5.8292, 'grad_norm': 76.09439086914062, 'learning_rate': 4.792553191489362e-05, 'epoch': 0.04}
{'loss': 7.4829, 'grad_norm': 129.79026794433594, 'learning_rate': 4.686170212765958e-05, 'epoch': 0.05}
{'loss': 6.2889, 'grad_norm': 132.99656677246094, 'learning_rate': 4.579787234042554e-05, 'epoch': 0.05}
{'loss': 6.2252, 'grad_norm': 64.91899108886719, 'learning_rate': 4.473404255319149e-05, 'epoch': 0.06}
{'loss': 5.9023, 'grad_norm': 56.10263442993164, 'learning_rate': 4.367021276595745e-05, 'epoch': 0.07}
{'loss': 5.4359, 'grad_norm': 27.496055603027344, 'learning_rate': 4.2606382978723406e-05, 'epoch': 0.08}
{'loss': 5.819, 'grad_norm': 31.327865600585938, 'learning_rate': 4.1542553191489364e-05, 'epoch': 0.09}
{'loss': 5.5857, 'grad_norm': 54.99040603637695, 'learning_rate': 4.047872340425532e-05, 'epoch': 0.09}
{'loss': 5.5119, 'grad_norm': 111.22478485107422, 'learning_rate': 3.941489361702128e-05, 'epoch': 0.1}
{'loss': 5.5494, 'grad_norm': 85.772216796875, 'learning_rate': 3.835106382978724e-05, 'epoch': 0.11}
{'loss': 5.2817, 'grad_norm': 57.43724060058594, 'learning_rate': 3.728723404255319e-05, 'epoch': 0.12}
{'loss': 5.3729, 'grad_norm': 68.43572235107422, 'learning_rate': 3.622340425531915e-05, 'epoch': 0.12}
{'loss': 5.3809, 'grad_norm': 31.21466827392578, 'learning_rate': 3.515957446808511e-05, 'epoch': 0.13}
{'loss': 5.1048, 'grad_norm': 19.86723518371582, 'learning_rate': 3.4095744680851066e-05, 'epoch': 0.14}
{'loss': 5.3403, 'grad_norm': 54.52139663696289, 'learning_rate': 3.3031914893617025e-05, 'epoch': 0.15}
{'loss': 5.3284, 'grad_norm': 61.581382751464844, 'learning_rate': 3.1968085106382976e-05, 'epoch': 0.15}
{'loss': 5.3259, 'grad_norm': 44.015995025634766, 'learning_rate': 3.090425531914894e-05, 'epoch': 0.16}
{'loss': 5.2078, 'grad_norm': 99.61437225341797, 'learning_rate': 2.9840425531914897e-05, 'epoch': 0.17}
{'loss': 5.3297, 'grad_norm': 73.34547424316406, 'learning_rate': 2.877659574468085e-05, 'epoch': 0.18}
{'loss': 5.3309, 'grad_norm': 58.058773040771484, 'learning_rate': 2.7712765957446813e-05, 'epoch': 0.19}
{'loss': 5.3316, 'grad_norm': 72.47061157226562, 'learning_rate': 2.664893617021277e-05, 'epoch': 0.19}
{'loss': 5.8511, 'grad_norm': 41.22067642211914, 'learning_rate': 2.5585106382978723e-05, 'epoch': 0.2}
{'loss': 5.3507, 'grad_norm': 44.94644546508789, 'learning_rate': 2.4521276595744682e-05, 'epoch': 0.21}
{'loss': 5.2421, 'grad_norm': 59.43817138671875, 'learning_rate': 2.345744680851064e-05, 'epoch': 0.22}
{'loss': 5.3326, 'grad_norm': 63.766761779785156, 'learning_rate': 2.23936170212766e-05, 'epoch': 0.22}
{'loss': 4.7197, 'grad_norm': 36.324649810791016, 'learning_rate': 2.1329787234042554e-05, 'epoch': 0.23}
{'loss': 5.2356, 'grad_norm': 67.80725860595703, 'learning_rate': 2.0265957446808512e-05, 'epoch': 0.24}
{'loss': 5.0321, 'grad_norm': 24.84149742126465, 'learning_rate': 1.920212765957447e-05, 'epoch': 0.25}
{'loss': 5.1068, 'grad_norm': 39.84535598754883, 'learning_rate': 1.8138297872340425e-05, 'epoch': 0.26}
{'loss': 5.2328, 'grad_norm': 44.034976959228516, 'learning_rate': 1.7074468085106384e-05, 'epoch': 0.26}
{'loss': 5.125, 'grad_norm': 50.26496124267578, 'learning_rate': 1.6010638297872342e-05, 'epoch': 0.27}
{'loss': 5.7095, 'grad_norm': 24.639537811279297, 'learning_rate': 1.4946808510638299e-05, 'epoch': 0.28}
{'loss': 5.1112, 'grad_norm': 51.77180480957031, 'learning_rate': 1.3882978723404256e-05, 'epoch': 0.29}
{'loss': 4.9236, 'grad_norm': 53.887115478515625, 'learning_rate': 1.2819148936170214e-05, 'epoch': 0.29}
{'loss': 5.2003, 'grad_norm': 33.9218864440918, 'learning_rate': 1.175531914893617e-05, 'epoch': 0.3}
{'loss': 4.878, 'grad_norm': 19.779212951660156, 'learning_rate': 1.0691489361702128e-05, 'epoch': 0.31}
{'loss': 5.3092, 'grad_norm': 39.95195007324219, 'learning_rate': 9.627659574468086e-06, 'epoch': 0.32}
{'loss': 4.8646, 'grad_norm': 58.45526885986328, 'learning_rate': 8.563829787234043e-06, 'epoch': 0.32}
{'loss': 4.9454, 'grad_norm': 60.82452392578125, 'learning_rate': 7.5e-06, 'epoch': 0.33}
{'loss': 5.17, 'grad_norm': 14.627202987670898, 'learning_rate': 6.436170212765958e-06, 'epoch': 0.34}
{'loss': 4.5996, 'grad_norm': 27.685869216918945, 'learning_rate': 5.372340425531915e-06, 'epoch': 0.35}
{'loss': 5.2212, 'grad_norm': 36.17313003540039, 'learning_rate': 4.308510638297873e-06, 'epoch': 0.36}
{'loss': 4.913, 'grad_norm': 24.20108985900879, 'learning_rate': 3.2446808510638296e-06, 'epoch': 0.36}
{'loss': 5.0596, 'grad_norm': 64.60440826416016, 'learning_rate': 2.1808510638297876e-06, 'epoch': 0.37}
{'loss': 5.0418, 'grad_norm': 45.498260498046875, 'learning_rate': 1.1170212765957447e-06, 'epoch': 0.38}
{'loss': 4.9805, 'grad_norm': 45.84481430053711, 'learning_rate': 5.319148936170213e-08, 'epoch': 0.39}
{'train_runtime': 11701.0159, 'train_samples_per_second': 1.367, 'train_steps_per_second': 0.085, 'train_loss': 5.577816673278809, 'epoch': 0.39}
{'eval': {'eval_loss': 0.3083468973636627, 'eval_log_loss': 1.019992094942966, 'eval_accuracy': 0.4759730376168732, 'eval_runtime': 1018.419, 'eval_samples_per_second': 4.516, 'eval_steps_per_second': 4.516, 'epoch': 0.3866415349668938}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
