[Step5] Distilling fold 0 using LLaMA-only OOF probs
{'loss': 4.8576, 'grad_norm': 30.47848892211914, 'learning_rate': 1.4499266324284665e-05, 'epoch': 1.63}
{'loss': 3.9146, 'grad_norm': 21.423341751098633, 'learning_rate': 1.4315847395451212e-05, 'epoch': 1.64}
{'loss': 4.7952, 'grad_norm': 27.664560317993164, 'learning_rate': 1.4132428466617756e-05, 'epoch': 1.65}
{'loss': 4.1452, 'grad_norm': 41.80085754394531, 'learning_rate': 1.3949009537784299e-05, 'epoch': 1.65}
{'loss': 4.1655, 'grad_norm': 45.5602912902832, 'learning_rate': 1.3765590608950846e-05, 'epoch': 1.66}
{'loss': 4.7989, 'grad_norm': 25.82974624633789, 'learning_rate': 1.358217168011739e-05, 'epoch': 1.67}
{'loss': 3.6792, 'grad_norm': 19.759477615356445, 'learning_rate': 1.3398752751283933e-05, 'epoch': 1.68}
{'loss': 4.671, 'grad_norm': 44.20382308959961, 'learning_rate': 1.3215333822450476e-05, 'epoch': 1.69}
{'loss': 4.117, 'grad_norm': 20.40230369567871, 'learning_rate': 1.3031914893617023e-05, 'epoch': 1.69}
{'loss': 4.4044, 'grad_norm': 29.867507934570312, 'learning_rate': 1.2848495964783567e-05, 'epoch': 1.7}
{'loss': 4.9064, 'grad_norm': 28.387685775756836, 'learning_rate': 1.266507703595011e-05, 'epoch': 1.71}
{'loss': 3.9546, 'grad_norm': 66.14097595214844, 'learning_rate': 1.2481658107116655e-05, 'epoch': 1.72}
{'loss': 4.7955, 'grad_norm': 37.37278366088867, 'learning_rate': 1.22982391782832e-05, 'epoch': 1.72}
{'loss': 4.034, 'grad_norm': 51.97481155395508, 'learning_rate': 1.2114820249449744e-05, 'epoch': 1.73}
{'loss': 4.4928, 'grad_norm': 49.00577163696289, 'learning_rate': 1.1931401320616289e-05, 'epoch': 1.74}
{'loss': 4.7048, 'grad_norm': 29.06151008605957, 'learning_rate': 1.1747982391782832e-05, 'epoch': 1.75}
{'loss': 4.1242, 'grad_norm': 53.2068977355957, 'learning_rate': 1.1564563462949376e-05, 'epoch': 1.76}
{'loss': 4.7925, 'grad_norm': 33.70920181274414, 'learning_rate': 1.1381144534115921e-05, 'epoch': 1.76}
{'loss': 4.1779, 'grad_norm': 26.38387107849121, 'learning_rate': 1.1197725605282464e-05, 'epoch': 1.77}
{'loss': 4.1701, 'grad_norm': 36.19256591796875, 'learning_rate': 1.101430667644901e-05, 'epoch': 1.78}
{'loss': 4.7282, 'grad_norm': 36.65163803100586, 'learning_rate': 1.0830887747615555e-05, 'epoch': 1.79}
{'loss': 3.872, 'grad_norm': 42.916683197021484, 'learning_rate': 1.0647468818782098e-05, 'epoch': 1.79}
{'loss': 4.6975, 'grad_norm': 27.63286018371582, 'learning_rate': 1.0464049889948643e-05, 'epoch': 1.8}
{'loss': 4.4319, 'grad_norm': 35.1551399230957, 'learning_rate': 1.0280630961115188e-05, 'epoch': 1.81}
{'loss': 4.1039, 'grad_norm': 44.469444274902344, 'learning_rate': 1.0097212032281732e-05, 'epoch': 1.82}
{'loss': 4.6704, 'grad_norm': 27.84247398376465, 'learning_rate': 9.913793103448277e-06, 'epoch': 1.82}
{'loss': 4.031, 'grad_norm': 59.68666076660156, 'learning_rate': 9.73037417461482e-06, 'epoch': 1.83}
{'loss': 4.5247, 'grad_norm': 53.734169006347656, 'learning_rate': 9.546955245781365e-06, 'epoch': 1.84}
{'loss': 4.2198, 'grad_norm': 35.19276809692383, 'learning_rate': 9.36353631694791e-06, 'epoch': 1.85}
{'loss': 4.516, 'grad_norm': 28.153112411499023, 'learning_rate': 9.180117388114454e-06, 'epoch': 1.86}
{'loss': 4.924, 'grad_norm': 48.817630767822266, 'learning_rate': 8.996698459281e-06, 'epoch': 1.86}
{'loss': 3.9982, 'grad_norm': 40.13550567626953, 'learning_rate': 8.813279530447543e-06, 'epoch': 1.87}
{'loss': 4.8575, 'grad_norm': 25.943721771240234, 'learning_rate': 8.629860601614088e-06, 'epoch': 1.88}
{'loss': 4.2281, 'grad_norm': 44.25536346435547, 'learning_rate': 8.446441672780631e-06, 'epoch': 1.89}
{'loss': 4.3488, 'grad_norm': 40.17276382446289, 'learning_rate': 8.263022743947176e-06, 'epoch': 1.89}
{'loss': 4.6823, 'grad_norm': 63.47554397583008, 'learning_rate': 8.07960381511372e-06, 'epoch': 1.9}
{'loss': 4.2799, 'grad_norm': 34.172523498535156, 'learning_rate': 7.896184886280263e-06, 'epoch': 1.91}
{'loss': 4.6277, 'grad_norm': 28.302093505859375, 'learning_rate': 7.712765957446808e-06, 'epoch': 1.92}
{'loss': 4.5768, 'grad_norm': 50.81379318237305, 'learning_rate': 7.5293470286133535e-06, 'epoch': 1.93}
{'loss': 4.6098, 'grad_norm': 23.119611740112305, 'learning_rate': 7.345928099779898e-06, 'epoch': 1.93}
{'loss': 5.1133, 'grad_norm': 28.911636352539062, 'learning_rate': 7.162509170946442e-06, 'epoch': 1.94}
{'loss': 4.2472, 'grad_norm': 25.117990493774414, 'learning_rate': 6.9790902421129855e-06, 'epoch': 1.95}
{'loss': 4.9443, 'grad_norm': 51.88470458984375, 'learning_rate': 6.795671313279531e-06, 'epoch': 1.96}
{'loss': 4.5736, 'grad_norm': 42.40834426879883, 'learning_rate': 6.612252384446076e-06, 'epoch': 1.96}
{'loss': 4.7306, 'grad_norm': 47.44683837890625, 'learning_rate': 6.428833455612619e-06, 'epoch': 1.97}
{'loss': 5.0778, 'grad_norm': 30.790172576904297, 'learning_rate': 6.245414526779164e-06, 'epoch': 1.98}
{'loss': 4.5899, 'grad_norm': 25.983102798461914, 'learning_rate': 6.061995597945709e-06, 'epoch': 1.99}
{'loss': 5.0837, 'grad_norm': 48.20408630371094, 'learning_rate': 5.878576669112253e-06, 'epoch': 1.99}
{'loss': 4.2552, 'grad_norm': 48.337039947509766, 'learning_rate': 5.695157740278797e-06, 'epoch': 2.0}
{'loss': 4.5303, 'grad_norm': 46.049476623535156, 'learning_rate': 5.5117388114453415e-06, 'epoch': 2.01}
{'loss': 3.4518, 'grad_norm': 40.477054595947266, 'learning_rate': 5.328319882611886e-06, 'epoch': 2.02}
{'loss': 4.1451, 'grad_norm': 28.108673095703125, 'learning_rate': 5.14490095377843e-06, 'epoch': 2.03}
{'loss': 4.1801, 'grad_norm': 40.41454315185547, 'learning_rate': 4.961482024944974e-06, 'epoch': 2.03}
{'loss': 3.4787, 'grad_norm': 35.14345932006836, 'learning_rate': 4.778063096111519e-06, 'epoch': 2.04}
{'loss': 4.3203, 'grad_norm': 39.88991928100586, 'learning_rate': 4.594644167278064e-06, 'epoch': 2.05}
{'loss': 3.5316, 'grad_norm': 33.782264709472656, 'learning_rate': 4.411225238444608e-06, 'epoch': 2.06}
{'loss': 4.0966, 'grad_norm': 36.223663330078125, 'learning_rate': 4.227806309611152e-06, 'epoch': 2.06}
{'loss': 4.1584, 'grad_norm': 45.10895538330078, 'learning_rate': 4.044387380777697e-06, 'epoch': 2.07}
{'loss': 3.4824, 'grad_norm': 37.4193229675293, 'learning_rate': 3.860968451944241e-06, 'epoch': 2.08}
{'loss': 4.3396, 'grad_norm': 32.61423110961914, 'learning_rate': 3.6775495231107857e-06, 'epoch': 2.09}
{'loss': 3.4254, 'grad_norm': 81.10387420654297, 'learning_rate': 3.4941305942773295e-06, 'epoch': 2.1}
{'loss': 3.9387, 'grad_norm': 43.727169036865234, 'learning_rate': 3.310711665443874e-06, 'epoch': 2.1}
{'loss': 4.159, 'grad_norm': 38.85483932495117, 'learning_rate': 3.127292736610418e-06, 'epoch': 2.11}
{'loss': 3.7115, 'grad_norm': 28.059240341186523, 'learning_rate': 2.943873807776963e-06, 'epoch': 2.12}
{'loss': 4.1908, 'grad_norm': 29.685232162475586, 'learning_rate': 2.760454878943507e-06, 'epoch': 2.13}
{'loss': 3.425, 'grad_norm': 31.876859664916992, 'learning_rate': 2.5770359501100514e-06, 'epoch': 2.13}
{'loss': 4.2477, 'grad_norm': 36.11127471923828, 'learning_rate': 2.3936170212765957e-06, 'epoch': 2.14}
{'loss': 4.0825, 'grad_norm': 37.97132110595703, 'learning_rate': 2.21019809244314e-06, 'epoch': 2.15}
{'loss': 3.4543, 'grad_norm': 43.31074142456055, 'learning_rate': 2.0267791636096847e-06, 'epoch': 2.16}
{'loss': 4.4228, 'grad_norm': 30.93124771118164, 'learning_rate': 1.8433602347762288e-06, 'epoch': 2.17}
{'loss': 3.6196, 'grad_norm': 33.51191711425781, 'learning_rate': 1.6599413059427735e-06, 'epoch': 2.17}
{'loss': 4.0425, 'grad_norm': 45.96267318725586, 'learning_rate': 1.4765223771093178e-06, 'epoch': 2.18}
{'loss': 4.257, 'grad_norm': 32.23751449584961, 'learning_rate': 1.293103448275862e-06, 'epoch': 2.19}
{'loss': 3.4829, 'grad_norm': 57.45644760131836, 'learning_rate': 1.1096845194424066e-06, 'epoch': 2.2}
{'loss': 4.4488, 'grad_norm': 42.78768539428711, 'learning_rate': 9.26265590608951e-07, 'epoch': 2.2}
{'loss': 3.284, 'grad_norm': 35.30823516845703, 'learning_rate': 7.428466617754953e-07, 'epoch': 2.21}
{'loss': 3.9659, 'grad_norm': 55.82175827026367, 'learning_rate': 5.594277329420397e-07, 'epoch': 2.22}
{'loss': 4.0181, 'grad_norm': 36.624629974365234, 'learning_rate': 3.76008804108584e-07, 'epoch': 2.23}
{'loss': 3.4166, 'grad_norm': 38.544986724853516, 'learning_rate': 1.925898752751284e-07, 'epoch': 2.23}
{'loss': 4.1964, 'grad_norm': 51.81881332397461, 'learning_rate': 9.170946441672781e-09, 'epoch': 2.24}
{'train_runtime': 19625.0787, 'train_samples_per_second': 4.729, 'train_steps_per_second': 0.296, 'train_loss': 1.1749989476697198, 'epoch': 2.24}
{'eval': {'eval_loss': 0.3085881769657135, 'eval_log_loss': 1.0057151756707319, 'eval_accuracy': 0.5255490323983475, 'eval_runtime': 1029.2551, 'eval_samples_per_second': 4.468, 'eval_steps_per_second': 4.468, 'epoch': 2.242424242424242}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
