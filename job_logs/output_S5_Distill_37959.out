[Step5] Distilling fold 3 using LLaMA-only OOF probs
{'loss': 8.3348, 'grad_norm': 110.53142547607422, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.01}
{'loss': 8.9067, 'grad_norm': 243.2040252685547, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
{'loss': 7.0031, 'grad_norm': 130.87493896484375, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.02}
{'loss': 7.9139, 'grad_norm': 93.18614196777344, 'learning_rate': 4.898936170212766e-05, 'epoch': 0.03}
{'loss': 5.9695, 'grad_norm': 148.9197235107422, 'learning_rate': 4.792553191489362e-05, 'epoch': 0.04}
{'loss': 7.0374, 'grad_norm': 84.0628662109375, 'learning_rate': 4.686170212765958e-05, 'epoch': 0.05}
{'loss': 6.4895, 'grad_norm': 67.25157165527344, 'learning_rate': 4.579787234042554e-05, 'epoch': 0.05}
{'loss': 6.0244, 'grad_norm': 54.72995376586914, 'learning_rate': 4.473404255319149e-05, 'epoch': 0.06}
{'loss': 5.9566, 'grad_norm': 63.764190673828125, 'learning_rate': 4.367021276595745e-05, 'epoch': 0.07}
{'loss': 5.4934, 'grad_norm': 50.12786865234375, 'learning_rate': 4.2606382978723406e-05, 'epoch': 0.08}
{'loss': 5.9123, 'grad_norm': 94.89906311035156, 'learning_rate': 4.1542553191489364e-05, 'epoch': 0.09}
{'loss': 5.4552, 'grad_norm': 38.1011962890625, 'learning_rate': 4.047872340425532e-05, 'epoch': 0.09}
{'loss': 5.8824, 'grad_norm': 73.77815246582031, 'learning_rate': 3.941489361702128e-05, 'epoch': 0.1}
{'loss': 5.5439, 'grad_norm': 68.05501556396484, 'learning_rate': 3.835106382978724e-05, 'epoch': 0.11}
{'loss': 5.5843, 'grad_norm': 48.70616912841797, 'learning_rate': 3.728723404255319e-05, 'epoch': 0.12}
{'loss': 5.5476, 'grad_norm': 49.80628204345703, 'learning_rate': 3.622340425531915e-05, 'epoch': 0.12}
{'loss': 5.4177, 'grad_norm': 71.41618347167969, 'learning_rate': 3.515957446808511e-05, 'epoch': 0.13}
{'loss': 5.2228, 'grad_norm': 32.807289123535156, 'learning_rate': 3.4095744680851066e-05, 'epoch': 0.14}
{'loss': 5.62, 'grad_norm': 28.01906967163086, 'learning_rate': 3.3031914893617025e-05, 'epoch': 0.15}
{'loss': 5.2826, 'grad_norm': 72.64859008789062, 'learning_rate': 3.1968085106382976e-05, 'epoch': 0.15}
{'loss': 5.5007, 'grad_norm': 33.261871337890625, 'learning_rate': 3.090425531914894e-05, 'epoch': 0.16}
{'loss': 5.3469, 'grad_norm': 42.112369537353516, 'learning_rate': 2.9840425531914897e-05, 'epoch': 0.17}
{'loss': 5.3764, 'grad_norm': 24.231586456298828, 'learning_rate': 2.877659574468085e-05, 'epoch': 0.18}
{'loss': 5.3875, 'grad_norm': 33.3143196105957, 'learning_rate': 2.7712765957446813e-05, 'epoch': 0.19}
{'loss': 5.316, 'grad_norm': 71.73600006103516, 'learning_rate': 2.664893617021277e-05, 'epoch': 0.19}
{'loss': 5.3044, 'grad_norm': 34.22400665283203, 'learning_rate': 2.5585106382978723e-05, 'epoch': 0.2}
{'loss': 5.2037, 'grad_norm': 33.62847900390625, 'learning_rate': 2.4521276595744682e-05, 'epoch': 0.21}
{'loss': 4.977, 'grad_norm': 48.51167297363281, 'learning_rate': 2.345744680851064e-05, 'epoch': 0.22}
{'loss': 5.3247, 'grad_norm': 19.91128158569336, 'learning_rate': 2.23936170212766e-05, 'epoch': 0.22}
{'loss': 4.8442, 'grad_norm': 59.58168411254883, 'learning_rate': 2.1329787234042554e-05, 'epoch': 0.23}
{'loss': 5.4257, 'grad_norm': 19.401100158691406, 'learning_rate': 2.0265957446808512e-05, 'epoch': 0.24}
{'loss': 5.2965, 'grad_norm': 16.114892959594727, 'learning_rate': 1.920212765957447e-05, 'epoch': 0.25}
{'loss': 4.8664, 'grad_norm': 33.80855178833008, 'learning_rate': 1.8138297872340425e-05, 'epoch': 0.26}
{'loss': 5.2353, 'grad_norm': 51.13713836669922, 'learning_rate': 1.7074468085106384e-05, 'epoch': 0.26}
{'loss': 4.6554, 'grad_norm': 47.2464485168457, 'learning_rate': 1.6010638297872342e-05, 'epoch': 0.27}
{'loss': 5.2902, 'grad_norm': 35.73954391479492, 'learning_rate': 1.4946808510638299e-05, 'epoch': 0.28}
{'loss': 4.9573, 'grad_norm': 37.117942810058594, 'learning_rate': 1.3882978723404256e-05, 'epoch': 0.29}
{'loss': 5.0281, 'grad_norm': 31.796106338500977, 'learning_rate': 1.2819148936170214e-05, 'epoch': 0.29}
{'loss': 5.1904, 'grad_norm': 34.13079071044922, 'learning_rate': 1.175531914893617e-05, 'epoch': 0.3}
{'loss': 4.873, 'grad_norm': 20.74933433532715, 'learning_rate': 1.0691489361702128e-05, 'epoch': 0.31}
{'loss': 5.1833, 'grad_norm': 38.012027740478516, 'learning_rate': 9.627659574468086e-06, 'epoch': 0.32}
{'loss': 5.015, 'grad_norm': 16.533180236816406, 'learning_rate': 8.563829787234043e-06, 'epoch': 0.32}
{'loss': 5.0534, 'grad_norm': 54.636268615722656, 'learning_rate': 7.5e-06, 'epoch': 0.33}
{'loss': 5.0625, 'grad_norm': 42.975440979003906, 'learning_rate': 6.436170212765958e-06, 'epoch': 0.34}
{'loss': 4.8248, 'grad_norm': 26.427602767944336, 'learning_rate': 5.372340425531915e-06, 'epoch': 0.35}
{'loss': 5.2476, 'grad_norm': 26.587688446044922, 'learning_rate': 4.308510638297873e-06, 'epoch': 0.36}
{'loss': 4.9208, 'grad_norm': 17.330059051513672, 'learning_rate': 3.2446808510638296e-06, 'epoch': 0.36}
{'loss': 4.9693, 'grad_norm': 15.523292541503906, 'learning_rate': 2.1808510638297876e-06, 'epoch': 0.37}
{'loss': 5.0094, 'grad_norm': 24.017107009887695, 'learning_rate': 1.1170212765957447e-06, 'epoch': 0.38}
{'loss': 4.9615, 'grad_norm': 39.35850524902344, 'learning_rate': 5.319148936170213e-08, 'epoch': 0.39}
{'train_runtime': 11802.3335, 'train_samples_per_second': 1.356, 'train_steps_per_second': 0.085, 'train_loss': 5.564910339355468, 'epoch': 0.39}
{'eval': {'eval_loss': 0.30988961458206177, 'eval_log_loss': 1.025463372645402, 'eval_accuracy': 0.4759730376168732, 'eval_runtime': 1013.6791, 'eval_samples_per_second': 4.537, 'eval_steps_per_second': 4.537, 'epoch': 0.3866321919628833}}
[Step5] Done fold 3 -> model_save/distilled_gemma2-9b_fold_3
