[Step5] Distilling fold 1 using LLaMA-only OOF probs
{'loss': 5.028, 'grad_norm': 20.45863151550293, 'learning_rate': 2.0022796352583586e-05, 'epoch': 1.01}
{'loss': 4.44, 'grad_norm': 42.361114501953125, 'learning_rate': 1.976950354609929e-05, 'epoch': 1.02}
{'loss': 5.0734, 'grad_norm': 50.85896301269531, 'learning_rate': 1.9516210739614996e-05, 'epoch': 1.03}
{'loss': 4.8347, 'grad_norm': 25.515933990478516, 'learning_rate': 1.92629179331307e-05, 'epoch': 1.04}
{'loss': 4.7562, 'grad_norm': 29.891189575195312, 'learning_rate': 1.9009625126646405e-05, 'epoch': 1.04}
{'loss': 5.1104, 'grad_norm': 17.022674560546875, 'learning_rate': 1.875633232016211e-05, 'epoch': 1.05}
{'loss': 4.461, 'grad_norm': 17.056283950805664, 'learning_rate': 1.850303951367781e-05, 'epoch': 1.06}
{'loss': 5.1064, 'grad_norm': 42.164764404296875, 'learning_rate': 1.8249746707193515e-05, 'epoch': 1.07}
{'loss': 4.7574, 'grad_norm': 38.01558303833008, 'learning_rate': 1.799645390070922e-05, 'epoch': 1.07}
{'loss': 4.7919, 'grad_norm': 29.53373908996582, 'learning_rate': 1.7743161094224924e-05, 'epoch': 1.08}
{'loss': 4.974, 'grad_norm': 55.67375946044922, 'learning_rate': 1.748986828774063e-05, 'epoch': 1.09}
{'loss': 4.297, 'grad_norm': 20.209117889404297, 'learning_rate': 1.7236575481256333e-05, 'epoch': 1.1}
{'loss': 4.8778, 'grad_norm': 41.541015625, 'learning_rate': 1.6983282674772038e-05, 'epoch': 1.11}
{'loss': 4.8858, 'grad_norm': 49.9918098449707, 'learning_rate': 1.672998986828774e-05, 'epoch': 1.11}
{'loss': 4.737, 'grad_norm': 45.304744720458984, 'learning_rate': 1.6476697061803443e-05, 'epoch': 1.12}
{'loss': 4.8256, 'grad_norm': 22.335222244262695, 'learning_rate': 1.6223404255319148e-05, 'epoch': 1.13}
{'loss': 4.225, 'grad_norm': 31.775712966918945, 'learning_rate': 1.5970111448834852e-05, 'epoch': 1.14}
{'loss': 4.8743, 'grad_norm': 30.461259841918945, 'learning_rate': 1.571681864235056e-05, 'epoch': 1.14}
{'loss': 4.5452, 'grad_norm': 50.7382926940918, 'learning_rate': 1.5463525835866265e-05, 'epoch': 1.15}
{'loss': 4.6182, 'grad_norm': 29.641353607177734, 'learning_rate': 1.5210233029381964e-05, 'epoch': 1.16}
{'loss': 5.0139, 'grad_norm': 60.711181640625, 'learning_rate': 1.4956940222897669e-05, 'epoch': 1.17}
{'loss': 4.2616, 'grad_norm': 56.96043395996094, 'learning_rate': 1.4703647416413373e-05, 'epoch': 1.18}
{'loss': 4.8391, 'grad_norm': 53.16500473022461, 'learning_rate': 1.4450354609929078e-05, 'epoch': 1.18}
{'loss': 4.8933, 'grad_norm': 51.45771408081055, 'learning_rate': 1.4197061803444783e-05, 'epoch': 1.19}
{'loss': 4.5512, 'grad_norm': 47.89820861816406, 'learning_rate': 1.3943768996960487e-05, 'epoch': 1.2}
{'loss': 4.9081, 'grad_norm': 33.616539001464844, 'learning_rate': 1.3690476190476192e-05, 'epoch': 1.21}
{'loss': 4.3813, 'grad_norm': 18.50337791442871, 'learning_rate': 1.3437183383991894e-05, 'epoch': 1.21}
{'loss': 4.9668, 'grad_norm': 17.340105056762695, 'learning_rate': 1.3183890577507599e-05, 'epoch': 1.22}
{'loss': 4.6127, 'grad_norm': 41.82442092895508, 'learning_rate': 1.2930597771023304e-05, 'epoch': 1.23}
{'loss': 4.6603, 'grad_norm': 43.55862808227539, 'learning_rate': 1.2677304964539008e-05, 'epoch': 1.24}
{'loss': 4.9522, 'grad_norm': 26.518966674804688, 'learning_rate': 1.2424012158054713e-05, 'epoch': 1.24}
{'loss': 3.8752, 'grad_norm': 18.576793670654297, 'learning_rate': 1.2170719351570415e-05, 'epoch': 1.25}
{'loss': 4.869, 'grad_norm': 75.39363861083984, 'learning_rate': 1.191742654508612e-05, 'epoch': 1.26}
{'loss': 4.6748, 'grad_norm': 58.54782485961914, 'learning_rate': 1.1664133738601825e-05, 'epoch': 1.27}
{'loss': 4.5395, 'grad_norm': 47.76193618774414, 'learning_rate': 1.1410840932117527e-05, 'epoch': 1.28}
{'loss': 4.7636, 'grad_norm': 19.177837371826172, 'learning_rate': 1.1157548125633232e-05, 'epoch': 1.28}
{'loss': 4.539, 'grad_norm': 31.09189796447754, 'learning_rate': 1.0904255319148937e-05, 'epoch': 1.29}
{'loss': 4.9452, 'grad_norm': 35.97896194458008, 'learning_rate': 1.0650962512664641e-05, 'epoch': 1.3}
{'loss': 4.7281, 'grad_norm': 37.49248504638672, 'learning_rate': 1.0397669706180346e-05, 'epoch': 1.31}
{'loss': 4.4608, 'grad_norm': 74.22390747070312, 'learning_rate': 1.014437689969605e-05, 'epoch': 1.31}
{'loss': 4.7901, 'grad_norm': 42.40995407104492, 'learning_rate': 9.891084093211753e-06, 'epoch': 1.32}
{'loss': 4.5181, 'grad_norm': 39.441322326660156, 'learning_rate': 9.637791286727458e-06, 'epoch': 1.33}
{'loss': 4.8987, 'grad_norm': 15.723505020141602, 'learning_rate': 9.384498480243162e-06, 'epoch': 1.34}
{'loss': 4.6699, 'grad_norm': 33.55105972290039, 'learning_rate': 9.131205673758867e-06, 'epoch': 1.35}
{'loss': 4.5287, 'grad_norm': 27.803085327148438, 'learning_rate': 8.87791286727457e-06, 'epoch': 1.35}
{'loss': 4.8156, 'grad_norm': 25.50801658630371, 'learning_rate': 8.624620060790274e-06, 'epoch': 1.36}
{'loss': 4.5432, 'grad_norm': 45.303131103515625, 'learning_rate': 8.371327254305979e-06, 'epoch': 1.37}
{'loss': 4.9786, 'grad_norm': 29.188688278198242, 'learning_rate': 8.118034447821681e-06, 'epoch': 1.38}
{'loss': 4.5675, 'grad_norm': 64.22029113769531, 'learning_rate': 7.864741641337386e-06, 'epoch': 1.38}
{'loss': 4.381, 'grad_norm': 45.51614761352539, 'learning_rate': 7.611448834853091e-06, 'epoch': 1.39}
{'loss': 4.9442, 'grad_norm': 71.76078033447266, 'learning_rate': 7.358156028368794e-06, 'epoch': 1.4}
{'loss': 4.0357, 'grad_norm': 17.500667572021484, 'learning_rate': 7.104863221884499e-06, 'epoch': 1.41}
{'loss': 4.6287, 'grad_norm': 19.834238052368164, 'learning_rate': 6.851570415400203e-06, 'epoch': 1.41}
{'loss': 4.0454, 'grad_norm': 68.73432922363281, 'learning_rate': 6.598277608915906e-06, 'epoch': 1.42}
{'loss': 4.2466, 'grad_norm': 33.186771392822266, 'learning_rate': 6.344984802431611e-06, 'epoch': 1.43}
{'loss': 4.8268, 'grad_norm': 43.87108612060547, 'learning_rate': 6.091691995947316e-06, 'epoch': 1.44}
{'loss': 3.9869, 'grad_norm': 42.86275100708008, 'learning_rate': 5.83839918946302e-06, 'epoch': 1.45}
{'loss': 4.6221, 'grad_norm': 21.268177032470703, 'learning_rate': 5.5851063829787235e-06, 'epoch': 1.45}
{'loss': 3.9031, 'grad_norm': 25.512765884399414, 'learning_rate': 5.331813576494428e-06, 'epoch': 1.46}
{'loss': 4.2486, 'grad_norm': 27.26333999633789, 'learning_rate': 5.078520770010132e-06, 'epoch': 1.47}
{'loss': 4.6281, 'grad_norm': 27.392757415771484, 'learning_rate': 4.825227963525836e-06, 'epoch': 1.48}
{'loss': 3.7576, 'grad_norm': 69.01586151123047, 'learning_rate': 4.57193515704154e-06, 'epoch': 1.48}
{'loss': 4.6954, 'grad_norm': 47.18916702270508, 'learning_rate': 4.3186423505572445e-06, 'epoch': 1.49}
{'loss': 3.927, 'grad_norm': 27.512451171875, 'learning_rate': 4.065349544072949e-06, 'epoch': 1.5}
{'loss': 4.2257, 'grad_norm': 18.472009658813477, 'learning_rate': 3.8120567375886527e-06, 'epoch': 1.51}
{'loss': 4.1969, 'grad_norm': 47.91947937011719, 'learning_rate': 3.5587639311043564e-06, 'epoch': 1.52}
{'loss': 3.9192, 'grad_norm': 69.55303955078125, 'learning_rate': 3.305471124620061e-06, 'epoch': 1.52}
{'loss': 4.5702, 'grad_norm': 39.97932434082031, 'learning_rate': 3.052178318135765e-06, 'epoch': 1.53}
{'loss': 4.496, 'grad_norm': 37.522743225097656, 'learning_rate': 2.798885511651469e-06, 'epoch': 1.54}
{'loss': 4.0238, 'grad_norm': 27.690120697021484, 'learning_rate': 2.5455927051671733e-06, 'epoch': 1.55}
{'loss': 4.4703, 'grad_norm': 19.09488868713379, 'learning_rate': 2.2922998986828774e-06, 'epoch': 1.55}
{'loss': 3.5732, 'grad_norm': 31.98784637451172, 'learning_rate': 2.039007092198582e-06, 'epoch': 1.56}
{'loss': 4.6374, 'grad_norm': 30.966949462890625, 'learning_rate': 1.7857142857142857e-06, 'epoch': 1.57}
{'loss': 4.1234, 'grad_norm': 23.324562072753906, 'learning_rate': 1.53242147922999e-06, 'epoch': 1.58}
{'loss': 3.7175, 'grad_norm': 26.32760238647461, 'learning_rate': 1.2791286727456941e-06, 'epoch': 1.58}
{'loss': 4.8264, 'grad_norm': 39.96858215332031, 'learning_rate': 1.0258358662613983e-06, 'epoch': 1.59}
{'loss': 3.6295, 'grad_norm': 42.014373779296875, 'learning_rate': 7.725430597771024e-07, 'epoch': 1.6}
{'loss': 4.7337, 'grad_norm': 29.364688873291016, 'learning_rate': 5.192502532928066e-07, 'epoch': 1.61}
{'loss': 4.2957, 'grad_norm': 41.804176330566406, 'learning_rate': 2.6595744680851066e-07, 'epoch': 1.62}
{'loss': 4.253, 'grad_norm': 60.46019744873047, 'learning_rate': 1.2664640324214793e-08, 'epoch': 1.62}
{'train_runtime': 19039.4595, 'train_samples_per_second': 3.53, 'train_steps_per_second': 0.221, 'train_loss': 1.731116116841634, 'epoch': 1.62}
{'eval': {'eval_loss': 0.3057858347892761, 'eval_log_loss': 1.0004594740733248, 'eval_accuracy': 0.5096760165253316, 'eval_runtime': 1021.6483, 'eval_samples_per_second': 4.502, 'eval_steps_per_second': 4.502, 'epoch': 1.6236527959015996}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
