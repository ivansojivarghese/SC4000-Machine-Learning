[Step5] Distilling fold 4 using LLaMA-only OOF probs
{'loss': 11.8529, 'grad_norm': 122.44274139404297, 'learning_rate': 3.234451718494272e-05, 'epoch': 0.39}
{'loss': 11.8956, 'grad_norm': 95.76113891601562, 'learning_rate': 3.19353518821604e-05, 'epoch': 0.4}
{'loss': 15.0251, 'grad_norm': 62.76049041748047, 'learning_rate': 3.152618657937807e-05, 'epoch': 0.41}
{'loss': 13.278, 'grad_norm': 41.8907470703125, 'learning_rate': 3.111702127659575e-05, 'epoch': 0.42}
{'loss': 10.829, 'grad_norm': 72.65950775146484, 'learning_rate': 3.070785597381342e-05, 'epoch': 0.43}
{'loss': 14.6321, 'grad_norm': 15.773600578308105, 'learning_rate': 3.02986906710311e-05, 'epoch': 0.43}
{'loss': 17.6077, 'grad_norm': 117.70761108398438, 'learning_rate': 2.988952536824877e-05, 'epoch': 0.44}
{'loss': 13.0308, 'grad_norm': 16.921506881713867, 'learning_rate': 2.9480360065466452e-05, 'epoch': 0.45}
{'loss': 15.8002, 'grad_norm': 86.70624542236328, 'learning_rate': 2.9071194762684123e-05, 'epoch': 0.46}
{'loss': 14.2463, 'grad_norm': 86.0719223022461, 'learning_rate': 2.86620294599018e-05, 'epoch': 0.46}
{'loss': 16.4318, 'grad_norm': 107.81486511230469, 'learning_rate': 2.825286415711948e-05, 'epoch': 0.47}
{'loss': 13.8357, 'grad_norm': 65.93672943115234, 'learning_rate': 2.7843698854337153e-05, 'epoch': 0.48}
{'loss': 13.4468, 'grad_norm': 53.45294189453125, 'learning_rate': 2.743453355155483e-05, 'epoch': 0.49}
{'loss': 17.1942, 'grad_norm': 85.79936981201172, 'learning_rate': 2.7025368248772502e-05, 'epoch': 0.49}
{'loss': 15.2526, 'grad_norm': 96.22003173828125, 'learning_rate': 2.6616202945990183e-05, 'epoch': 0.5}
{'loss': 13.5288, 'grad_norm': 54.58074188232422, 'learning_rate': 2.6207037643207855e-05, 'epoch': 0.51}
{'loss': 13.5206, 'grad_norm': 87.08206939697266, 'learning_rate': 2.5797872340425532e-05, 'epoch': 0.52}
{'loss': 11.8588, 'grad_norm': 231.20016479492188, 'learning_rate': 2.538870703764321e-05, 'epoch': 0.53}
{'loss': 12.5997, 'grad_norm': 61.25674819946289, 'learning_rate': 2.4979541734860885e-05, 'epoch': 0.53}
{'loss': 12.2767, 'grad_norm': 197.2639923095703, 'learning_rate': 2.457037643207856e-05, 'epoch': 0.54}
{'loss': 14.9909, 'grad_norm': 78.15242004394531, 'learning_rate': 2.4161211129296237e-05, 'epoch': 0.55}
{'loss': 11.6329, 'grad_norm': 65.08586120605469, 'learning_rate': 2.3752045826513915e-05, 'epoch': 0.56}
{'loss': 9.3532, 'grad_norm': 69.88018798828125, 'learning_rate': 2.334288052373159e-05, 'epoch': 0.56}
{'loss': 11.4541, 'grad_norm': 27.575956344604492, 'learning_rate': 2.2933715220949264e-05, 'epoch': 0.57}
{'loss': 14.3685, 'grad_norm': 134.55282592773438, 'learning_rate': 2.252454991816694e-05, 'epoch': 0.58}
{'loss': 13.2403, 'grad_norm': 50.1322021484375, 'learning_rate': 2.2115384615384616e-05, 'epoch': 0.59}
{'loss': 11.9806, 'grad_norm': 63.04290008544922, 'learning_rate': 2.170621931260229e-05, 'epoch': 0.6}
{'loss': 11.5007, 'grad_norm': 64.28736877441406, 'learning_rate': 2.129705400981997e-05, 'epoch': 0.6}
{'loss': 17.176, 'grad_norm': 128.97555541992188, 'learning_rate': 2.0887888707037647e-05, 'epoch': 0.61}
{'loss': 12.7224, 'grad_norm': 76.93927001953125, 'learning_rate': 2.047872340425532e-05, 'epoch': 0.62}
{'loss': 13.6807, 'grad_norm': 23.618783950805664, 'learning_rate': 2.0069558101472996e-05, 'epoch': 0.63}
{'loss': 11.1619, 'grad_norm': 57.539833068847656, 'learning_rate': 1.966039279869067e-05, 'epoch': 0.63}
{'loss': 11.1546, 'grad_norm': 104.9137954711914, 'learning_rate': 1.9251227495908348e-05, 'epoch': 0.64}
{'loss': 13.0545, 'grad_norm': 68.35127258300781, 'learning_rate': 1.8842062193126022e-05, 'epoch': 0.65}
{'loss': 11.6165, 'grad_norm': 90.89856719970703, 'learning_rate': 1.84328968903437e-05, 'epoch': 0.66}
{'loss': 14.2937, 'grad_norm': 129.0371856689453, 'learning_rate': 1.8023731587561378e-05, 'epoch': 0.67}
{'loss': 15.2068, 'grad_norm': 54.390804290771484, 'learning_rate': 1.7614566284779053e-05, 'epoch': 0.67}
{'loss': 13.9645, 'grad_norm': 90.8848876953125, 'learning_rate': 1.7205400981996727e-05, 'epoch': 0.68}
{'loss': 9.877, 'grad_norm': 150.9456787109375, 'learning_rate': 1.67962356792144e-05, 'epoch': 0.69}
{'loss': 11.9428, 'grad_norm': 23.750194549560547, 'learning_rate': 1.638707037643208e-05, 'epoch': 0.7}
{'loss': 12.5442, 'grad_norm': 61.06749725341797, 'learning_rate': 1.5977905073649754e-05, 'epoch': 0.7}
{'loss': 11.4946, 'grad_norm': 57.633758544921875, 'learning_rate': 1.556873977086743e-05, 'epoch': 0.71}
{'loss': 11.3266, 'grad_norm': 40.463741302490234, 'learning_rate': 1.5159574468085108e-05, 'epoch': 0.72}
{'loss': 12.2437, 'grad_norm': 57.738555908203125, 'learning_rate': 1.4750409165302784e-05, 'epoch': 0.73}
{'loss': 13.4311, 'grad_norm': 93.42842102050781, 'learning_rate': 1.4341243862520459e-05, 'epoch': 0.73}
{'loss': 12.466, 'grad_norm': 28.65029525756836, 'learning_rate': 1.3932078559738135e-05, 'epoch': 0.74}
{'loss': 12.7146, 'grad_norm': 46.45401382446289, 'learning_rate': 1.3522913256955811e-05, 'epoch': 0.75}
{'loss': 15.4043, 'grad_norm': 107.29120635986328, 'learning_rate': 1.3113747954173485e-05, 'epoch': 0.76}
{'loss': 11.2525, 'grad_norm': 63.74659729003906, 'learning_rate': 1.2704582651391162e-05, 'epoch': 0.77}
{'loss': 10.6026, 'grad_norm': 188.1601104736328, 'learning_rate': 1.2295417348608838e-05, 'epoch': 0.77}
{'loss': 10.6305, 'grad_norm': 54.990840911865234, 'learning_rate': 1.1886252045826514e-05, 'epoch': 0.78}
{'loss': 15.1707, 'grad_norm': 66.11375427246094, 'learning_rate': 1.147708674304419e-05, 'epoch': 0.79}
{'loss': 12.4584, 'grad_norm': 33.01639175415039, 'learning_rate': 1.1067921440261866e-05, 'epoch': 0.8}
{'loss': 12.9468, 'grad_norm': 101.09535217285156, 'learning_rate': 1.0658756137479543e-05, 'epoch': 0.8}
{'loss': 11.6582, 'grad_norm': 75.52564239501953, 'learning_rate': 1.0249590834697217e-05, 'epoch': 0.81}
{'loss': 14.3496, 'grad_norm': 29.460800170898438, 'learning_rate': 9.840425531914895e-06, 'epoch': 0.82}
{'loss': 14.5597, 'grad_norm': 62.393592834472656, 'learning_rate': 9.43126022913257e-06, 'epoch': 0.83}
{'loss': 10.9037, 'grad_norm': 35.562599182128906, 'learning_rate': 9.022094926350246e-06, 'epoch': 0.84}
{'loss': 12.6215, 'grad_norm': 53.186920166015625, 'learning_rate': 8.612929623567922e-06, 'epoch': 0.84}
{'loss': 14.0652, 'grad_norm': 68.54013061523438, 'learning_rate': 8.203764320785598e-06, 'epoch': 0.85}
{'loss': 12.759, 'grad_norm': 26.865535736083984, 'learning_rate': 7.794599018003274e-06, 'epoch': 0.86}
{'loss': 10.2517, 'grad_norm': 55.0200080871582, 'learning_rate': 7.3854337152209495e-06, 'epoch': 0.87}
{'loss': 12.3037, 'grad_norm': 82.6699447631836, 'learning_rate': 6.976268412438626e-06, 'epoch': 0.87}
{'loss': 15.115, 'grad_norm': 72.92292785644531, 'learning_rate': 6.567103109656302e-06, 'epoch': 0.88}
{'loss': 12.8754, 'grad_norm': 63.79812240600586, 'learning_rate': 6.157937806873977e-06, 'epoch': 0.89}
{'loss': 13.5349, 'grad_norm': 47.87211608886719, 'learning_rate': 5.748772504091653e-06, 'epoch': 0.9}
{'loss': 12.1999, 'grad_norm': 156.77685546875, 'learning_rate': 5.3396072013093295e-06, 'epoch': 0.9}
{'loss': 14.2021, 'grad_norm': 135.306884765625, 'learning_rate': 4.930441898527005e-06, 'epoch': 0.91}
{'loss': 11.9132, 'grad_norm': 55.24002456665039, 'learning_rate': 4.521276595744681e-06, 'epoch': 0.92}
{'loss': 13.9659, 'grad_norm': 188.85052490234375, 'learning_rate': 4.112111292962357e-06, 'epoch': 0.93}
{'loss': 13.5739, 'grad_norm': 123.04370880126953, 'learning_rate': 3.702945990180033e-06, 'epoch': 0.94}
{'loss': 12.6643, 'grad_norm': 81.28120422363281, 'learning_rate': 3.2937806873977087e-06, 'epoch': 0.94}
{'loss': 11.5831, 'grad_norm': 43.72773742675781, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.95}
{'loss': 12.8224, 'grad_norm': 77.77079772949219, 'learning_rate': 2.4754500818330606e-06, 'epoch': 0.96}
{'loss': 13.0727, 'grad_norm': 93.63572692871094, 'learning_rate': 2.066284779050737e-06, 'epoch': 0.97}
{'loss': 11.3709, 'grad_norm': 39.27989959716797, 'learning_rate': 1.6571194762684126e-06, 'epoch': 0.97}
{'loss': 11.52, 'grad_norm': 74.87214660644531, 'learning_rate': 1.2479541734860885e-06, 'epoch': 0.98}
{'loss': 12.6793, 'grad_norm': 73.7435073852539, 'learning_rate': 8.387888707037644e-07, 'epoch': 0.99}
{'loss': 9.9337, 'grad_norm': 59.71742630004883, 'learning_rate': 4.2962356792144027e-07, 'epoch': 1.0}
{'loss': 14.689, 'grad_norm': 104.54752349853516, 'learning_rate': 2.0458265139116204e-08, 'epoch': 1.01}
{'train_runtime': 18751.0175, 'train_samples_per_second': 2.219, 'train_steps_per_second': 0.139, 'train_loss': 7.987620644202599, 'epoch': 1.01}
{'eval': {'eval_loss': 0.7845306992530823, 'eval_log_loss': 1.0566851422484862, 'eval_accuracy': 0.4627092846270928, 'eval_runtime': 1054.4847, 'eval_samples_per_second': 4.361, 'eval_steps_per_second': 4.361, 'epoch': 1.0054128506874804}}
[Step5] Done fold 4 -> model_save/distilled_gemma2-9b_fold_4
