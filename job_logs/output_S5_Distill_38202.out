[Step5] Distilling fold 2 using LLaMA-only OOF probs
{'loss': 5.3029, 'grad_norm': 35.94854736328125, 'learning_rate': 3.234451718494272e-05, 'epoch': 0.39}
{'loss': 5.0056, 'grad_norm': 47.94593811035156, 'learning_rate': 3.19353518821604e-05, 'epoch': 0.4}
{'loss': 4.7474, 'grad_norm': 47.531646728515625, 'learning_rate': 3.152618657937807e-05, 'epoch': 0.41}
{'loss': 5.182, 'grad_norm': 42.21200180053711, 'learning_rate': 3.111702127659575e-05, 'epoch': 0.42}
{'loss': 4.9436, 'grad_norm': 30.44683837890625, 'learning_rate': 3.070785597381342e-05, 'epoch': 0.43}
{'loss': 5.3628, 'grad_norm': 86.4358901977539, 'learning_rate': 3.02986906710311e-05, 'epoch': 0.43}
{'loss': 5.3612, 'grad_norm': 30.566879272460938, 'learning_rate': 2.988952536824877e-05, 'epoch': 0.44}
{'loss': 4.8723, 'grad_norm': 53.06886672973633, 'learning_rate': 2.9480360065466452e-05, 'epoch': 0.45}
{'loss': 5.3479, 'grad_norm': 40.518577575683594, 'learning_rate': 2.9071194762684123e-05, 'epoch': 0.46}
{'loss': 4.8171, 'grad_norm': 72.17247772216797, 'learning_rate': 2.86620294599018e-05, 'epoch': 0.46}
{'loss': 5.4389, 'grad_norm': 14.540732383728027, 'learning_rate': 2.825286415711948e-05, 'epoch': 0.47}
{'loss': 5.0975, 'grad_norm': 25.894489288330078, 'learning_rate': 2.7843698854337153e-05, 'epoch': 0.48}
{'loss': 4.9271, 'grad_norm': 38.681095123291016, 'learning_rate': 2.743453355155483e-05, 'epoch': 0.49}
{'loss': 5.0746, 'grad_norm': 46.22377014160156, 'learning_rate': 2.7025368248772502e-05, 'epoch': 0.49}
{'loss': 4.8514, 'grad_norm': 42.68436813354492, 'learning_rate': 2.6616202945990183e-05, 'epoch': 0.5}
{'loss': 5.3395, 'grad_norm': 16.469024658203125, 'learning_rate': 2.6207037643207855e-05, 'epoch': 0.51}
{'loss': 5.1646, 'grad_norm': 43.77429962158203, 'learning_rate': 2.5797872340425532e-05, 'epoch': 0.52}
{'loss': 4.8134, 'grad_norm': 36.6274299621582, 'learning_rate': 2.538870703764321e-05, 'epoch': 0.53}
{'loss': 5.0989, 'grad_norm': 29.00006675720215, 'learning_rate': 2.4979541734860885e-05, 'epoch': 0.53}
{'loss': 4.6659, 'grad_norm': 37.08715057373047, 'learning_rate': 2.457037643207856e-05, 'epoch': 0.54}
{'loss': 5.2947, 'grad_norm': 28.086341857910156, 'learning_rate': 2.4161211129296237e-05, 'epoch': 0.55}
{'loss': 5.078, 'grad_norm': 40.53711700439453, 'learning_rate': 2.3752045826513915e-05, 'epoch': 0.56}
{'loss': 4.92, 'grad_norm': 51.302207946777344, 'learning_rate': 2.334288052373159e-05, 'epoch': 0.56}
{'loss': 5.1485, 'grad_norm': 27.343677520751953, 'learning_rate': 2.2933715220949264e-05, 'epoch': 0.57}
{'loss': 4.8923, 'grad_norm': 23.336397171020508, 'learning_rate': 2.252454991816694e-05, 'epoch': 0.58}
{'loss': 5.2492, 'grad_norm': 14.319637298583984, 'learning_rate': 2.2115384615384616e-05, 'epoch': 0.59}
{'loss': 4.9842, 'grad_norm': 27.3922119140625, 'learning_rate': 2.170621931260229e-05, 'epoch': 0.6}
{'loss': 4.8968, 'grad_norm': 42.371734619140625, 'learning_rate': 2.129705400981997e-05, 'epoch': 0.6}
{'loss': 5.1864, 'grad_norm': 32.01871109008789, 'learning_rate': 2.0887888707037647e-05, 'epoch': 0.61}
{'loss': 4.7794, 'grad_norm': 25.867090225219727, 'learning_rate': 2.047872340425532e-05, 'epoch': 0.62}
{'loss': 5.2739, 'grad_norm': 45.54903793334961, 'learning_rate': 2.0069558101472996e-05, 'epoch': 0.63}
{'loss': 5.0261, 'grad_norm': 20.363290786743164, 'learning_rate': 1.966039279869067e-05, 'epoch': 0.63}
{'loss': 4.894, 'grad_norm': 23.074953079223633, 'learning_rate': 1.9251227495908348e-05, 'epoch': 0.64}
{'loss': 5.1282, 'grad_norm': 51.19294357299805, 'learning_rate': 1.8842062193126022e-05, 'epoch': 0.65}
{'loss': 4.6822, 'grad_norm': 23.199283599853516, 'learning_rate': 1.84328968903437e-05, 'epoch': 0.66}
{'loss': 4.9931, 'grad_norm': 44.082191467285156, 'learning_rate': 1.8023731587561378e-05, 'epoch': 0.67}
{'loss': 4.8159, 'grad_norm': 36.43740463256836, 'learning_rate': 1.7614566284779053e-05, 'epoch': 0.67}
{'loss': 4.7823, 'grad_norm': 55.70105743408203, 'learning_rate': 1.7205400981996727e-05, 'epoch': 0.68}
{'loss': 4.7889, 'grad_norm': 23.031240463256836, 'learning_rate': 1.67962356792144e-05, 'epoch': 0.69}
{'loss': 4.7234, 'grad_norm': 20.20037078857422, 'learning_rate': 1.638707037643208e-05, 'epoch': 0.7}
{'loss': 5.1974, 'grad_norm': 40.21253967285156, 'learning_rate': 1.5977905073649754e-05, 'epoch': 0.7}
{'loss': 4.9483, 'grad_norm': 51.21811294555664, 'learning_rate': 1.556873977086743e-05, 'epoch': 0.71}
{'loss': 4.7776, 'grad_norm': 20.007837295532227, 'learning_rate': 1.5159574468085108e-05, 'epoch': 0.72}
{'loss': 5.0482, 'grad_norm': 13.733389854431152, 'learning_rate': 1.4750409165302784e-05, 'epoch': 0.73}
{'loss': 4.752, 'grad_norm': 55.473575592041016, 'learning_rate': 1.4341243862520459e-05, 'epoch': 0.73}
{'loss': 5.1193, 'grad_norm': 24.623849868774414, 'learning_rate': 1.3932078559738135e-05, 'epoch': 0.74}
{'loss': 4.8633, 'grad_norm': 67.76318359375, 'learning_rate': 1.3522913256955811e-05, 'epoch': 0.75}
{'loss': 4.7898, 'grad_norm': 27.850528717041016, 'learning_rate': 1.3113747954173485e-05, 'epoch': 0.76}
{'loss': 5.1844, 'grad_norm': 38.01951599121094, 'learning_rate': 1.2704582651391162e-05, 'epoch': 0.77}
{'loss': 4.6243, 'grad_norm': 31.63722038269043, 'learning_rate': 1.2295417348608838e-05, 'epoch': 0.77}
{'loss': 5.1447, 'grad_norm': 45.46075439453125, 'learning_rate': 1.1886252045826514e-05, 'epoch': 0.78}
{'loss': 4.844, 'grad_norm': 31.544286727905273, 'learning_rate': 1.147708674304419e-05, 'epoch': 0.79}
{'loss': 4.5713, 'grad_norm': 17.9309024810791, 'learning_rate': 1.1067921440261866e-05, 'epoch': 0.8}
{'loss': 5.0656, 'grad_norm': 40.26129913330078, 'learning_rate': 1.0658756137479543e-05, 'epoch': 0.8}
{'loss': 4.4046, 'grad_norm': 41.571006774902344, 'learning_rate': 1.0249590834697217e-05, 'epoch': 0.81}
{'loss': 5.1835, 'grad_norm': 40.63969421386719, 'learning_rate': 9.840425531914895e-06, 'epoch': 0.82}
{'loss': 4.9144, 'grad_norm': 22.607925415039062, 'learning_rate': 9.43126022913257e-06, 'epoch': 0.83}
{'loss': 4.4883, 'grad_norm': 49.449222564697266, 'learning_rate': 9.022094926350246e-06, 'epoch': 0.84}
{'loss': 5.1548, 'grad_norm': 33.8529167175293, 'learning_rate': 8.612929623567922e-06, 'epoch': 0.84}
{'loss': 4.4282, 'grad_norm': 19.33445167541504, 'learning_rate': 8.203764320785598e-06, 'epoch': 0.85}
{'loss': 5.1522, 'grad_norm': 20.40180015563965, 'learning_rate': 7.794599018003274e-06, 'epoch': 0.86}
{'loss': 4.7659, 'grad_norm': 62.36470413208008, 'learning_rate': 7.3854337152209495e-06, 'epoch': 0.87}
{'loss': 4.5352, 'grad_norm': 39.786075592041016, 'learning_rate': 6.976268412438626e-06, 'epoch': 0.87}
{'loss': 5.0419, 'grad_norm': 24.398155212402344, 'learning_rate': 6.567103109656302e-06, 'epoch': 0.88}
{'loss': 4.2672, 'grad_norm': 50.02553176879883, 'learning_rate': 6.157937806873977e-06, 'epoch': 0.89}
{'loss': 5.1179, 'grad_norm': 27.478235244750977, 'learning_rate': 5.748772504091653e-06, 'epoch': 0.9}
{'loss': 4.7966, 'grad_norm': 31.642349243164062, 'learning_rate': 5.3396072013093295e-06, 'epoch': 0.9}
{'loss': 4.7989, 'grad_norm': 45.649513244628906, 'learning_rate': 4.930441898527005e-06, 'epoch': 0.91}
{'loss': 5.0821, 'grad_norm': 32.906959533691406, 'learning_rate': 4.521276595744681e-06, 'epoch': 0.92}
{'loss': 4.4911, 'grad_norm': 24.767868041992188, 'learning_rate': 4.112111292962357e-06, 'epoch': 0.93}
{'loss': 5.0652, 'grad_norm': 13.408595085144043, 'learning_rate': 3.702945990180033e-06, 'epoch': 0.94}
{'loss': 4.7868, 'grad_norm': 55.0362434387207, 'learning_rate': 3.2937806873977087e-06, 'epoch': 0.94}
{'loss': 4.7483, 'grad_norm': 57.65422439575195, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.95}
{'loss': 5.0182, 'grad_norm': 27.049182891845703, 'learning_rate': 2.4754500818330606e-06, 'epoch': 0.96}
{'loss': 4.4397, 'grad_norm': 34.742645263671875, 'learning_rate': 2.066284779050737e-06, 'epoch': 0.97}
{'loss': 4.9346, 'grad_norm': 50.673057556152344, 'learning_rate': 1.6571194762684126e-06, 'epoch': 0.97}
{'loss': 4.8365, 'grad_norm': 24.792951583862305, 'learning_rate': 1.2479541734860885e-06, 'epoch': 0.98}
{'loss': 4.5, 'grad_norm': 25.342809677124023, 'learning_rate': 8.387888707037644e-07, 'epoch': 0.99}
{'loss': 4.852, 'grad_norm': 28.636152267456055, 'learning_rate': 4.2962356792144027e-07, 'epoch': 1.0}
{'loss': 4.7763, 'grad_norm': 30.948823928833008, 'learning_rate': 2.0458265139116204e-08, 'epoch': 1.01}
{'train_runtime': 20337.174, 'train_samples_per_second': 2.046, 'train_steps_per_second': 0.128, 'train_loss': 3.0343241471510667, 'epoch': 1.01}
{'eval': {'eval_loss': 0.29916784167289734, 'eval_log_loss': 0.9847092925155175, 'eval_accuracy': 0.5033702978908459, 'eval_runtime': 1025.6814, 'eval_samples_per_second': 4.484, 'eval_steps_per_second': 4.484, 'epoch': 1.0054128506874804}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
