[Step5] Distilling fold 4 using LLaMA-only OOF probs
{'loss': 11.7144, 'grad_norm': 30.921491622924805, 'learning_rate': 2.0022796352583586e-05, 'epoch': 1.01}
{'loss': 11.248, 'grad_norm': 26.428194046020508, 'learning_rate': 1.976950354609929e-05, 'epoch': 1.02}
{'loss': 15.0724, 'grad_norm': 43.967613220214844, 'learning_rate': 1.9516210739614996e-05, 'epoch': 1.03}
{'loss': 12.0403, 'grad_norm': 51.1473503112793, 'learning_rate': 1.92629179331307e-05, 'epoch': 1.04}
{'loss': 13.5496, 'grad_norm': 52.630558013916016, 'learning_rate': 1.9009625126646405e-05, 'epoch': 1.04}
{'loss': 10.3654, 'grad_norm': 70.12738800048828, 'learning_rate': 1.875633232016211e-05, 'epoch': 1.05}
{'loss': 14.5595, 'grad_norm': 70.33063507080078, 'learning_rate': 1.850303951367781e-05, 'epoch': 1.06}
{'loss': 12.7548, 'grad_norm': 140.21173095703125, 'learning_rate': 1.8249746707193515e-05, 'epoch': 1.07}
{'loss': 12.3796, 'grad_norm': 84.99295043945312, 'learning_rate': 1.799645390070922e-05, 'epoch': 1.07}
{'loss': 12.0277, 'grad_norm': 49.71073532104492, 'learning_rate': 1.7743161094224924e-05, 'epoch': 1.08}
{'loss': 10.558, 'grad_norm': 51.36100387573242, 'learning_rate': 1.748986828774063e-05, 'epoch': 1.09}
{'loss': 13.6179, 'grad_norm': 126.09576416015625, 'learning_rate': 1.7236575481256333e-05, 'epoch': 1.1}
{'loss': 12.8679, 'grad_norm': 92.73748779296875, 'learning_rate': 1.6983282674772038e-05, 'epoch': 1.11}
{'loss': 12.9682, 'grad_norm': 94.02338409423828, 'learning_rate': 1.672998986828774e-05, 'epoch': 1.11}
{'loss': 12.2038, 'grad_norm': 108.86605834960938, 'learning_rate': 1.6476697061803443e-05, 'epoch': 1.12}
{'loss': 11.4059, 'grad_norm': 108.95448303222656, 'learning_rate': 1.6223404255319148e-05, 'epoch': 1.13}
{'loss': 13.7166, 'grad_norm': 60.092262268066406, 'learning_rate': 1.5970111448834852e-05, 'epoch': 1.14}
{'loss': 13.2368, 'grad_norm': 51.8385124206543, 'learning_rate': 1.571681864235056e-05, 'epoch': 1.14}
{'loss': 11.2231, 'grad_norm': 127.22705841064453, 'learning_rate': 1.5463525835866265e-05, 'epoch': 1.15}
{'loss': 11.7777, 'grad_norm': 79.36959075927734, 'learning_rate': 1.5210233029381964e-05, 'epoch': 1.16}
{'loss': 10.84, 'grad_norm': 60.63602828979492, 'learning_rate': 1.4956940222897669e-05, 'epoch': 1.17}
{'loss': 13.2769, 'grad_norm': 65.75719451904297, 'learning_rate': 1.4703647416413373e-05, 'epoch': 1.18}
{'loss': 12.9705, 'grad_norm': 88.1019287109375, 'learning_rate': 1.4450354609929078e-05, 'epoch': 1.18}
{'loss': 13.2895, 'grad_norm': 54.429019927978516, 'learning_rate': 1.4197061803444783e-05, 'epoch': 1.19}
{'loss': 8.5972, 'grad_norm': 68.15742492675781, 'learning_rate': 1.3943768996960487e-05, 'epoch': 1.2}
{'loss': 12.3425, 'grad_norm': 61.637454986572266, 'learning_rate': 1.3690476190476192e-05, 'epoch': 1.21}
{'loss': 12.0403, 'grad_norm': 111.79566192626953, 'learning_rate': 1.3437183383991894e-05, 'epoch': 1.21}
{'loss': 11.4318, 'grad_norm': 134.67088317871094, 'learning_rate': 1.3183890577507599e-05, 'epoch': 1.22}
{'loss': 13.4829, 'grad_norm': 141.78121948242188, 'learning_rate': 1.2930597771023304e-05, 'epoch': 1.23}
{'loss': 11.5693, 'grad_norm': 43.77045822143555, 'learning_rate': 1.2677304964539008e-05, 'epoch': 1.24}
{'loss': 13.3422, 'grad_norm': 73.38186645507812, 'learning_rate': 1.2424012158054713e-05, 'epoch': 1.24}
{'loss': 14.6059, 'grad_norm': 62.17033004760742, 'learning_rate': 1.2170719351570415e-05, 'epoch': 1.25}
{'loss': 12.458, 'grad_norm': 120.18241119384766, 'learning_rate': 1.191742654508612e-05, 'epoch': 1.26}
{'loss': 12.4978, 'grad_norm': 74.29304504394531, 'learning_rate': 1.1664133738601825e-05, 'epoch': 1.27}
{'loss': 11.6361, 'grad_norm': 74.78067779541016, 'learning_rate': 1.1410840932117527e-05, 'epoch': 1.28}
{'loss': 14.1206, 'grad_norm': 85.87960052490234, 'learning_rate': 1.1157548125633232e-05, 'epoch': 1.28}
{'loss': 12.702, 'grad_norm': 84.91619873046875, 'learning_rate': 1.0904255319148937e-05, 'epoch': 1.29}
{'loss': 9.1401, 'grad_norm': 139.28294372558594, 'learning_rate': 1.0650962512664641e-05, 'epoch': 1.3}
{'loss': 11.2884, 'grad_norm': 40.36704635620117, 'learning_rate': 1.0397669706180346e-05, 'epoch': 1.31}
{'loss': 12.6433, 'grad_norm': 107.87969970703125, 'learning_rate': 1.014437689969605e-05, 'epoch': 1.31}
{'loss': 13.1265, 'grad_norm': 83.72571563720703, 'learning_rate': 9.891084093211753e-06, 'epoch': 1.32}
{'loss': 12.2203, 'grad_norm': 135.4818115234375, 'learning_rate': 9.637791286727458e-06, 'epoch': 1.33}
{'loss': 14.2183, 'grad_norm': 43.530517578125, 'learning_rate': 9.384498480243162e-06, 'epoch': 1.34}
{'loss': 14.3383, 'grad_norm': 62.630313873291016, 'learning_rate': 9.131205673758867e-06, 'epoch': 1.35}
{'loss': 11.3203, 'grad_norm': 145.40574645996094, 'learning_rate': 8.87791286727457e-06, 'epoch': 1.35}
{'loss': 11.1992, 'grad_norm': 175.80357360839844, 'learning_rate': 8.624620060790274e-06, 'epoch': 1.36}
{'loss': 13.36, 'grad_norm': 88.6407470703125, 'learning_rate': 8.371327254305979e-06, 'epoch': 1.37}
{'loss': 14.3708, 'grad_norm': 115.80644226074219, 'learning_rate': 8.118034447821681e-06, 'epoch': 1.38}
{'loss': 10.3144, 'grad_norm': 95.60681915283203, 'learning_rate': 7.864741641337386e-06, 'epoch': 1.38}
{'loss': 11.2602, 'grad_norm': 48.27841567993164, 'learning_rate': 7.611448834853091e-06, 'epoch': 1.39}
{'loss': 10.3902, 'grad_norm': 61.84791946411133, 'learning_rate': 7.358156028368794e-06, 'epoch': 1.4}
{'loss': 13.8334, 'grad_norm': 195.94674682617188, 'learning_rate': 7.104863221884499e-06, 'epoch': 1.41}
{'loss': 12.5061, 'grad_norm': 88.91049194335938, 'learning_rate': 6.851570415400203e-06, 'epoch': 1.41}
{'loss': 11.7755, 'grad_norm': 21.180824279785156, 'learning_rate': 6.598277608915906e-06, 'epoch': 1.42}
{'loss': 11.4278, 'grad_norm': 75.71166229248047, 'learning_rate': 6.344984802431611e-06, 'epoch': 1.43}
{'loss': 14.7112, 'grad_norm': 132.39483642578125, 'learning_rate': 6.091691995947316e-06, 'epoch': 1.44}
{'loss': 14.5406, 'grad_norm': 69.91971588134766, 'learning_rate': 5.83839918946302e-06, 'epoch': 1.45}
{'loss': 11.4595, 'grad_norm': 128.45062255859375, 'learning_rate': 5.5851063829787235e-06, 'epoch': 1.45}
{'loss': 12.5276, 'grad_norm': 103.780029296875, 'learning_rate': 5.331813576494428e-06, 'epoch': 1.46}
{'loss': 16.0094, 'grad_norm': 138.44944763183594, 'learning_rate': 5.078520770010132e-06, 'epoch': 1.47}
{'loss': 10.9943, 'grad_norm': 64.25572204589844, 'learning_rate': 4.825227963525836e-06, 'epoch': 1.48}
{'loss': 15.1264, 'grad_norm': 61.15298843383789, 'learning_rate': 4.57193515704154e-06, 'epoch': 1.48}
{'loss': 14.917, 'grad_norm': 181.0802001953125, 'learning_rate': 4.3186423505572445e-06, 'epoch': 1.49}
{'loss': 11.3588, 'grad_norm': 39.223793029785156, 'learning_rate': 4.065349544072949e-06, 'epoch': 1.5}
{'loss': 13.6528, 'grad_norm': 117.56266021728516, 'learning_rate': 3.8120567375886527e-06, 'epoch': 1.51}
{'loss': 12.1807, 'grad_norm': 78.36375427246094, 'learning_rate': 3.5587639311043564e-06, 'epoch': 1.52}
{'loss': 9.8267, 'grad_norm': 43.90156936645508, 'learning_rate': 3.305471124620061e-06, 'epoch': 1.52}
{'loss': 11.0607, 'grad_norm': 56.19602966308594, 'learning_rate': 3.052178318135765e-06, 'epoch': 1.53}
{'loss': 11.3822, 'grad_norm': 65.33283996582031, 'learning_rate': 2.798885511651469e-06, 'epoch': 1.54}
{'loss': 12.628, 'grad_norm': 118.10675048828125, 'learning_rate': 2.5455927051671733e-06, 'epoch': 1.55}
{'loss': 11.418, 'grad_norm': 145.62489318847656, 'learning_rate': 2.2922998986828774e-06, 'epoch': 1.55}
{'loss': 10.1612, 'grad_norm': 100.6159896850586, 'learning_rate': 2.039007092198582e-06, 'epoch': 1.56}
{'loss': 8.0436, 'grad_norm': 40.93421173095703, 'learning_rate': 1.7857142857142857e-06, 'epoch': 1.57}
{'loss': 13.6571, 'grad_norm': 125.65328216552734, 'learning_rate': 1.53242147922999e-06, 'epoch': 1.58}
{'loss': 13.6045, 'grad_norm': 33.516883850097656, 'learning_rate': 1.2791286727456941e-06, 'epoch': 1.58}
{'loss': 12.0741, 'grad_norm': 91.14018249511719, 'learning_rate': 1.0258358662613983e-06, 'epoch': 1.59}
{'loss': 8.1528, 'grad_norm': 161.6882781982422, 'learning_rate': 7.725430597771024e-07, 'epoch': 1.6}
{'loss': 14.263, 'grad_norm': 55.24222183227539, 'learning_rate': 5.192502532928066e-07, 'epoch': 1.61}
{'loss': 13.7123, 'grad_norm': 74.56177520751953, 'learning_rate': 2.6595744680851066e-07, 'epoch': 1.62}
{'loss': 11.9061, 'grad_norm': 163.19354248046875, 'learning_rate': 1.2664640324214793e-08, 'epoch': 1.62}
{'train_runtime': 19003.0954, 'train_samples_per_second': 3.536, 'train_steps_per_second': 0.221, 'train_loss': 4.707584835234142, 'epoch': 1.62}
{'eval': {'eval_loss': 0.7912408113479614, 'eval_log_loss': 1.083769369683381, 'eval_accuracy': 0.47532072189606434, 'eval_runtime': 1033.3385, 'eval_samples_per_second': 4.451, 'eval_steps_per_second': 4.451, 'epoch': 1.6236377256361307}}
[Step5] Done fold 4 -> model_save/distilled_gemma2-9b_fold_4
