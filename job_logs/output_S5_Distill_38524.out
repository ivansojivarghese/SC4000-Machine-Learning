[Step5] Distilling fold 3 using LLaMA-only OOF probs
{'loss': 5.1306, 'grad_norm': 97.05120086669922, 'learning_rate': 2.0022796352583586e-05, 'epoch': 1.01}
{'loss': 4.472, 'grad_norm': 58.33653259277344, 'learning_rate': 1.976950354609929e-05, 'epoch': 1.02}
{'loss': 5.0866, 'grad_norm': 14.579802513122559, 'learning_rate': 1.9516210739614996e-05, 'epoch': 1.03}
{'loss': 4.6626, 'grad_norm': 17.88235092163086, 'learning_rate': 1.92629179331307e-05, 'epoch': 1.04}
{'loss': 4.5785, 'grad_norm': 70.0860366821289, 'learning_rate': 1.9009625126646405e-05, 'epoch': 1.04}
{'loss': 4.9398, 'grad_norm': 52.337615966796875, 'learning_rate': 1.875633232016211e-05, 'epoch': 1.05}
{'loss': 4.8152, 'grad_norm': 55.5955696105957, 'learning_rate': 1.850303951367781e-05, 'epoch': 1.06}
{'loss': 4.4809, 'grad_norm': 60.73633575439453, 'learning_rate': 1.8249746707193515e-05, 'epoch': 1.07}
{'loss': 4.7607, 'grad_norm': 46.723670959472656, 'learning_rate': 1.799645390070922e-05, 'epoch': 1.07}
{'loss': 4.9381, 'grad_norm': 53.71978759765625, 'learning_rate': 1.7743161094224924e-05, 'epoch': 1.08}
{'loss': 4.9136, 'grad_norm': 63.591922760009766, 'learning_rate': 1.748986828774063e-05, 'epoch': 1.09}
{'loss': 4.5048, 'grad_norm': 36.28343200683594, 'learning_rate': 1.7236575481256333e-05, 'epoch': 1.1}
{'loss': 5.2158, 'grad_norm': 56.79462432861328, 'learning_rate': 1.6983282674772038e-05, 'epoch': 1.11}
{'loss': 4.8037, 'grad_norm': 42.57939910888672, 'learning_rate': 1.672998986828774e-05, 'epoch': 1.11}
{'loss': 4.6743, 'grad_norm': 31.434328079223633, 'learning_rate': 1.6476697061803443e-05, 'epoch': 1.12}
{'loss': 5.1253, 'grad_norm': 31.781408309936523, 'learning_rate': 1.6223404255319148e-05, 'epoch': 1.13}
{'loss': 4.4838, 'grad_norm': 22.68118667602539, 'learning_rate': 1.5970111448834852e-05, 'epoch': 1.14}
{'loss': 5.0825, 'grad_norm': 24.45964241027832, 'learning_rate': 1.571681864235056e-05, 'epoch': 1.14}
{'loss': 4.864, 'grad_norm': 24.21710777282715, 'learning_rate': 1.5463525835866265e-05, 'epoch': 1.15}
{'loss': 4.5075, 'grad_norm': 25.391193389892578, 'learning_rate': 1.5210233029381964e-05, 'epoch': 1.16}
{'loss': 4.8636, 'grad_norm': 60.428035736083984, 'learning_rate': 1.4956940222897669e-05, 'epoch': 1.17}
{'loss': 4.1846, 'grad_norm': 77.37244415283203, 'learning_rate': 1.4703647416413373e-05, 'epoch': 1.18}
{'loss': 4.941, 'grad_norm': 37.10634994506836, 'learning_rate': 1.4450354609929078e-05, 'epoch': 1.18}
{'loss': 4.7326, 'grad_norm': 38.90443801879883, 'learning_rate': 1.4197061803444783e-05, 'epoch': 1.19}
{'loss': 4.6627, 'grad_norm': 26.981569290161133, 'learning_rate': 1.3943768996960487e-05, 'epoch': 1.2}
{'loss': 4.8449, 'grad_norm': 26.04468536376953, 'learning_rate': 1.3690476190476192e-05, 'epoch': 1.21}
{'loss': 3.9263, 'grad_norm': 28.453384399414062, 'learning_rate': 1.3437183383991894e-05, 'epoch': 1.21}
{'loss': 4.5529, 'grad_norm': 30.6651611328125, 'learning_rate': 1.3183890577507599e-05, 'epoch': 1.22}
{'loss': 4.5464, 'grad_norm': 47.2307243347168, 'learning_rate': 1.2930597771023304e-05, 'epoch': 1.23}
{'loss': 4.7163, 'grad_norm': 33.43472671508789, 'learning_rate': 1.2677304964539008e-05, 'epoch': 1.24}
{'loss': 4.913, 'grad_norm': 56.78382110595703, 'learning_rate': 1.2424012158054713e-05, 'epoch': 1.24}
{'loss': 3.8937, 'grad_norm': 21.20160675048828, 'learning_rate': 1.2170719351570415e-05, 'epoch': 1.25}
{'loss': 4.8214, 'grad_norm': 52.07617950439453, 'learning_rate': 1.191742654508612e-05, 'epoch': 1.26}
{'loss': 4.445, 'grad_norm': 71.09959411621094, 'learning_rate': 1.1664133738601825e-05, 'epoch': 1.27}
{'loss': 4.1947, 'grad_norm': 30.229270935058594, 'learning_rate': 1.1410840932117527e-05, 'epoch': 1.28}
{'loss': 4.8426, 'grad_norm': 21.5286808013916, 'learning_rate': 1.1157548125633232e-05, 'epoch': 1.28}
{'loss': 4.0564, 'grad_norm': 21.71341323852539, 'learning_rate': 1.0904255319148937e-05, 'epoch': 1.29}
{'loss': 4.6766, 'grad_norm': 46.71907043457031, 'learning_rate': 1.0650962512664641e-05, 'epoch': 1.3}
{'loss': 4.6761, 'grad_norm': 27.16578483581543, 'learning_rate': 1.0397669706180346e-05, 'epoch': 1.31}
{'loss': 4.6079, 'grad_norm': 20.89548683166504, 'learning_rate': 1.014437689969605e-05, 'epoch': 1.31}
{'loss': 4.9739, 'grad_norm': 47.090858459472656, 'learning_rate': 9.891084093211753e-06, 'epoch': 1.32}
{'loss': 4.5147, 'grad_norm': 22.99619483947754, 'learning_rate': 9.637791286727458e-06, 'epoch': 1.33}
{'loss': 4.9472, 'grad_norm': 34.879539489746094, 'learning_rate': 9.384498480243162e-06, 'epoch': 1.34}
{'loss': 4.8394, 'grad_norm': 41.31339645385742, 'learning_rate': 9.131205673758867e-06, 'epoch': 1.35}
{'loss': 4.721, 'grad_norm': 62.62030029296875, 'learning_rate': 8.87791286727457e-06, 'epoch': 1.35}
{'loss': 4.9307, 'grad_norm': 45.98313903808594, 'learning_rate': 8.624620060790274e-06, 'epoch': 1.36}
{'loss': 4.3336, 'grad_norm': 44.959659576416016, 'learning_rate': 8.371327254305979e-06, 'epoch': 1.37}
{'loss': 4.9128, 'grad_norm': 29.573516845703125, 'learning_rate': 8.118034447821681e-06, 'epoch': 1.38}
{'loss': 4.7172, 'grad_norm': 41.57278823852539, 'learning_rate': 7.864741641337386e-06, 'epoch': 1.38}
{'loss': 4.3755, 'grad_norm': 41.31315231323242, 'learning_rate': 7.611448834853091e-06, 'epoch': 1.39}
{'loss': 4.4687, 'grad_norm': 25.09825897216797, 'learning_rate': 7.358156028368794e-06, 'epoch': 1.4}
{'loss': 3.2614, 'grad_norm': 80.69541931152344, 'learning_rate': 7.104863221884499e-06, 'epoch': 1.41}
{'loss': 4.5845, 'grad_norm': 51.128211975097656, 'learning_rate': 6.851570415400203e-06, 'epoch': 1.41}
{'loss': 4.0218, 'grad_norm': 40.20757293701172, 'learning_rate': 6.598277608915906e-06, 'epoch': 1.42}
{'loss': 4.1214, 'grad_norm': 31.136259078979492, 'learning_rate': 6.344984802431611e-06, 'epoch': 1.43}
{'loss': 4.5523, 'grad_norm': 35.48640823364258, 'learning_rate': 6.091691995947316e-06, 'epoch': 1.44}
{'loss': 3.6663, 'grad_norm': 47.06383514404297, 'learning_rate': 5.83839918946302e-06, 'epoch': 1.45}
{'loss': 4.6536, 'grad_norm': 53.03157043457031, 'learning_rate': 5.5851063829787235e-06, 'epoch': 1.45}
{'loss': 3.8664, 'grad_norm': 20.897207260131836, 'learning_rate': 5.331813576494428e-06, 'epoch': 1.46}
{'loss': 4.4025, 'grad_norm': 30.51058006286621, 'learning_rate': 5.078520770010132e-06, 'epoch': 1.47}
{'loss': 4.4714, 'grad_norm': 64.0059814453125, 'learning_rate': 4.825227963525836e-06, 'epoch': 1.48}
{'loss': 3.6524, 'grad_norm': 33.59187698364258, 'learning_rate': 4.57193515704154e-06, 'epoch': 1.48}
{'loss': 4.6066, 'grad_norm': 48.006221771240234, 'learning_rate': 4.3186423505572445e-06, 'epoch': 1.49}
{'loss': 4.4114, 'grad_norm': 23.619033813476562, 'learning_rate': 4.065349544072949e-06, 'epoch': 1.5}
{'loss': 4.3639, 'grad_norm': 30.732261657714844, 'learning_rate': 3.8120567375886527e-06, 'epoch': 1.51}
{'loss': 4.7606, 'grad_norm': 26.061391830444336, 'learning_rate': 3.5587639311043564e-06, 'epoch': 1.52}
{'loss': 3.8851, 'grad_norm': 41.61205291748047, 'learning_rate': 3.305471124620061e-06, 'epoch': 1.52}
{'loss': 4.553, 'grad_norm': 36.460994720458984, 'learning_rate': 3.052178318135765e-06, 'epoch': 1.53}
{'loss': 3.9825, 'grad_norm': 55.66938018798828, 'learning_rate': 2.798885511651469e-06, 'epoch': 1.54}
{'loss': 3.8964, 'grad_norm': 32.60402297973633, 'learning_rate': 2.5455927051671733e-06, 'epoch': 1.55}
{'loss': 4.6556, 'grad_norm': 31.18834686279297, 'learning_rate': 2.2922998986828774e-06, 'epoch': 1.55}
{'loss': 3.8625, 'grad_norm': 48.320255279541016, 'learning_rate': 2.039007092198582e-06, 'epoch': 1.56}
{'loss': 4.8442, 'grad_norm': 34.53217315673828, 'learning_rate': 1.7857142857142857e-06, 'epoch': 1.57}
{'loss': 4.0291, 'grad_norm': 36.958003997802734, 'learning_rate': 1.53242147922999e-06, 'epoch': 1.58}
{'loss': 4.1421, 'grad_norm': 38.607025146484375, 'learning_rate': 1.2791286727456941e-06, 'epoch': 1.58}
{'loss': 4.7006, 'grad_norm': 58.55234909057617, 'learning_rate': 1.0258358662613983e-06, 'epoch': 1.59}
{'loss': 3.6658, 'grad_norm': 30.841459274291992, 'learning_rate': 7.725430597771024e-07, 'epoch': 1.6}
{'loss': 4.4635, 'grad_norm': 52.753578186035156, 'learning_rate': 5.192502532928066e-07, 'epoch': 1.61}
{'loss': 4.2431, 'grad_norm': 40.4078369140625, 'learning_rate': 2.6595744680851066e-07, 'epoch': 1.62}
{'loss': 4.0982, 'grad_norm': 22.326278686523438, 'learning_rate': 1.2664640324214793e-08, 'epoch': 1.62}
{'train_runtime': 18945.4417, 'train_samples_per_second': 3.547, 'train_steps_per_second': 0.222, 'train_loss': 1.7204359853835334, 'epoch': 1.62}
{'eval': {'eval_loss': 0.3065837025642395, 'eval_log_loss': 1.0038174024149413, 'eval_accuracy': 0.5164166123070233, 'eval_runtime': 1019.5652, 'eval_samples_per_second': 4.511, 'eval_steps_per_second': 4.511, 'epoch': 1.6236377256361307}}
[Step5] Done fold 3 -> model_save/distilled_gemma2-9b_fold_3
