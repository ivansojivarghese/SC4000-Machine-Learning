[Step6] Averaging LoRA adapters across folds: 0,1,2,3,4 -> model_save/avg_lora
[Step6] Found 5 adapters: ['model_save/distilled_gemma2-9b_fold_0/adapter_model.safetensors', 'model_save/distilled_gemma2-9b_fold_1/adapter_model.safetensors', 'model_save/distilled_gemma2-9b_fold_2/adapter_model.safetensors', 'model_save/distilled_gemma2-9b_fold_3/adapter_model.safetensors', 'model_save/distilled_gemma2-9b_fold_4/adapter_model.safetensors']
[Step6] Wrote averaged adapter -> model_save/avg_lora/adapter_model.safetensors
[Step6] Copied adapter_config.json from model_save/distilled_gemma2-9b_fold_0/adapter_config.json -> model_save/avg_lora/adapter_config.json
[Step6] Merging averaged LoRA into base: google/gemma-2-9b-it -> model_save/final_merged_model
[Merge] Saving merged model to model_save/final_merged_model
[Step6] Done: merged model in model_save/final_merged_model
