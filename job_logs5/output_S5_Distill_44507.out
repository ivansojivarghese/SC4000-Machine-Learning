[Step5] Distilling fold 0 using LLaMA-only OOF probs
[Step5] Using teacher OOF table: model_save/teacher_logits/oof_probs_fold0_merged.parquet (selector=auto)
[Step5] Step-capped training: --max_steps=3000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 10.3808, 'grad_norm': 27.524154663085938, 'learning_rate': 1.597517730496454e-05, 'epoch': 1.25}
{'loss': 10.4811, 'grad_norm': 22.087936401367188, 'learning_rate': 1.420212765957447e-05, 'epoch': 1.31}
{'loss': 10.3994, 'grad_norm': 66.9840087890625, 'learning_rate': 1.2429078014184398e-05, 'epoch': 1.37}
{'loss': 10.341, 'grad_norm': 38.998695373535156, 'learning_rate': 1.0656028368794328e-05, 'epoch': 1.43}
{'loss': 10.4493, 'grad_norm': 111.68907165527344, 'learning_rate': 8.882978723404256e-06, 'epoch': 1.49}
{'loss': 10.3693, 'grad_norm': 53.55702209472656, 'learning_rate': 7.109929078014185e-06, 'epoch': 1.55}
{'loss': 10.3467, 'grad_norm': 11.583353042602539, 'learning_rate': 5.336879432624114e-06, 'epoch': 1.61}
{'loss': 10.2744, 'grad_norm': 18.863554000854492, 'learning_rate': 3.563829787234043e-06, 'epoch': 1.67}
{'loss': 10.3611, 'grad_norm': 13.959734916687012, 'learning_rate': 1.790780141843972e-06, 'epoch': 1.73}
{'loss': 10.3196, 'grad_norm': 19.702716827392578, 'learning_rate': 1.773049645390071e-08, 'epoch': 1.79}
{'train_runtime': 14774.2944, 'train_samples_per_second': 6.498, 'train_steps_per_second': 0.203, 'train_loss': 3.457422607421875, 'epoch': 1.79}
[Distill][Eval] step=3000 epoch=1.79 eval_loss=2.6085 eval_log_loss=1.7539 acc=0.4263
{'eval': {'eval_loss': 2.608504056930542, 'eval_log_loss': 1.7538987436217204, 'eval_accuracy': 0.4263149050579734, 'eval_runtime': 805.1909, 'eval_samples_per_second': 7.391, 'eval_steps_per_second': 0.924, 'epoch': 1.7922330097087378}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
