[Step4] SAVE_ROOT=/scratch-shared/tc1proj005/folds FOLDS=[1] MODELS=['llama'] SAVE_LASTTOK=False TOPK=0
[Step4] Sharding enabled: shard_id=1 of 5 strategy=range suffix='_sh1-of-5' WRITE_ENSEMBLE=False
[Step4] MAX_SEQ_LEN=768 FUSED_PAIRS=True PAD_TO_MULT=8 SAVE_LOGPROBS=False
[Step4] Loaded data/fold_data/fold_1_train.csv rows=59505 cols=9
[Step4][Shard] Fold 1: range strategy rows 11901:23802 -> 11901/59505.
[Step4] Loaded data/fold_data/fold_1_val.csv rows=19159 cols=9
[Step4] Loading llama fold 1 from LoRA /scratch-shared/tc1proj005/folds/llama_fold_1_lora + base /scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged
[Step4][Info] Tokenizer source for llama fold 1: /scratch-shared/tc1proj005/folds/llama_fold_1_lora
[Step4][Info] Accelerate device_map detected; skipping explicit model.to().
[Step4][Subset] llama fold 1: requested 50000 > available 11901. Using full set.
[Step4][Debug] Preparing last-token repr on 11901 texts (SAVE_LASTTOK=False)
[Step4][Debug] Starting TRAIN log-likelihood (N=11901), batch_size=8, progress_every=50
[Step4] Using batched loglik batch_size=8 fused=True progress_every=50
[Step4][Progress] Fused batch 8/11901 (2.14 ex/s) elapsed=0.1m
[Step4][Progress] Fused batch 408/11901 (2.31 ex/s) elapsed=2.9m
[Step4][Progress] Fused batch 808/11901 (2.34 ex/s) elapsed=5.8m
[Step4][Progress] Fused batch 1208/11901 (2.34 ex/s) elapsed=8.6m
[Step4][Progress] Fused batch 1608/11901 (2.32 ex/s) elapsed=11.5m
[Step4][Progress] Fused batch 2008/11901 (2.33 ex/s) elapsed=14.4m
[Step4][Progress] Fused batch 2408/11901 (2.34 ex/s) elapsed=17.1m
[Step4][Progress] Fused batch 2808/11901 (2.34 ex/s) elapsed=20.0m
[Step4][Progress] Fused batch 3208/11901 (2.34 ex/s) elapsed=22.9m
[Step4][Progress] Fused batch 3608/11901 (2.34 ex/s) elapsed=25.7m
[Step4][Progress] Fused batch 4008/11901 (2.35 ex/s) elapsed=28.5m
[Step4][Progress] Fused batch 4408/11901 (2.35 ex/s) elapsed=31.3m
[Step4][Progress] Fused batch 4808/11901 (2.35 ex/s) elapsed=34.1m
[Step4][Progress] Fused batch 5208/11901 (2.35 ex/s) elapsed=36.9m
[Step4][Progress] Fused batch 5608/11901 (2.35 ex/s) elapsed=39.8m
[Step4][Progress] Fused batch 6008/11901 (2.35 ex/s) elapsed=42.6m
[Step4][Progress] Fused batch 6408/11901 (2.35 ex/s) elapsed=45.5m
[Step4][Progress] Fused batch 6808/11901 (2.35 ex/s) elapsed=48.3m
[Step4][Progress] Fused batch 7208/11901 (2.35 ex/s) elapsed=51.1m
[Step4][Progress] Fused batch 7608/11901 (2.35 ex/s) elapsed=53.9m
[Step4][Progress] Fused batch 8008/11901 (2.35 ex/s) elapsed=56.7m
[Step4][Progress] Fused batch 8408/11901 (2.35 ex/s) elapsed=59.6m
[Step4][Progress] Fused batch 8808/11901 (2.35 ex/s) elapsed=62.3m
[Step4][Progress] Fused batch 9208/11901 (2.36 ex/s) elapsed=65.1m
[Step4][Progress] Fused batch 9608/11901 (2.36 ex/s) elapsed=68.0m
[Step4][Progress] Fused batch 10008/11901 (2.36 ex/s) elapsed=70.8m
[Step4][Progress] Fused batch 10408/11901 (2.36 ex/s) elapsed=73.6m
[Step4][Progress] Fused batch 10808/11901 (2.36 ex/s) elapsed=76.4m
[Step4][Progress] Fused batch 11208/11901 (2.36 ex/s) elapsed=79.2m
[Step4][Progress] Fused batch 11608/11901 (2.36 ex/s) elapsed=82.0m
[Step4] Saved tensor -> model_save/teacher_logits/llama_fold_1_train_probs_sh1-of-5.pt shape=(11901, 3)
[Step4] Per-model/fold inference done.
[Step4][OOF] Table -> model_save/teacher_logits/oof_probs_sh1-of-5.parquet rows=11901
[Step4] Finished.
[Step4] Done
