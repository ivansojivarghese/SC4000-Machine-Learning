[Step5] Distilling fold 2 using LLaMA-only OOF probs
[Step5] Using teacher OOF table: model_save/teacher_logits/oof_probs_fold2_merged.parquet (selector=auto)
[Step5] Step-capped training: --max_steps=3000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 10.0734, 'grad_norm': 19.95120620727539, 'learning_rate': 1.597517730496454e-05, 'epoch': 1.25}
{'loss': 10.1264, 'grad_norm': 78.79512786865234, 'learning_rate': 1.420212765957447e-05, 'epoch': 1.31}
{'loss': 10.2274, 'grad_norm': 56.435394287109375, 'learning_rate': 1.2429078014184398e-05, 'epoch': 1.37}
{'loss': 10.1543, 'grad_norm': 37.53411865234375, 'learning_rate': 1.0656028368794328e-05, 'epoch': 1.43}
{'loss': 10.2824, 'grad_norm': 25.735498428344727, 'learning_rate': 8.882978723404256e-06, 'epoch': 1.49}
{'loss': 10.2491, 'grad_norm': 31.565719604492188, 'learning_rate': 7.109929078014185e-06, 'epoch': 1.55}
{'loss': 10.0711, 'grad_norm': 19.071271896362305, 'learning_rate': 5.336879432624114e-06, 'epoch': 1.61}
{'loss': 9.8955, 'grad_norm': 59.20673751831055, 'learning_rate': 3.563829787234043e-06, 'epoch': 1.67}
{'loss': 9.9385, 'grad_norm': 45.90962219238281, 'learning_rate': 1.790780141843972e-06, 'epoch': 1.73}
{'loss': 10.0141, 'grad_norm': 33.3503532409668, 'learning_rate': 1.773049645390071e-08, 'epoch': 1.79}
{'train_runtime': 14527.229, 'train_samples_per_second': 6.608, 'train_steps_per_second': 0.207, 'train_loss': 3.367740214029948, 'epoch': 1.79}
[Distill][Eval] step=3000 epoch=1.79 eval_loss=2.5530 eval_log_loss=1.9539 acc=0.3183
{'eval': {'eval_loss': 2.553010940551758, 'eval_log_loss': 1.953869870876263, 'eval_accuracy': 0.31826583767434047, 'eval_runtime': 807.6954, 'eval_samples_per_second': 7.368, 'eval_steps_per_second': 0.921, 'epoch': 1.7922330097087378}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
