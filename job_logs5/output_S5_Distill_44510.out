[Step5] Distilling fold 1 using LLaMA-only OOF probs
[Step5] Using teacher OOF table: model_save/teacher_logits/oof_probs_fold1_merged.parquet (selector=auto)
[Step5] Step-capped training: --max_steps=3000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 3.108, 'grad_norm': 24.965124130249023, 'learning_rate': 1.597517730496454e-05, 'epoch': 1.25}
{'loss': 3.3027, 'grad_norm': 33.818817138671875, 'learning_rate': 1.420212765957447e-05, 'epoch': 1.31}
{'loss': 3.2951, 'grad_norm': 9.85590934753418, 'learning_rate': 1.2429078014184398e-05, 'epoch': 1.37}
{'loss': 3.1901, 'grad_norm': 9.3027982711792, 'learning_rate': 1.0656028368794328e-05, 'epoch': 1.43}
{'loss': 3.3461, 'grad_norm': 21.385881423950195, 'learning_rate': 8.882978723404256e-06, 'epoch': 1.49}
{'loss': 3.2327, 'grad_norm': 82.77845001220703, 'learning_rate': 7.109929078014185e-06, 'epoch': 1.55}
{'loss': 3.0376, 'grad_norm': 13.058034896850586, 'learning_rate': 5.336879432624114e-06, 'epoch': 1.61}
{'loss': 3.0903, 'grad_norm': 11.289978981018066, 'learning_rate': 3.563829787234043e-06, 'epoch': 1.67}
{'loss': 3.0513, 'grad_norm': 16.11838722229004, 'learning_rate': 1.790780141843972e-06, 'epoch': 1.73}
{'loss': 3.0408, 'grad_norm': 15.563105583190918, 'learning_rate': 1.773049645390071e-08, 'epoch': 1.79}
{'train_runtime': 14846.7527, 'train_samples_per_second': 6.466, 'train_steps_per_second': 0.202, 'train_loss': 1.056488739013672, 'epoch': 1.79}
[Distill][Eval] step=3000 epoch=1.79 eval_loss=0.8310 eval_log_loss=1.1726 acc=0.4840
{'eval': {'eval_loss': 0.8310034275054932, 'eval_log_loss': 1.1726407267152084, 'eval_accuracy': 0.4839522769282473, 'eval_runtime': 807.0103, 'eval_samples_per_second': 7.374, 'eval_steps_per_second': 0.922, 'epoch': 1.7922330097087378}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
