Suggested pipeline flow:

Step 1: Post-pretrain large models on UT dataset
Step 2: Split Kaggle + 33k data into 5 folds
Step 3: Train LLaMA3 70B & Qwen2 72B on folds
Step 4: Infer logits for all training data
Step 5: Distill logits into Gemma2-9B model
Step 6: Ensemble LoRA layers from 5 folds
Step 7: Quantize final model to 8-bit (GPTQ)
Step 8: Apply TTA during inference
Step 9: Evaluate CV and LB


HF token:

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


Commands:

conda activate myenv
conda deactivate myenv
module load anaconda


Step 1:

Here are the exact commands to submit one model at a time to your scratch. Run these on the cluster login node (bash), from the repo folder.

# Gemma
RUN_STAGE=gemma SCRATCH_BASE=/scratch-shared/tc1proj005 sbatch step1_post_pretrain.sh
OR
RUN_STAGE=gemma SCRATCH_BASE=/scratch-shared/tc1proj005 POSTPRE_EPOCHS=1 POSTPRE_LR=1e-5 sbatch step1_post_pretrain.sh
OR
POSTPRE_SUBSET_SIZE=30000 POSTPRE_GRAD_ACCUM=16 POSTPRE_MAXLEN=512 RUN_STAGE=gemma SCRATCH_BASE=/scratch-shared/tc1proj005 sbatch step1_post_pretrain.sh
OR
export RUN_STAGE=gemma
export POSTPRE_MAXLEN=512
export POSTPRE_SUBSET_SIZE=8000
export POSTPRE_PER_DEVICE_BS=2
export POSTPRE_GRAD_ACCUM=16
export POSTPRE_LORA_R=16
export POSTPRE_LORA_ALPHA=32
export POSTPRE_EPOCHS=1
export POSTPRE_MAX_STEPS=-1          # allow auto-cap
export POSTPRE_TIME_BUDGET_HOURS=6
export POSTPRE_DISABLE_CHKPT=1
sbatch step1_post_pretrain.sh

# Qwen
RUN_STAGE=qwen SCRATCH_BASE=/scratch-shared/tc1proj005 sbatch step1_post_pretrain.sh
OR
export RUN_STAGE=qwen
export POSTPRE_FORCE_MAX_STEPS=240
export POSTPRE_MAXLEN=512
export POSTPRE_SUBSET_SIZE=6000
export POSTPRE_LORA_R=16
export POSTPRE_LORA_ALPHA=32
export POSTPRE_PER_DEVICE_BS=1
export POSTPRE_GRAD_ACCUM=12
export POSTPRE_DISABLE_CHKPT=1
export POSTPRE_MAX_STEPS=-1
sbatch step1_post_pretrain.sh
AND
export RUN_STAGE=qwen
export POSTPRE_MERGE_ONLY=1
sbatch step1_post_pretrain.sh

# LLaMA
RUN_STAGE=llama SCRATCH_BASE=/scratch-shared/tc1proj005 sbatch step1_post_pretrain.sh
OR
export RUN_STAGE=llama
unset POSTPRE_MERGE_ONLY
unset POSTPRE_SKIP_IF_EXISTS
export POSTPRE_FORCE_MAX_STEPS=370
export POSTPRE_MAXLEN=512
export POSTPRE_SUBSET_SIZE=6000
export POSTPRE_LORA_R=32
export POSTPRE_LORA_ALPHA=64
export POSTPRE_PER_DEVICE_BS=1
export POSTPRE_GRAD_ACCUM=12
# Keep gradient checkpointing ON (do NOT set POSTPRE_DISABLE_CHKPT)
sbatch step1_post_pretrain.sh



Step 2:
sbatch step2_make_folds.sh
OR
sbatch --export=ALL,SCRATCH_BASE=/scratch-shared/tc1proj005 step2_make_folds.sh



Step 3:
export TEACHER_LORA_R=16
export TEACHER_LORA_ALPHA=32
export TEACHER_MAXLEN=512
export TEACHER_GRAD_ACCUM=8
export TEACHER_MAX_STEPS=300   # or adjust
sbatch step3_train_teachers.sh
OR
export TEACHER_TIME_BUDGET_HOURS=6
export TEACHER_EST_STEP_SEC=55   # if real step slower, it will lower steps further
export TEACHER_MAX_STEPS=80      # upper bound before auto-cap
export HF_TOKEN=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
# (Optionally specify local bases if you know their dirs)
# export LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged
# export QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-14b_merged
sbatch step3_train_teachers.sh
OR
----
export HF_TOKEN=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
sbatch --export=ALL,TEACHER_FOLDS=0,TEACHER_MAX_STEPS=30,LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged,QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged step3_train_teachers.sh
OR
TEACHER_FOLDS=0 \
LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged \
QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged \
TEACHER_DISABLE_SYMLINKS=1 TEACHER_REQUIRE_WEIGHTS=1 \
TEACHER_SKIP_TRAIN=0 \
sbatch step3_train_teachers.sh


export HF_TOKEN=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
sbatch --export=ALL,TEACHER_FOLDS=1,TEACHER_MAX_STEPS=30,LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged,QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged step3_train_teachers.sh
OR
TEACHER_FOLDS=1 \
LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged \
QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged \
TEACHER_DISABLE_SYMLINKS=1 TEACHER_REQUIRE_WEIGHTS=1 \
TEACHER_SKIP_TRAIN=0 \
sbatch step3_train_teachers.sh


export HF_TOKEN=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
sbatch --export=ALL,TEACHER_FOLDS=2,TEACHER_MAX_STEPS=30,LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged,QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged step3_train_teachers.sh
OR
TEACHER_FOLDS=2 \
LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged \
QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged \
TEACHER_DISABLE_SYMLINKS=1 TEACHER_REQUIRE_WEIGHTS=1 \
TEACHER_SKIP_TRAIN=0 \
sbatch step3_train_teachers.sh


export HF_TOKEN=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
sbatch --export=ALL,TEACHER_FOLDS=3,TEACHER_MAX_STEPS=30,LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged,QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged step3_train_teachers.sh
OR


export HF_TOKEN=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
sbatch --export=ALL,TEACHER_FOLDS=4,TEACHER_MAX_STEPS=30,LLAMA_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_llama3-8b_merged,QWEN_FT_BASE=/scratch-shared/tc1proj005/post_pretrain_qwen2-72b_merged step3_train_teachers.sh
OR



Step 4:
