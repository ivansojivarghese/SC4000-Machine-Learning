[Step5] Distilling fold 2 using LLaMA-only OOF probs
[Step5] Step-capped training: --max_steps=6000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 3.5158, 'grad_norm': 62.6762809753418, 'learning_rate': 8.430851063829787e-06, 'epoch': 3.02}
{'loss': 3.5131, 'grad_norm': 33.530696868896484, 'learning_rate': 7.98758865248227e-06, 'epoch': 3.05}
{'loss': 3.541, 'grad_norm': 32.07878494262695, 'learning_rate': 7.5443262411347525e-06, 'epoch': 3.08}
{'loss': 3.6827, 'grad_norm': 45.23744201660156, 'learning_rate': 7.101063829787235e-06, 'epoch': 3.11}
{'loss': 3.3385, 'grad_norm': 46.695316314697266, 'learning_rate': 6.6578014184397175e-06, 'epoch': 3.14}
{'loss': 3.5732, 'grad_norm': 61.4781608581543, 'learning_rate': 6.214539007092199e-06, 'epoch': 3.17}
{'loss': 3.5587, 'grad_norm': 34.887027740478516, 'learning_rate': 5.771276595744681e-06, 'epoch': 3.2}
{'loss': 3.4057, 'grad_norm': 51.808048248291016, 'learning_rate': 5.328014184397164e-06, 'epoch': 3.23}
{'loss': 3.7026, 'grad_norm': 42.86760711669922, 'learning_rate': 4.884751773049646e-06, 'epoch': 3.26}
{'loss': 3.6089, 'grad_norm': 38.7589111328125, 'learning_rate': 4.441489361702128e-06, 'epoch': 3.29}
{'loss': 3.4411, 'grad_norm': 40.84065246582031, 'learning_rate': 3.99822695035461e-06, 'epoch': 3.32}
{'loss': 3.6291, 'grad_norm': 39.85470962524414, 'learning_rate': 3.5549645390070927e-06, 'epoch': 3.35}
{'loss': 3.3878, 'grad_norm': 49.242401123046875, 'learning_rate': 3.111702127659575e-06, 'epoch': 3.38}
{'loss': 3.4141, 'grad_norm': 50.600765228271484, 'learning_rate': 2.668439716312057e-06, 'epoch': 3.41}
{'loss': 3.528, 'grad_norm': 49.263648986816406, 'learning_rate': 2.225177304964539e-06, 'epoch': 3.44}
{'loss': 3.6405, 'grad_norm': 110.59962463378906, 'learning_rate': 1.7819148936170215e-06, 'epoch': 3.47}
{'loss': 3.4764, 'grad_norm': 86.78815460205078, 'learning_rate': 1.3386524822695036e-06, 'epoch': 3.5}
{'loss': 3.6117, 'grad_norm': 47.63325119018555, 'learning_rate': 8.95390070921986e-07, 'epoch': 3.53}
{'loss': 3.4293, 'grad_norm': 37.881649017333984, 'learning_rate': 4.5212765957446806e-07, 'epoch': 3.56}
{'loss': 3.666, 'grad_norm': 40.915897369384766, 'learning_rate': 8.865248226950355e-09, 'epoch': 3.58}
{'train_runtime': 14597.2788, 'train_samples_per_second': 13.153, 'train_steps_per_second': 0.411, 'train_loss': 0.5888686955769856, 'epoch': 3.58}
[Distill][Eval] step=6000 epoch=3.58 eval_loss=1.2263 eval_log_loss=1.3899 acc=0.4260
{'eval': {'eval_loss': 1.2263115644454956, 'eval_log_loss': 1.3898510720636326, 'eval_accuracy': 0.42597882708788437, 'eval_runtime': 821.8835, 'eval_samples_per_second': 7.241, 'eval_steps_per_second': 0.905, 'epoch': 3.5849141150112023}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
