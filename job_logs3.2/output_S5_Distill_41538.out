[Step5] Distilling fold 2 using LLaMA-only OOF probs
[Step5] Step-capped training: --max_steps=4000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 4.083, 'grad_norm': 10.846404075622559, 'learning_rate': 1.2646276595744683e-05, 'epoch': 1.82}
{'loss': 4.436, 'grad_norm': 16.790874481201172, 'learning_rate': 1.1981382978723405e-05, 'epoch': 1.85}
{'loss': 4.2809, 'grad_norm': 29.10495948791504, 'learning_rate': 1.1316489361702127e-05, 'epoch': 1.88}
{'loss': 4.1701, 'grad_norm': 12.699490547180176, 'learning_rate': 1.0651595744680851e-05, 'epoch': 1.91}
{'loss': 4.2068, 'grad_norm': 14.20249080657959, 'learning_rate': 9.986702127659575e-06, 'epoch': 1.94}
{'loss': 4.3116, 'grad_norm': 9.818349838256836, 'learning_rate': 9.321808510638298e-06, 'epoch': 1.97}
{'loss': 4.1767, 'grad_norm': 13.677449226379395, 'learning_rate': 8.656914893617022e-06, 'epoch': 2.0}
{'loss': 3.8903, 'grad_norm': 16.688753128051758, 'learning_rate': 7.992021276595744e-06, 'epoch': 2.03}
{'loss': 3.8642, 'grad_norm': 14.472139358520508, 'learning_rate': 7.327127659574469e-06, 'epoch': 2.06}
{'loss': 4.0682, 'grad_norm': 22.03628158569336, 'learning_rate': 6.662234042553192e-06, 'epoch': 2.09}
{'loss': 4.1396, 'grad_norm': 26.39024543762207, 'learning_rate': 5.997340425531915e-06, 'epoch': 2.12}
{'loss': 4.0095, 'grad_norm': 23.314191818237305, 'learning_rate': 5.332446808510638e-06, 'epoch': 2.15}
{'loss': 4.1341, 'grad_norm': 17.664432525634766, 'learning_rate': 4.667553191489362e-06, 'epoch': 2.18}
{'loss': 4.0143, 'grad_norm': 24.311351776123047, 'learning_rate': 4.0026595744680856e-06, 'epoch': 2.21}
{'loss': 3.8436, 'grad_norm': 21.713247299194336, 'learning_rate': 3.337765957446809e-06, 'epoch': 2.24}
{'loss': 3.7701, 'grad_norm': 19.770004272460938, 'learning_rate': 2.672872340425532e-06, 'epoch': 2.27}
{'loss': 3.9714, 'grad_norm': 19.175283432006836, 'learning_rate': 2.0079787234042555e-06, 'epoch': 2.3}
{'loss': 4.2926, 'grad_norm': 25.408666610717773, 'learning_rate': 1.3430851063829788e-06, 'epoch': 2.33}
{'loss': 4.2292, 'grad_norm': 41.191810607910156, 'learning_rate': 6.781914893617022e-07, 'epoch': 2.36}
{'loss': 3.9883, 'grad_norm': 22.073598861694336, 'learning_rate': 1.3297872340425533e-08, 'epoch': 2.39}
{'train_runtime': 14393.7656, 'train_samples_per_second': 8.893, 'train_steps_per_second': 0.278, 'train_loss': 1.0235029983520507, 'epoch': 2.39}
[Distill][Eval] step=4000 epoch=2.39 eval_loss=1.1271 eval_log_loss=1.2968 acc=0.4310
{'eval': {'eval_loss': 1.1270941495895386, 'eval_log_loss': 1.2967816395116893, 'eval_accuracy': 0.4310199966392203, 'eval_runtime': 803.7052, 'eval_samples_per_second': 7.404, 'eval_steps_per_second': 0.926, 'epoch': 2.3901418969380135}}
[Step5] Done fold 2 -> model_save/distilled_gemma2-9b_fold_2
