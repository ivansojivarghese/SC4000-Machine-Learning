[Step5] Distilling fold 1 using LLaMA-only OOF probs
[Step5] Step-capped training: --max_steps=5000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 0.8466, 'grad_norm': 7.094582557678223, 'learning_rate': 1.0117021276595745e-05, 'epoch': 2.42}
{'loss': 0.8941, 'grad_norm': 8.959953308105469, 'learning_rate': 9.585106382978724e-06, 'epoch': 2.45}
{'loss': 0.9154, 'grad_norm': 13.791404724121094, 'learning_rate': 9.053191489361702e-06, 'epoch': 2.48}
{'loss': 0.9505, 'grad_norm': 12.867952346801758, 'learning_rate': 8.52127659574468e-06, 'epoch': 2.51}
{'loss': 0.9569, 'grad_norm': 9.85690689086914, 'learning_rate': 7.98936170212766e-06, 'epoch': 2.54}
{'loss': 1.0272, 'grad_norm': 11.613615989685059, 'learning_rate': 7.457446808510639e-06, 'epoch': 2.57}
{'loss': 0.9277, 'grad_norm': 12.120387077331543, 'learning_rate': 6.925531914893617e-06, 'epoch': 2.6}
{'loss': 0.8348, 'grad_norm': 16.066513061523438, 'learning_rate': 6.393617021276596e-06, 'epoch': 2.63}
{'loss': 0.9109, 'grad_norm': 10.780454635620117, 'learning_rate': 5.861702127659575e-06, 'epoch': 2.66}
{'loss': 0.9205, 'grad_norm': 13.8902006149292, 'learning_rate': 5.3297872340425535e-06, 'epoch': 2.69}
{'loss': 0.9542, 'grad_norm': 11.977545738220215, 'learning_rate': 4.797872340425532e-06, 'epoch': 2.72}
{'loss': 0.918, 'grad_norm': 12.864794731140137, 'learning_rate': 4.26595744680851e-06, 'epoch': 2.75}
{'loss': 0.9574, 'grad_norm': 11.940849304199219, 'learning_rate': 3.7340425531914894e-06, 'epoch': 2.78}
{'loss': 0.8108, 'grad_norm': 14.54803466796875, 'learning_rate': 3.2021276595744686e-06, 'epoch': 2.81}
{'loss': 0.7581, 'grad_norm': 10.631585121154785, 'learning_rate': 2.670212765957447e-06, 'epoch': 2.84}
{'loss': 0.7915, 'grad_norm': 16.85953140258789, 'learning_rate': 2.1382978723404258e-06, 'epoch': 2.87}
{'loss': 0.8223, 'grad_norm': 12.71910285949707, 'learning_rate': 1.6063829787234043e-06, 'epoch': 2.9}
{'loss': 0.815, 'grad_norm': 9.945076942443848, 'learning_rate': 1.074468085106383e-06, 'epoch': 2.93}
{'loss': 0.8339, 'grad_norm': 16.489253997802734, 'learning_rate': 5.425531914893618e-07, 'epoch': 2.96}
{'loss': 0.8609, 'grad_norm': 12.82036018371582, 'learning_rate': 1.0638297872340427e-08, 'epoch': 2.99}
{'train_runtime': 14522.2152, 'train_samples_per_second': 11.018, 'train_steps_per_second': 0.344, 'train_loss': 0.17706758193969727, 'epoch': 2.99}
[Distill][Eval] step=5000 epoch=2.99 eval_loss=0.3313 eval_log_loss=1.0743 acc=0.5329
{'eval': {'eval_loss': 0.3312528431415558, 'eval_log_loss': 1.0743098936082698, 'eval_accuracy': 0.5328516215762057, 'eval_runtime': 806.5769, 'eval_samples_per_second': 7.378, 'eval_steps_per_second': 0.922, 'epoch': 2.9870052277819266}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
