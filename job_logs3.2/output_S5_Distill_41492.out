[Step5] Distilling fold 1 using LLaMA-only OOF probs
[Step5] Step-capped training: --max_steps=4000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 1.0494, 'grad_norm': 6.4942169189453125, 'learning_rate': 1.2646276595744683e-05, 'epoch': 1.82}
{'loss': 1.0253, 'grad_norm': 10.654274940490723, 'learning_rate': 1.1981382978723405e-05, 'epoch': 1.85}
{'loss': 1.054, 'grad_norm': 10.353540420532227, 'learning_rate': 1.1316489361702127e-05, 'epoch': 1.88}
{'loss': 1.0339, 'grad_norm': 4.845811367034912, 'learning_rate': 1.0651595744680851e-05, 'epoch': 1.91}
{'loss': 1.0542, 'grad_norm': 11.28916072845459, 'learning_rate': 9.986702127659575e-06, 'epoch': 1.94}
{'loss': 1.0862, 'grad_norm': 7.007524490356445, 'learning_rate': 9.321808510638298e-06, 'epoch': 1.97}
{'loss': 1.0183, 'grad_norm': 8.652506828308105, 'learning_rate': 8.656914893617022e-06, 'epoch': 2.0}
{'loss': 0.9873, 'grad_norm': 10.530303001403809, 'learning_rate': 7.992021276595744e-06, 'epoch': 2.03}
{'loss': 0.9645, 'grad_norm': 10.646833419799805, 'learning_rate': 7.327127659574469e-06, 'epoch': 2.06}
{'loss': 0.9745, 'grad_norm': 6.593258857727051, 'learning_rate': 6.662234042553192e-06, 'epoch': 2.09}
{'loss': 0.978, 'grad_norm': 7.834810733795166, 'learning_rate': 5.997340425531915e-06, 'epoch': 2.12}
{'loss': 0.9966, 'grad_norm': 10.927979469299316, 'learning_rate': 5.332446808510638e-06, 'epoch': 2.15}
{'loss': 0.9622, 'grad_norm': 9.19051456451416, 'learning_rate': 4.667553191489362e-06, 'epoch': 2.18}
{'loss': 0.9701, 'grad_norm': 9.479673385620117, 'learning_rate': 4.0026595744680856e-06, 'epoch': 2.21}
{'loss': 0.9804, 'grad_norm': 7.095280647277832, 'learning_rate': 3.337765957446809e-06, 'epoch': 2.24}
{'loss': 0.9469, 'grad_norm': 9.029461860656738, 'learning_rate': 2.672872340425532e-06, 'epoch': 2.27}
{'loss': 0.9594, 'grad_norm': 12.397529602050781, 'learning_rate': 2.0079787234042555e-06, 'epoch': 2.3}
{'loss': 0.9875, 'grad_norm': 9.085652351379395, 'learning_rate': 1.3430851063829788e-06, 'epoch': 2.33}
{'loss': 0.949, 'grad_norm': 10.02071475982666, 'learning_rate': 6.781914893617022e-07, 'epoch': 2.36}
{'loss': 0.9465, 'grad_norm': 11.072528839111328, 'learning_rate': 1.3297872340425533e-08, 'epoch': 2.39}
{'train_runtime': 14540.7528, 'train_samples_per_second': 8.803, 'train_steps_per_second': 0.275, 'train_loss': 0.2490550832748413, 'epoch': 2.39}
[Distill][Eval] step=4000 epoch=2.39 eval_loss=0.3019 eval_log_loss=0.9809 acc=0.5493
{'eval': {'eval_loss': 0.3018820285797119, 'eval_log_loss': 0.9808922603305777, 'eval_accuracy': 0.5493194421105696, 'eval_runtime': 819.8215, 'eval_samples_per_second': 7.259, 'eval_steps_per_second': 0.908, 'epoch': 2.3901418969380135}}
[Step5] Done fold 1 -> model_save/distilled_gemma2-9b_fold_1
