[Step5] Distilling fold 0 using LLaMA-only OOF probs
[Step5] Step-capped training: --max_steps=3000 will limit total steps (overrides epochs)
[Step5] Eval strategy=steps steps | Save strategy=steps (save_steps=200) | logging_steps=100
{'loss': 1.0975, 'grad_norm': 7.931448459625244, 'learning_rate': 1.6861702127659575e-05, 'epoch': 1.22}
{'loss': 1.1122, 'grad_norm': 9.699689865112305, 'learning_rate': 1.597517730496454e-05, 'epoch': 1.25}
{'loss': 1.1073, 'grad_norm': 5.208898067474365, 'learning_rate': 1.5088652482269505e-05, 'epoch': 1.28}
{'loss': 1.1095, 'grad_norm': 6.464396953582764, 'learning_rate': 1.420212765957447e-05, 'epoch': 1.31}
{'loss': 1.129, 'grad_norm': 6.789892673492432, 'learning_rate': 1.3315602836879435e-05, 'epoch': 1.34}
{'loss': 1.0802, 'grad_norm': 6.32806396484375, 'learning_rate': 1.2429078014184398e-05, 'epoch': 1.37}
{'loss': 1.0983, 'grad_norm': 4.754828929901123, 'learning_rate': 1.1542553191489362e-05, 'epoch': 1.4}
{'loss': 1.067, 'grad_norm': 4.133097171783447, 'learning_rate': 1.0656028368794328e-05, 'epoch': 1.43}
{'loss': 1.0627, 'grad_norm': 6.401528835296631, 'learning_rate': 9.769503546099292e-06, 'epoch': 1.46}
{'loss': 1.0971, 'grad_norm': 3.6136538982391357, 'learning_rate': 8.882978723404256e-06, 'epoch': 1.49}
{'loss': 1.1412, 'grad_norm': 7.069066047668457, 'learning_rate': 7.99645390070922e-06, 'epoch': 1.52}
{'loss': 1.1572, 'grad_norm': 4.010916233062744, 'learning_rate': 7.109929078014185e-06, 'epoch': 1.55}
{'loss': 1.1177, 'grad_norm': 6.665676116943359, 'learning_rate': 6.22340425531915e-06, 'epoch': 1.58}
{'loss': 1.0449, 'grad_norm': 7.248373508453369, 'learning_rate': 5.336879432624114e-06, 'epoch': 1.61}
{'loss': 1.0238, 'grad_norm': 4.382761478424072, 'learning_rate': 4.450354609929078e-06, 'epoch': 1.64}
{'loss': 1.0369, 'grad_norm': 4.665048599243164, 'learning_rate': 3.563829787234043e-06, 'epoch': 1.67}
{'loss': 1.0138, 'grad_norm': 12.125799179077148, 'learning_rate': 2.6773049645390073e-06, 'epoch': 1.7}
{'loss': 1.02, 'grad_norm': 4.44598388671875, 'learning_rate': 1.790780141843972e-06, 'epoch': 1.73}
{'loss': 1.0282, 'grad_norm': 6.343697547912598, 'learning_rate': 9.042553191489361e-07, 'epoch': 1.76}
{'loss': 0.9982, 'grad_norm': 6.259654998779297, 'learning_rate': 1.773049645390071e-08, 'epoch': 1.79}
{'train_runtime': 14631.7074, 'train_samples_per_second': 6.561, 'train_steps_per_second': 0.205, 'train_loss': 0.3590416514078776, 'epoch': 1.79}
[Distill][Eval] step=3000 epoch=1.79 eval_loss=0.2933 eval_log_loss=0.9549 acc=0.5387
{'eval': {'eval_loss': 0.29332229495048523, 'eval_log_loss': 0.9549464287102504, 'eval_accuracy': 0.5387329860527642, 'eval_runtime': 809.5522, 'eval_samples_per_second': 7.351, 'eval_steps_per_second': 0.919, 'epoch': 1.7922330097087378}}
[Step5] Done fold 0 -> model_save/distilled_gemma2-9b_fold_0
